{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a1b731-b850-4b3c-ac62-2d46becc1afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ml_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab8f960e-454b-4a6e-9e93-f7e721a531e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def awgn(signal, SNR, measured=False, return_true_snr=False):\n",
    "    \n",
    "    if measured:\n",
    "        # Measure signal power \n",
    "        s_p = np.mean(abs(signal)**2)\n",
    "    else:\n",
    "        s_p = 1\n",
    "    \n",
    "    # Calculate noise power\n",
    "    n_p = s_p/(10 **(SNR/10))\n",
    "    \n",
    "    # Generate complex noise\n",
    "    noise = np.sqrt(n_p/2)*(np.random.randn(*signal.shape) + \\\n",
    "                                np.random.randn(*signal.shape)*1j)\n",
    "    \n",
    "    # Add signal and noise \n",
    "    signal_noisy = signal + noise \n",
    "    \n",
    "    if not return_true_snr:\n",
    "        return signal_noisy\n",
    "    else:\n",
    "        return signal_noisy, s_p/(np.mean(abs(noise)**2))\n",
    "    \n",
    "def pulse_shape(symbols, sps=5):\n",
    "    num_weights = 251\n",
    "    x = np.arange(-int(num_weights/2),int(num_weights/2)+1,1)/sps\n",
    "    sinc_weights = np.sinc(x)\n",
    "    \n",
    "    padded_symbols = np.zeros(len(symbols)*sps, dtype=complex)\n",
    "    padded_symbols[np.arange(0,len(padded_symbols),sps)] = symbols\n",
    "    \n",
    "    return np.convolve(padded_symbols, sinc_weights, mode='same')\n",
    "\n",
    "# Function to generate BPSK\n",
    "def generate_bpsk(num_symbols, noise=50):\n",
    "    bits = np.random.randint(0,2,num_symbols)\n",
    "    bpsk_scheme = [1+0j, -1+0j]\n",
    "    bpsk_symbols = np.array([bpsk_scheme[i] for i in bits])\n",
    "    \n",
    "    bpsk_symbols = awgn(bpsk_symbols, noise)\n",
    "    \n",
    "    return bpsk_symbols\n",
    "\n",
    "# Function to generate QPSK\n",
    "def generate_qpsk(num_symbols, noise=50):\n",
    "    qpsk_scheme= [1+1j, 1-1j, -1+1j, -1-1j]\n",
    "    ints = np.random.randint(0,4,num_symbols)\n",
    "    qpsk_symbols = np.array([qpsk_scheme[i] for i in ints])/np.sqrt(2)\n",
    "    \n",
    "    return qpsk_symbols\n",
    "\n",
    "# Function to generate QAM\n",
    "def generate_qam(num_symbols, noise=50):\n",
    "    qam_scheme = [-3-3j, -3-1j, -3+3j, -3+1j,  \\\n",
    "                  -1-3j, -1-1j, -1+3j, -1+1j,  \\\n",
    "                   3-3j,  3-1j,  3+3j,  3+1j,  \\\n",
    "                   1-3j,  1-1j,  1+3j,  1+1j]\n",
    "    ints = np.random.randint(0,16,num_symbols)\n",
    "    qam_symbols = np.array([qam_scheme[i] for i in ints])\n",
    "    qam_symbols = qam_symbols/np.mean(np.abs(qam_scheme))\n",
    "    \n",
    "    return qam_symbols\n",
    "\n",
    "# Function to generate 4-ASK\n",
    "def generate_ask4(num_symbols, noise=50):\n",
    "    ask4_scheme = [3+0j, 1+0j, -1+0j, -3+0j]\n",
    "    ints = np.random.randint(0,4,num_symbols)\n",
    "    ask4_symbols = np.array([ask4_scheme[i] for i in ints])\n",
    "    ask4_symbols = ask4_symbols/np.mean(np.abs(ask4_scheme))\n",
    "    \n",
    "    return ask4_symbols\n",
    "\n",
    "# Function to generate 8-PSK\n",
    "def generate_psk8(num_symbols, noise=50):\n",
    "    psk8_scheme = [ 1+0j, 0.7071+0.7071j, 0+1j, -0.7071+0.7071j, \\\n",
    "                   -1+0j, -0.7071-0.7071j, 0-1j, 0.7071-0.7071j]\n",
    "    \n",
    "    ints = np.random.randint(0,8,num_symbols)\n",
    "    psk8_symbols = np.array([psk8_scheme[i] for i in ints])\n",
    "    psk8_symbols = psk8_symbols/np.mean(np.abs(psk8_scheme))\n",
    "    \n",
    "    return psk8_symbols\n",
    "\n",
    "def gen_tensor_data(mod_scheme, num_frames=32, samples_per_frame=128, sps=5, snr=30):\n",
    "            \n",
    "    symbols_required = int(np.ceil(samples_per_frame/sps))*num_frames\n",
    "        \n",
    "    # Mod scheme has to be one of: 'BPSK', 'QPSK', '16-QAM'\n",
    "    if mod_scheme == 'BPSK':\n",
    "        symbols = pulse_shape(generate_bpsk(symbols_required), sps=sps)[:num_frames*samples_per_frame]\n",
    "    elif mod_scheme == 'QPSK':\n",
    "        symbols = pulse_shape(generate_qpsk(symbols_required), sps=sps)[:num_frames*samples_per_frame]\n",
    "    elif mod_scheme == '8-PSK':\n",
    "        symbols = pulse_shape(generate_psk8(symbols_required), sps=sps)[:num_frames*samples_per_frame]\n",
    "    elif mod_scheme == '16-QAM':\n",
    "        symbols = pulse_shape(generate_qam(symbols_required), sps=sps)[:num_frames*samples_per_frame]\n",
    "    elif mod_scheme == '4-ASK':\n",
    "        symbols = pulse_shape(generate_ask4(symbols_required), sps=sps)[:num_frames*samples_per_frame]\n",
    "\n",
    "    # Add noise and split into frames\n",
    "    frames, sn = awgn(symbols.reshape(num_frames,-1), snr, measured=True, return_true_snr=True)\n",
    "\n",
    "    # Normalize to unit energy per frame\n",
    "    for i, frame in enumerate(frames):\n",
    "        power = np.mean((np.abs(frame)))\n",
    "        frames[i] = frame / power\n",
    "\n",
    "    # Split into I/Q, add extra channel to make a 4-D tensor\n",
    "    return torch.FloatTensor(np.stack((frames.real, frames.imag),axis=1)), sn\n",
    "\n",
    "def gen_data_from_list(mod_scheme, snr_range, num_frames=32, samples_per_frame=128):\n",
    "    \n",
    "    # total dataset size\n",
    "    frames = torch.zeros((num_frames*len(snr_range), 2, samples_per_frame), dtype=torch.float)\n",
    "    \n",
    "    # snr labels for multitask\n",
    "    snrs_db = torch.zeros(num_frames*len(snr_range), dtype=torch.float)\n",
    "    snrs_linear = torch.zeros(num_frames*len(snr_range), dtype=torch.float)\n",
    "    \n",
    "    for i, snr in enumerate(snr_range):\n",
    "        frames[i*num_frames:(i+1)*num_frames], sn = gen_tensor_data(mod_scheme, num_frames=num_frames, samples_per_frame=samples_per_frame, snr=snr)\n",
    "        snrs_db[i*num_frames:(i+1)*num_frames] = snr\n",
    "        snrs_linear[i*num_frames:(i+1)*num_frames] = sn\n",
    "    \n",
    "    return frames, (snrs_db, snrs_linear)\n",
    "\n",
    "# Function returns a torch dataloader with specified batch_size and num_frames\n",
    "# number of examples per snr level\n",
    "def gen_loader(num_frames=32, samples_per_frame=1024, snr=[30], batch_size=32):\n",
    "    \n",
    "    # Generate the individual waveforms for each modulation scheme\n",
    "    bpsk_data, (bpsk_snrs_db, bpsk_snrs_linear) = gen_data_from_list('BPSK', snr, num_frames=num_frames, samples_per_frame=samples_per_frame)\n",
    "    qpsk_data, (qpsk_snrs_db, qpsk_snrs_linear) = gen_data_from_list('QPSK', snr, num_frames=num_frames, samples_per_frame=samples_per_frame)\n",
    "    psk_data, (psk_snrs_db, psk_snrs_linear) = gen_data_from_list('8-PSK', snr, num_frames=num_frames, samples_per_frame=samples_per_frame)\n",
    "    qam_data, (qam_snrs_db, qam_snrs_linear) = gen_data_from_list('16-QAM', snr, num_frames=num_frames, samples_per_frame=samples_per_frame)\n",
    "    ask_data, (ask_snrs_db, ask_snrs_linear) = gen_data_from_list('4-ASK', snr, num_frames=num_frames, samples_per_frame=samples_per_frame)\n",
    "    \n",
    "    # Concat them into a single training data tensor\n",
    "    train_data = torch.cat((bpsk_data, qpsk_data, psk_data, qam_data, ask_data))\n",
    "    \n",
    "    # Create class labels\n",
    "    bpsk_labels = torch.zeros(bpsk_data.shape[0])\n",
    "    qpsk_labels = torch.ones(qpsk_data.shape[0])\n",
    "    psk_labels = torch.ones(psk_data.shape[0])*2\n",
    "    qam_labels = torch.ones(qam_data.shape[0])*3\n",
    "    ask_labels = torch.ones(ask_data.shape[0])*4\n",
    "\n",
    "    # Concat class labels\n",
    "    # We will be using cross entropy loss, which expects a long tensor as the label hence the .long() here\n",
    "    train_labels = torch.cat((bpsk_labels, qpsk_labels, psk_labels, qam_labels, ask_labels)).long()\n",
    "    \n",
    "    # SNR labels\n",
    "    snr_labels_db = torch.cat((bpsk_snrs_db, qpsk_snrs_db, psk_snrs_db, qam_snrs_db, ask_snrs_db))\n",
    "    snr_labels_linear = torch.cat((bpsk_snrs_linear, qpsk_snrs_linear, psk_snrs_linear, qam_snrs_linear, ask_snrs_linear))\n",
    "    \n",
    "    train_snr_range = np.arange(-15,16,2, dtype=int)\n",
    "    snr_indexes = {index:value for index, value in zip(train_snr_range, np.arange(len(train_snr_range)))}\n",
    "    snr_labels_classes = torch.tensor(list(map(lambda label: snr_indexes[label], snr_labels_db.numpy().astype(int))), dtype=int)\n",
    "    \n",
    "    # if gpu\n",
    "    train_data = train_data.cuda()\n",
    "    train_labels = train_labels.cuda()\n",
    "    snr_labels_db = snr_labels_db.cuda()\n",
    "    snr_labels_linear = snr_labels_linear.cuda()\n",
    "    snr_labels_classes = snr_labels_classes.cuda()\n",
    "    \n",
    "    # Create a Torch dataset\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_data, train_labels, snr_labels_linear, snr_labels_db, snr_labels_classes)\n",
    "    \n",
    "    return torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72c2913d-4b51-4409-b886-1a4a1aa602af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class amc_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(amc_model, self).__init__()\n",
    "        \n",
    "        # 3 conv layers with a 9 sample wide kernel and padding so that the\n",
    "        # size of the output remains consistent with the input for each layer\n",
    "        self.convolutions = nn.Sequential(\n",
    "                    nn.Conv1d(2, 64, 3, padding=1),\n",
    "                    nn.MaxPool1d(2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(64, 64, 3, padding=1),\n",
    "                    nn.MaxPool1d(2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(64, 64, 3, padding=1),\n",
    "                    nn.MaxPool1d(2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(64, 64, 3, padding=1),\n",
    "                    nn.MaxPool1d(2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(64, 64, 3, padding=1),\n",
    "                    nn.MaxPool1d(2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(64, 64, 3, padding=1),\n",
    "                    nn.MaxPool1d(2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(64, 64, 3, padding=1),\n",
    "                    nn.MaxPool1d(2),\n",
    "                    nn.ReLU()\n",
    "                    )\n",
    "        \n",
    "        # 128 samples x 16 output filters x 2 channels (I/Q) = 4096\n",
    "        self.fc1 = nn.Linear(512, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Extract features with convolutional layers\n",
    "        x = self.convolutions(x)\n",
    "        \n",
    "        # Flatten so it's compatible with fully connected layers for classification\n",
    "        x = torch.flatten(x,1)\n",
    "#         print(x.shape)\n",
    "        \n",
    "#         First fully connected layer\n",
    "        x = F.selu(self.fc1(x))\n",
    "        \n",
    "        # Final layer responsible for classifying the 5 modulation schemes\n",
    "        x = F.selu(self.fc2(x))\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fbb8015-c67d-42fc-bb82-407d49bf529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, loss_fn, num_epochs=5, verbose=False):\n",
    "    losses, val_losses = [], []\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        \n",
    "        for x, y, _, _, _ in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_hat = model(x)\n",
    "            \n",
    "            loss = loss_fn(y_hat,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        losses.append(running_loss/len(train_loader))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            running_val_loss = 0\n",
    "            for x, y, _, _, _ in val_loader:\n",
    "                y_hat = model(x)\n",
    "                \n",
    "                val_loss = loss_fn(y_hat, y)\n",
    "                running_val_loss += val_loss.item()\n",
    "            val_losses.append(running_val_loss/len(val_loader))\n",
    "        \n",
    "        if val_losses[-1] < best_loss:\n",
    "            print(f'val_losses[-1] = {val_losses[-1]}, best_loss = {best_loss}, model saved at {epoch}')\n",
    "            saved_model = model.state_dict()\n",
    "            best_loss = val_losses[-1]\n",
    "            \n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Loss: {losses[-1]}, Val loss: {val_losses[-1]}\")\n",
    "            \n",
    "    model.load_state_dict(saved_model)\n",
    "    \n",
    "    return model, losses, val_losses\n",
    "\n",
    "def test_model(model, snr_range, samples_per_frame=128, num_frames=512):\n",
    "    accs = []\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval().cpu()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for snr in snr_range:\n",
    "\n",
    "            bpsk_data, _ = gen_tensor_data('BPSK', num_frames=num_frames, samples_per_frame=samples_per_frame, snr=snr)\n",
    "            qpsk_data, _ = gen_tensor_data('QPSK', num_frames=num_frames, samples_per_frame=samples_per_frame, snr=snr)\n",
    "            psk_data, _ = gen_tensor_data('8-PSK', num_frames=num_frames, samples_per_frame=samples_per_frame, snr=snr)\n",
    "            qam_data, _ = gen_tensor_data('16-QAM', num_frames=num_frames, samples_per_frame=samples_per_frame, snr=snr)\n",
    "            ask_data, _ = gen_tensor_data('4-ASK', num_frames=num_frames, samples_per_frame=samples_per_frame, snr=snr)\n",
    "\n",
    "            test_data = torch.cat((bpsk_data, qpsk_data, psk_data, qam_data, ask_data))\n",
    "\n",
    "            bpsk_labels = torch.zeros(bpsk_data.shape[0])\n",
    "            qpsk_labels = torch.ones(qpsk_data.shape[0])\n",
    "            psk_labels = torch.ones(qam_data.shape[0])*2\n",
    "            qam_labels = torch.ones(qam_data.shape[0])*3\n",
    "            ask_labels = torch.ones(ask_data.shape[0])*4\n",
    "\n",
    "            test_labels = torch.cat((bpsk_labels, qpsk_labels, psk_labels, qam_labels, ask_labels))\n",
    "\n",
    "            results = torch.argmax(model(test_data),axis=1)\n",
    "            accs.append(torch.sum(results == test_labels).float() / test_data.shape[0])\n",
    "            \n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc25ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2023)\n",
    "train_snr = np.arange(-15,16,2)\n",
    "\n",
    "train_loader = gen_loader(num_frames=64, snr=train_snr, batch_size=32) # turn back to 512 later\n",
    "val_loader = gen_loader(num_frames=64, snr=train_snr, batch_size=32) # turn back to 64 later\n",
    "\n",
    "dataset = {'train_loader': train_loader,\n",
    "           'val_loader': val_loader}\n",
    "\n",
    "torch.save(dataset, f\"data/amc_data_512_all_cases.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a91b6e9-96af-46ec-b6c4-ca8b9710794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load(f\"data/amc_data_512_all_cases.pt\")\n",
    "train_loader = dataset['train_loader']\n",
    "val_loader = dataset['val_loader']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ac2191d-c275-4476-b4ea-9241a74bb4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "val_losses[-1] = 1.0234725389629602, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.0069258406758308, best_loss = 1.0234725389629602, model saved at 2\n",
      "val_losses[-1] = 0.8618613950908184, best_loss = 1.0069258406758308, model saved at 3\n",
      "val_losses[-1] = 0.810238928347826, best_loss = 0.8618613950908184, model saved at 5\n",
      "val_losses[-1] = 0.7633293464779853, best_loss = 0.810238928347826, model saved at 6\n",
      "val_losses[-1] = 0.752984382212162, best_loss = 0.7633293464779853, model saved at 7\n",
      "val_losses[-1] = 0.71509401910007, best_loss = 0.752984382212162, model saved at 8\n",
      "val_losses[-1] = 0.6305076463147998, best_loss = 0.71509401910007, model saved at 9\n",
      "val_losses[-1] = 0.6105141328647733, best_loss = 0.6305076463147998, model saved at 10\n",
      "val_losses[-1] = 0.5832104706205428, best_loss = 0.6105141328647733, model saved at 12\n",
      "val_losses[-1] = 0.5761468101292848, best_loss = 0.5832104706205428, model saved at 15\n",
      "val_losses[-1] = 0.5478129600174725, best_loss = 0.5761468101292848, model saved at 16\n",
      "val_losses[-1] = 0.5126305348239839, best_loss = 0.5478129600174725, model saved at 17\n",
      "iter 1\n",
      "val_losses[-1] = 1.1681248243898152, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.0768196329474449, best_loss = 1.1681248243898152, model saved at 1\n",
      "val_losses[-1] = 0.9884368371218443, best_loss = 1.0768196329474449, model saved at 2\n",
      "val_losses[-1] = 0.8985297501087188, best_loss = 0.9884368371218443, model saved at 3\n",
      "val_losses[-1] = 0.7969383053481579, best_loss = 0.8985297501087188, model saved at 4\n",
      "val_losses[-1] = 0.7063218953087926, best_loss = 0.7969383053481579, model saved at 5\n",
      "val_losses[-1] = 0.6360106762498617, best_loss = 0.7063218953087926, model saved at 6\n",
      "val_losses[-1] = 0.616419188119471, best_loss = 0.6360106762498617, model saved at 8\n",
      "val_losses[-1] = 0.5898117484524846, best_loss = 0.616419188119471, model saved at 10\n",
      "val_losses[-1] = 0.5857166802510619, best_loss = 0.5898117484524846, model saved at 12\n",
      "val_losses[-1] = 0.533707370609045, best_loss = 0.5857166802510619, model saved at 13\n",
      "val_losses[-1] = 0.5174838257022202, best_loss = 0.533707370609045, model saved at 17\n",
      "iter 2\n",
      "val_losses[-1] = 1.236873548477888, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.0103459887206554, best_loss = 1.236873548477888, model saved at 1\n",
      "val_losses[-1] = 0.9764484073966742, best_loss = 1.0103459887206554, model saved at 2\n",
      "val_losses[-1] = 0.8504716981202364, best_loss = 0.9764484073966742, model saved at 4\n",
      "val_losses[-1] = 0.7918197125196457, best_loss = 0.8504716981202364, model saved at 5\n",
      "val_losses[-1] = 0.7564324881881476, best_loss = 0.7918197125196457, model saved at 6\n",
      "val_losses[-1] = 0.633511607721448, best_loss = 0.7564324881881476, model saved at 8\n",
      "val_losses[-1] = 0.58860076200217, best_loss = 0.633511607721448, model saved at 9\n",
      "val_losses[-1] = 0.5686679000034929, best_loss = 0.58860076200217, model saved at 12\n",
      "val_losses[-1] = 0.5426169628277421, best_loss = 0.5686679000034929, model saved at 14\n",
      "val_losses[-1] = 0.533953688852489, best_loss = 0.5426169628277421, model saved at 16\n",
      "val_losses[-1] = 0.5122137045487761, best_loss = 0.533953688852489, model saved at 22\n",
      "iter 3\n",
      "val_losses[-1] = 1.074348609149456, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.0158379901200534, best_loss = 1.074348609149456, model saved at 1\n",
      "val_losses[-1] = 1.0132273752242327, best_loss = 1.0158379901200534, model saved at 3\n",
      "val_losses[-1] = 0.9977208904922008, best_loss = 1.0132273752242327, model saved at 4\n",
      "val_losses[-1] = 0.8155098661780358, best_loss = 0.9977208904922008, model saved at 6\n",
      "val_losses[-1] = 0.774200115725398, best_loss = 0.8155098661780358, model saved at 8\n",
      "val_losses[-1] = 0.645134099945426, best_loss = 0.774200115725398, model saved at 9\n",
      "val_losses[-1] = 0.6269245848059655, best_loss = 0.645134099945426, model saved at 10\n",
      "val_losses[-1] = 0.6109795605763793, best_loss = 0.6269245848059655, model saved at 12\n",
      "val_losses[-1] = 0.6004213925451041, best_loss = 0.6109795605763793, model saved at 13\n",
      "val_losses[-1] = 0.5640524769201875, best_loss = 0.6004213925451041, model saved at 14\n",
      "val_losses[-1] = 0.543505086377263, best_loss = 0.5640524769201875, model saved at 20\n",
      "val_losses[-1] = 0.5426548792049288, best_loss = 0.543505086377263, model saved at 24\n",
      "iter 4\n",
      "val_losses[-1] = 1.0216159872710704, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.0088892307132482, best_loss = 1.0216159872710704, model saved at 4\n",
      "val_losses[-1] = 0.857683603093028, best_loss = 1.0088892307132482, model saved at 5\n",
      "val_losses[-1] = 0.8218991164118051, best_loss = 0.857683603093028, model saved at 6\n",
      "val_losses[-1] = 0.736322694644332, best_loss = 0.8218991164118051, model saved at 7\n",
      "val_losses[-1] = 0.7286264533177018, best_loss = 0.736322694644332, model saved at 8\n",
      "val_losses[-1] = 0.6936107633635402, best_loss = 0.7286264533177018, model saved at 9\n",
      "val_losses[-1] = 0.6460077719762921, best_loss = 0.6936107633635402, model saved at 10\n",
      "val_losses[-1] = 0.6367215268313885, best_loss = 0.6460077719762921, model saved at 11\n",
      "val_losses[-1] = 0.5933240920305252, best_loss = 0.6367215268313885, model saved at 12\n",
      "val_losses[-1] = 0.5573491742834449, best_loss = 0.5933240920305252, model saved at 14\n",
      "val_losses[-1] = 0.5551753064617515, best_loss = 0.5573491742834449, model saved at 16\n",
      "val_losses[-1] = 0.5338831981644034, best_loss = 0.5551753064617515, model saved at 17\n",
      "iter 5\n",
      "val_losses[-1] = 1.0454847592860461, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.0094206877052785, best_loss = 1.0454847592860461, model saved at 1\n",
      "val_losses[-1] = 1.0056117303669452, best_loss = 1.0094206877052785, model saved at 2\n",
      "val_losses[-1] = 0.8333305310457945, best_loss = 1.0056117303669452, model saved at 5\n",
      "val_losses[-1] = 0.7977941740304232, best_loss = 0.8333305310457945, model saved at 7\n",
      "val_losses[-1] = 0.6293081995099783, best_loss = 0.7977941740304232, model saved at 9\n",
      "val_losses[-1] = 0.6267777735367417, best_loss = 0.6293081995099783, model saved at 10\n",
      "val_losses[-1] = 0.604730392433703, best_loss = 0.6267777735367417, model saved at 12\n",
      "val_losses[-1] = 0.5632871527224779, best_loss = 0.604730392433703, model saved at 13\n",
      "val_losses[-1] = 0.5378312765620649, best_loss = 0.5632871527224779, model saved at 16\n",
      "val_losses[-1] = 0.5313260290771723, best_loss = 0.5378312765620649, model saved at 18\n",
      "val_losses[-1] = 0.5243113288655877, best_loss = 0.5313260290771723, model saved at 19\n",
      "val_losses[-1] = 0.5237683601677418, best_loss = 0.5243113288655877, model saved at 20\n",
      "iter 6\n",
      "val_losses[-1] = 1.1425435688346623, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.121646998077631, best_loss = 1.1425435688346623, model saved at 1\n",
      "val_losses[-1] = 1.0148659404367208, best_loss = 1.121646998077631, model saved at 2\n",
      "val_losses[-1] = 0.9480205859988928, best_loss = 1.0148659404367208, model saved at 4\n",
      "val_losses[-1] = 0.8622599743306637, best_loss = 0.9480205859988928, model saved at 5\n",
      "val_losses[-1] = 0.85973070114851, best_loss = 0.8622599743306637, model saved at 6\n",
      "val_losses[-1] = 0.7131891071796417, best_loss = 0.85973070114851, model saved at 7\n",
      "val_losses[-1] = 0.6588334115222096, best_loss = 0.7131891071796417, model saved at 8\n",
      "val_losses[-1] = 0.630600268766284, best_loss = 0.6588334115222096, model saved at 10\n",
      "val_losses[-1] = 0.6194281404837966, best_loss = 0.630600268766284, model saved at 11\n",
      "val_losses[-1] = 0.5893419984728098, best_loss = 0.6194281404837966, model saved at 12\n",
      "val_losses[-1] = 0.541579256951809, best_loss = 0.5893419984728098, model saved at 16\n",
      "val_losses[-1] = 0.5322024593129754, best_loss = 0.541579256951809, model saved at 19\n",
      "val_losses[-1] = 0.5286038440652192, best_loss = 0.5322024593129754, model saved at 22\n",
      "iter 7\n",
      "val_losses[-1] = 1.016641241312027, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.009492529183626, best_loss = 1.016641241312027, model saved at 2\n",
      "val_losses[-1] = 0.9985546559095383, best_loss = 1.009492529183626, model saved at 3\n",
      "val_losses[-1] = 0.9958897929638624, best_loss = 0.9985546559095383, model saved at 4\n",
      "val_losses[-1] = 0.8355271521955728, best_loss = 0.9958897929638624, model saved at 6\n",
      "val_losses[-1] = 0.8323044657707215, best_loss = 0.8355271521955728, model saved at 7\n",
      "val_losses[-1] = 0.7363124020397663, best_loss = 0.8323044657707215, model saved at 8\n",
      "val_losses[-1] = 0.6984563333913684, best_loss = 0.7363124020397663, model saved at 9\n",
      "val_losses[-1] = 0.6933743469417095, best_loss = 0.6984563333913684, model saved at 10\n",
      "val_losses[-1] = 0.5973631316795945, best_loss = 0.6933743469417095, model saved at 11\n",
      "val_losses[-1] = 0.5709193592891098, best_loss = 0.5973631316795945, model saved at 13\n",
      "val_losses[-1] = 0.5638182463124395, best_loss = 0.5709193592891098, model saved at 15\n",
      "val_losses[-1] = 0.5550637232139707, best_loss = 0.5638182463124395, model saved at 20\n",
      "val_losses[-1] = 0.540221362374723, best_loss = 0.5550637232139707, model saved at 21\n",
      "val_losses[-1] = 0.5217558013275265, best_loss = 0.540221362374723, model saved at 25\n",
      "iter 8\n",
      "val_losses[-1] = 1.2028893817216157, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.1059129562228918, best_loss = 1.2028893817216157, model saved at 1\n",
      "val_losses[-1] = 0.9959879364818335, best_loss = 1.1059129562228918, model saved at 2\n",
      "val_losses[-1] = 0.8948374576866627, best_loss = 0.9959879364818335, model saved at 4\n",
      "val_losses[-1] = 0.7994543761014938, best_loss = 0.8948374576866627, model saved at 5\n",
      "val_losses[-1] = 0.6923029402270913, best_loss = 0.7994543761014938, model saved at 6\n",
      "val_losses[-1] = 0.6335799215361476, best_loss = 0.6923029402270913, model saved at 7\n",
      "val_losses[-1] = 0.6022169332951307, best_loss = 0.6335799215361476, model saved at 8\n",
      "val_losses[-1] = 0.5929722720757127, best_loss = 0.6022169332951307, model saved at 10\n",
      "val_losses[-1] = 0.5618523280136287, best_loss = 0.5929722720757127, model saved at 13\n",
      "val_losses[-1] = 0.5254316524602473, best_loss = 0.5618523280136287, model saved at 18\n",
      "iter 9\n",
      "val_losses[-1] = 1.1332007218152285, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.0396616961807013, best_loss = 1.1332007218152285, model saved at 1\n",
      "val_losses[-1] = 0.9864275120198727, best_loss = 1.0396616961807013, model saved at 3\n",
      "val_losses[-1] = 0.8654788251966238, best_loss = 0.9864275120198727, model saved at 5\n",
      "val_losses[-1] = 0.8505426064133644, best_loss = 0.8654788251966238, model saved at 6\n",
      "val_losses[-1] = 0.8404903445392847, best_loss = 0.8505426064133644, model saved at 7\n",
      "val_losses[-1] = 0.7838456086814404, best_loss = 0.8404903445392847, model saved at 8\n",
      "val_losses[-1] = 0.6732359953224659, best_loss = 0.7838456086814404, model saved at 10\n",
      "val_losses[-1] = 0.612350295484066, best_loss = 0.6732359953224659, model saved at 11\n",
      "val_losses[-1] = 0.5991871738806367, best_loss = 0.612350295484066, model saved at 12\n",
      "val_losses[-1] = 0.5699483720585704, best_loss = 0.5991871738806367, model saved at 14\n",
      "val_losses[-1] = 0.5528802809305489, best_loss = 0.5699483720585704, model saved at 17\n",
      "val_losses[-1] = 0.5427561369724572, best_loss = 0.5528802809305489, model saved at 23\n",
      "val_losses[-1] = 0.5423162763006986, best_loss = 0.5427561369724572, model saved at 25\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "num_iter = 10\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "models = []\n",
    "for i in range(num_iter):\n",
    "    print(f\"iter {i}\")\n",
    "    \n",
    "    dataset = torch.load(f\"data/amc_data_512_all_cases.pt\")\n",
    "    train_loader = dataset['train_loader']\n",
    "    val_loader = dataset['val_loader']\n",
    "    \n",
    "    torch.manual_seed(i)\n",
    "    model = amc_model()\n",
    "    model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0001)\n",
    "\n",
    "    model, losses, val_losses = \\\n",
    "    train(model, optimizer, train_loader, val_loader, loss_fn, num_epochs=num_epochs, verbose=False)\n",
    "\n",
    "    model_config = {\"weights\": model.state_dict(),\n",
    "                    \"losses\": losses,\n",
    "                    \"val_losses\": val_losses}\n",
    "\n",
    "    models.append(model_config)\n",
    "torch.save(models, f'baselines/models/amc_baseline.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "071eb6da-1207-492a-bdb7-576a2a8892f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_range = np.arange(-15,16,2)\n",
    "\n",
    "results = []\n",
    "for model_config in torch.load(f'baselines/models/amc_baseline.pt'):\n",
    "    model = amc_model()\n",
    "    model.load_state_dict(model_config['weights'])\n",
    "    accs = np.array(test_model(model, snr_range, samples_per_frame=1024, num_frames=32)) # change to num_frames=256 later\n",
    "\n",
    "    result = {\"accs_mod\": accs,\n",
    "              \"snr_range\": snr_range,\n",
    "              \"model\": model_config}\n",
    "    results.append(result)\n",
    "torch.save(results, f'baselines/results/amc_baseline.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adc35ed5-dd4d-47f6-a427-44152ae90c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc4800b7940>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for result in torch.load(f'baselines/results/amc_baseline.pt'):\n",
    "    plt.plot(result['accs_mod'])\n",
    "plt.plot(np.mean([result['accs_mod'] for result in torch.load(f'baselines/results/amc_baseline.pt')], axis=0), 'k--', linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6f3876d-a4d5-47c4-80ca-62c504d32bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_config in torch.load(f'baselines/models/amc_baseline.pt'):\n",
    "    plt.plot(model_config['val_losses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5900df74-504b-4d67-98fb-0e004036339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = gen_loader(num_frames=32, snr=np.arange(-15,16,2), batch_size=32)\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "y_test = np.zeros(len(test_loader)*32,)\n",
    "y_hats = np.zeros(len(test_loader)*32,)\n",
    "\n",
    "for i, (x,y,_,_,_) in enumerate(test_loader):\n",
    "    y_hats[32*i:32*(i+1)] = model(x).argmax(axis=1).detach().cpu()\n",
    "    y_test[32*i:32*(i+1)] = y.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76eaf100-a933-494e-8ba5-a593b5108583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7fc47b3504c0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import confusion matrix function and use to get our cm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_hats, y_test) # y_test, and y_hat come from latest test loop above\n",
    "\n",
    "# Plot the confusion matrix, Blues cmap is probably most popular choice\n",
    "plt.imshow(cm, cmap='Blues')\n",
    "plt.xticks([0,1,2,3,4], ['bpsk', 'qpsk', '8-PSK', '16-qam', '4-ask'])\n",
    "plt.yticks([0,1,2,3,4], ['bpsk', 'qpsk', '8-PSK', '16-qam', '4-ask'])\n",
    "plt.ylabel('True Classes')\n",
    "plt.xlabel('Predicted Classes')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa9e4550-e04b-4d1f-9018-8d1176e00340",
   "metadata": {},
   "source": [
    "## Add MTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dd8dca6-9f96-4ec0-b601-a990aa22b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class amc_model_mtl(nn.Module):\n",
    "    def __init__(self, case=0):\n",
    "        super(amc_model_mtl, self).__init__()\n",
    "            \n",
    "        # 3 conv layers with a 9 sample wide kernel and padding so that the\n",
    "        # size of the output remains consistent with the input for each layer\n",
    "        self.convolutions = nn.Sequential(\n",
    "                    nn.Conv1d(2, 64, 3, padding=1),\n",
    "                    nn.MaxPool1d(2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(64, 64, 3, padding=1),\n",
    "                    nn.MaxPool1d(2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(64, 64, 3, padding=1),\n",
    "                    nn.MaxPool1d(2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(64, 64, 3, padding=1),\n",
    "                    nn.MaxPool1d(2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(64, 64, 3, padding=1),\n",
    "                    nn.MaxPool1d(2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(64, 64, 3, padding=1),\n",
    "                    nn.MaxPool1d(2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(64, 64, 3, padding=1),\n",
    "                    nn.MaxPool1d(2),\n",
    "                    nn.ReLU()\n",
    "                    )\n",
    "        \n",
    "        # Noise estimator network\n",
    "        # Case 0 -> estimate linear SNR, no negative values\n",
    "        if case == 0:\n",
    "            self.noise_estimator = nn.Sequential(\n",
    "                                        nn.Linear(512,512),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(512,128),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(128,64),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(64,1),\n",
    "                                        nn.ReLU())\n",
    "        # Case 1 -> estimate SNR dBs, negative and positive possible\n",
    "        elif case == 1:\n",
    "            self.noise_estimator = nn.Sequential(\n",
    "                                        nn.Linear(512,512),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(512,128),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(128,64),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(64,1))\n",
    "        \n",
    "        # Case 2 -> 16 SNR levels = 16 classes going into CrossEntropyLoss softmax\n",
    "        elif case == 2:\n",
    "            self.noise_estimator = nn.Sequential(\n",
    "                                        nn.Linear(512,512),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(512,128),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(128,64),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(64,16))\n",
    "        \n",
    "        # 128 samples x 16 output filters x 2 channels (I/Q) = 4096\n",
    "        self.fc1 = nn.Linear(512, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Extract features with convolutional layers\n",
    "        x = self.convolutions(x)\n",
    "        \n",
    "        # Flatten so it's compatible with fully connected layers for classification\n",
    "        x_shared = torch.flatten(x,1)\n",
    "#         print(x.shape)\n",
    "        \n",
    "#         First fully connected layer\n",
    "        y = F.selu(self.fc1(x_shared))\n",
    "        \n",
    "        # Final layer responsible for classifying the 5 modulation schemes\n",
    "        y = F.selu(self.fc2(y))\n",
    "        \n",
    "        y = self.fc3(y)\n",
    "        \n",
    "        n = self.noise_estimator(x_shared).squeeze()\n",
    "        \n",
    "        return y, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88bb0346-2b30-4cd4-ac83-436bf314f043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mtl(model, optimizer, train_loader, val_loader, loss_fns, loss_ratios=(0.5, 0.5),\n",
    "          num_epochs=5, verbose=False, case=0):\n",
    "    \n",
    "    loss_fn_amc = loss_fns[0]\n",
    "    loss_fn_snr = loss_fns[1]\n",
    "    \n",
    "    losses, val_losses = [], []\n",
    "    \n",
    "    losses_mod, val_losses_mod = [], []\n",
    "    losses_snr, val_losses_snr = [], []\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        running_mod_loss, running_snr_loss = 0, 0\n",
    "        \n",
    "        for x, y, *z_all in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_hat, z_hat = model(x)\n",
    "            \n",
    "            loss_1 = loss_fn_amc(y_hat, y)\n",
    "            loss_2 = loss_fn_snr(z_hat, z_all[case])\n",
    "            loss = loss_1*loss_ratios[0] + loss_2*loss_ratios[1]\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            running_mod_loss += loss_1.item()\n",
    "            running_snr_loss += loss_2.item()\n",
    "            \n",
    "        losses.append(running_loss/len(train_loader))\n",
    "        losses_mod.append(running_mod_loss/len(train_loader))\n",
    "        losses_snr.append(running_snr_loss/len(train_loader))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            running_val_loss = 0\n",
    "            running_snr_val_loss = 0\n",
    "            for x, y, z_all in val_loader:\n",
    "                y_hat, z_hat = model(x)\n",
    "                \n",
    "                val_loss = loss_fn_amc(y_hat, y)\n",
    "                running_val_loss += val_loss.item()\n",
    "                \n",
    "                snr_val_loss = loss_fn_snr(z_hat, z_all[case])\n",
    "                running_snr_val_loss += snr_val_loss.item()\n",
    "                \n",
    "            val_losses.append(running_val_loss/len(val_loader))\n",
    "            val_losses_snr.append(running_snr_val_loss/len(val_loader))\n",
    "        \n",
    "        if val_losses[-1] < best_loss:\n",
    "            print(f'val_losses[-1] = {val_losses[-1]}, best_loss = {best_loss}, model saved at {epoch}')\n",
    "            saved_model = model.state_dict()\n",
    "            best_loss = val_losses[-1]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Loss: {losses[-1]}, Val loss: {val_losses[-1]}\")\n",
    "            \n",
    "    model.load_state_dict(saved_model)\n",
    "    \n",
    "    return model, losses, losses_mod, losses_snr, val_losses, val_losses_snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33ae96f3-96b7-4749-abb7-9073555b4c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss_ratios = []\n",
    "for i in range(1,10):\n",
    "    models = []\n",
    "\n",
    "    amc_weight = 1\n",
    "    snr_weight = round(1 - i*0.1, 1)\n",
    "    \n",
    "    loss_ratios = (amc_weight, snr_weight)\n",
    "    \n",
    "    all_loss_ratios.append(loss_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3657d3b2-75a3-4a3b-9a88-d3d31e29b849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.9),\n",
       " (1, 0.8),\n",
       " (1, 0.7),\n",
       " (1, 0.6),\n",
       " (1, 0.5),\n",
       " (1, 0.4),\n",
       " (1, 0.3),\n",
       " (1, 0.2),\n",
       " (1, 0.1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_loss_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec369453-fd30-4e5d-a7b7-5d81ba4fbb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.9)\n",
      "iter 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-f00080d4c4bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         model, losses, losses_mod, losses_snr, val_losses, val_losses_snr = train_mtl(model_mtl, optimizer, train_loader, \n\u001b[0m\u001b[1;32m     22\u001b[0m                                                                                       \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                                                       \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-6d94decfb740>\u001b[0m in \u001b[0;36mtrain_mtl\u001b[0;34m(model, optimizer, train_loader, val_loader, loss_fns, loss_ratios, num_epochs, verbose, case)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn_amc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn_snr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_ratios\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_ratios\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hdd/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hdd/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hdd/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3111\u001b[0m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3112\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "loss_fns = (nn.CrossEntropyLoss(), nn.MSELoss())\n",
    "num_epochs = 5\n",
    "num_iter = 2\n",
    "\n",
    "for loss_ratios in all_loss_ratios:\n",
    "    print(loss_ratios)\n",
    "    \n",
    "    models = []\n",
    "    for j in range(num_iter):\n",
    "        print(f\"iter {j}\")\n",
    "        dataset = torch.load(f\"data/amc_data_512_all_cases.pt\")\n",
    "        train_loader = dataset['train_loader']\n",
    "        val_loader = dataset['val_loader']\n",
    "        \n",
    "        torch.manual_seed(j)\n",
    "        model_mtl = amc_model_mtl()\n",
    "        model_mtl.cuda()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model_mtl.parameters(), lr=0.0005, weight_decay=0.0001)\n",
    "\n",
    "        model, losses, losses_mod, losses_snr, val_losses, val_losses_snr = train_mtl(model_mtl, optimizer, train_loader, \n",
    "                                                                                      val_loader, loss_fns, \n",
    "                                                                                      num_epochs=num_epochs, verbose=False, \n",
    "                                                                                      loss_ratios=loss_ratios, case = 0)\n",
    "        \n",
    "        model_config = {\"weights\": model_mtl.state_dict(),\n",
    "                        \"losses\": losses,\n",
    "                        \"val_losses\": val_losses,\n",
    "                        \"losses_mod\": losses_mod,\n",
    "                        \"losses_snr\": losses_snr,\n",
    "                        \"val_losses_snr\": val_losses_snr}\n",
    "        \n",
    "        models.append(model_config)\n",
    "    torch.save(models, f'models/case_0/amc_{loss_ratios[0]}_{loss_ratios[1]}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "490cd119-93b0-486f-bcf9-56497eee2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_mtl(model, snr_range, num_frames=128, samples_per_frame=1024):\n",
    "    accs = []\n",
    "    snr_errs = []\n",
    "\n",
    "    model.eval().cpu()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for snr in snr_range:\n",
    "\n",
    "            bpsk_data, _ = gen_tensor_data('BPSK', num_frames=num_frames, samples_per_frame=samples_per_frame, snr=snr)\n",
    "            qpsk_data, _ = gen_tensor_data('QPSK', num_frames=num_frames, samples_per_frame=samples_per_frame, snr=snr)\n",
    "            psk_data, _ = gen_tensor_data('8-PSK', num_frames=num_frames, samples_per_frame=samples_per_frame, snr=snr)\n",
    "            qam_data, _ = gen_tensor_data('16-QAM', num_frames=num_frames, samples_per_frame=samples_per_frame, snr=snr)\n",
    "            ask_data, _ = gen_tensor_data('4-ASK', num_frames=num_frames, samples_per_frame=samples_per_frame, snr=snr)\n",
    "\n",
    "            test_data = torch.cat((bpsk_data, qpsk_data, psk_data, qam_data, ask_data))\n",
    "\n",
    "            bpsk_labels = torch.zeros(bpsk_data.shape[0])\n",
    "            qpsk_labels = torch.ones(qpsk_data.shape[0])\n",
    "            psk_labels = torch.ones(qam_data.shape[0])*2\n",
    "            qam_labels = torch.ones(qam_data.shape[0])*3\n",
    "            ask_labels = torch.ones(ask_data.shape[0])*4\n",
    "\n",
    "            test_labels = torch.cat((bpsk_labels, qpsk_labels, psk_labels, qam_labels, ask_labels))\n",
    "\n",
    "            y_hat, snr_hat = model(test_data)\n",
    "            \n",
    "            results = torch.argmax(y_hat, axis=1)\n",
    "            \n",
    "            accs.append(torch.sum(results == test_labels).float() / test_data.shape[0])\n",
    "            \n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0617de5f-6f76-46a8-b0dc-b64a09b74b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.9)\n",
      "(1, 0.8)\n",
      "(1, 0.7)\n",
      "(1, 0.6)\n",
      "(1, 0.5)\n"
     ]
    }
   ],
   "source": [
    "snr_range = np.arange(-15,16,2)\n",
    "\n",
    "# for models in sorted(os.listdir('models')):\n",
    "for loss_ratios in all_loss_ratios:\n",
    "    print(loss_ratios)\n",
    "    results = []\n",
    "    for model in torch.load(f'models/case_0/amc_{loss_ratios[0]}_{loss_ratios[1]}.pt'):\n",
    "        model_mtl = amc_model_mtl()\n",
    "        model_mtl.load_state_dict(model['weights'])\n",
    "        accs_mod = test_model_mtl(model_mtl, snr_range, num_frames=128)\n",
    "        \n",
    "        result = {\"accs_mod\": accs_mod,\n",
    "                   \"snr_range\": snr_range,\n",
    "                   \"model\": model}\n",
    "        results.append(result)\n",
    "    torch.save(results, f'results/case_0/amc_{loss_ratios[0]}_{loss_ratios[1]}.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8374c98-4563-43c0-b782-843db28e8f67",
   "metadata": {},
   "source": [
    "## Add MTL, case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e3d3c81-10d8-42ef-8f16-7df00855ade2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.9)\n",
      "iter 0\n",
      "val_losses[-1] = 1.441458959132433, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.971820030361414, best_loss = 1.441458959132433, model saved at 1\n",
      "val_losses[-1] = 0.7484014149755239, best_loss = 0.971820030361414, model saved at 2\n",
      "val_losses[-1] = 0.7161662861704826, best_loss = 0.7484014149755239, model saved at 3\n",
      "val_losses[-1] = 0.6407490365207196, best_loss = 0.7161662861704826, model saved at 4\n",
      "val_losses[-1] = 0.6119059583172202, best_loss = 0.6407490365207196, model saved at 5\n",
      "val_losses[-1] = 0.5840258114039898, best_loss = 0.6119059583172202, model saved at 6\n",
      "val_losses[-1] = 0.574721753038466, best_loss = 0.5840258114039898, model saved at 7\n",
      "val_losses[-1] = 0.527362172678113, best_loss = 0.574721753038466, model saved at 9\n",
      "val_losses[-1] = 0.5004938848316669, best_loss = 0.527362172678113, model saved at 10\n",
      "val_losses[-1] = 0.4578455202281475, best_loss = 0.5004938848316669, model saved at 14\n",
      "val_losses[-1] = 0.4421844758559018, best_loss = 0.4578455202281475, model saved at 17\n",
      "val_losses[-1] = 0.43409531507641075, best_loss = 0.4421844758559018, model saved at 24\n",
      "val_losses[-1] = 0.4310732212848961, best_loss = 0.43409531507641075, model saved at 27\n",
      "iter 1\n",
      "val_losses[-1] = 1.2065691877156497, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8369298808276653, best_loss = 1.2065691877156497, model saved at 1\n",
      "val_losses[-1] = 0.7451147709041834, best_loss = 0.8369298808276653, model saved at 3\n",
      "val_losses[-1] = 0.6519248289987445, best_loss = 0.7451147709041834, model saved at 4\n",
      "val_losses[-1] = 0.5400918150320649, best_loss = 0.6519248289987445, model saved at 6\n",
      "val_losses[-1] = 0.4823295554146171, best_loss = 0.5400918150320649, model saved at 8\n",
      "val_losses[-1] = 0.4636588651686907, best_loss = 0.4823295554146171, model saved at 11\n",
      "val_losses[-1] = 0.4530723393894732, best_loss = 0.4636588651686907, model saved at 13\n",
      "val_losses[-1] = 0.4477542256936431, best_loss = 0.4530723393894732, model saved at 14\n",
      "val_losses[-1] = 0.43589276894927026, best_loss = 0.4477542256936431, model saved at 18\n",
      "val_losses[-1] = 0.4347468839026988, best_loss = 0.43589276894927026, model saved at 28\n",
      "iter 2\n",
      "val_losses[-1] = 1.6118403904139995, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8851194527000189, best_loss = 1.6118403904139995, model saved at 1\n",
      "val_losses[-1] = 0.7630913950502872, best_loss = 0.8851194527000189, model saved at 2\n",
      "val_losses[-1] = 0.7506432821974158, best_loss = 0.7630913950502872, model saved at 4\n",
      "val_losses[-1] = 0.62507827244699, best_loss = 0.7506432821974158, model saved at 5\n",
      "val_losses[-1] = 0.5866445673629641, best_loss = 0.62507827244699, model saved at 7\n",
      "val_losses[-1] = 0.5318067716434598, best_loss = 0.5866445673629641, model saved at 11\n",
      "val_losses[-1] = 0.4915599306114018, best_loss = 0.5318067716434598, model saved at 12\n",
      "val_losses[-1] = 0.47245982894673944, best_loss = 0.4915599306114018, model saved at 13\n",
      "val_losses[-1] = 0.45291751390323043, best_loss = 0.47245982894673944, model saved at 18\n",
      "val_losses[-1] = 0.4512118902988732, best_loss = 0.45291751390323043, model saved at 21\n",
      "val_losses[-1] = 0.4469877382740378, best_loss = 0.4512118902988732, model saved at 22\n",
      "val_losses[-1] = 0.4309975293464959, best_loss = 0.4469877382740378, model saved at 27\n",
      "iter 3\n",
      "val_losses[-1] = 1.603373397141695, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.26770511418581, best_loss = 1.603373397141695, model saved at 1\n",
      "val_losses[-1] = 0.8342394832521677, best_loss = 1.26770511418581, model saved at 2\n",
      "val_losses[-1] = 0.8278367914259434, best_loss = 0.8342394832521677, model saved at 3\n",
      "val_losses[-1] = 0.680289932154119, best_loss = 0.8278367914259434, model saved at 4\n",
      "val_losses[-1] = 0.6479088759049774, best_loss = 0.680289932154119, model saved at 5\n",
      "val_losses[-1] = 0.5948920782655478, best_loss = 0.6479088759049774, model saved at 6\n",
      "val_losses[-1] = 0.5120016905479133, best_loss = 0.5948920782655478, model saved at 8\n",
      "val_losses[-1] = 0.474077710416168, best_loss = 0.5120016905479133, model saved at 11\n",
      "val_losses[-1] = 0.4686712212860584, best_loss = 0.474077710416168, model saved at 14\n",
      "val_losses[-1] = 0.45380830205976963, best_loss = 0.4686712212860584, model saved at 16\n",
      "val_losses[-1] = 0.4477460809983313, best_loss = 0.45380830205976963, model saved at 19\n",
      "iter 4\n",
      "val_losses[-1] = 1.467377483844757, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.9990019552409649, best_loss = 1.467377483844757, model saved at 1\n",
      "val_losses[-1] = 0.7924758043140173, best_loss = 0.9990019552409649, model saved at 2\n",
      "val_losses[-1] = 0.7812780762091279, best_loss = 0.7924758043140173, model saved at 3\n",
      "val_losses[-1] = 0.7386119224131107, best_loss = 0.7812780762091279, model saved at 4\n",
      "val_losses[-1] = 0.6538483066484332, best_loss = 0.7386119224131107, model saved at 5\n",
      "val_losses[-1] = 0.6134062148630619, best_loss = 0.6538483066484332, model saved at 6\n",
      "val_losses[-1] = 0.6041960878297686, best_loss = 0.6134062148630619, model saved at 7\n",
      "val_losses[-1] = 0.55773662077263, best_loss = 0.6041960878297686, model saved at 8\n",
      "val_losses[-1] = 0.5294381261803209, best_loss = 0.55773662077263, model saved at 9\n",
      "val_losses[-1] = 0.4775606752373278, best_loss = 0.5294381261803209, model saved at 11\n",
      "val_losses[-1] = 0.45746817272156476, best_loss = 0.4775606752373278, model saved at 12\n",
      "val_losses[-1] = 0.45262272711843254, best_loss = 0.45746817272156476, model saved at 16\n",
      "val_losses[-1] = 0.44507154282182454, best_loss = 0.45262272711843254, model saved at 22\n",
      "val_losses[-1] = 0.43469662871211767, best_loss = 0.44507154282182454, model saved at 24\n",
      "val_losses[-1] = 0.4268490059301257, best_loss = 0.43469662871211767, model saved at 28\n",
      "(1, 0.8)\n",
      "iter 0\n",
      "val_losses[-1] = 1.539451827853918, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.9839578386396169, best_loss = 1.539451827853918, model saved at 1\n",
      "val_losses[-1] = 0.7737268969416619, best_loss = 0.9839578386396169, model saved at 2\n",
      "val_losses[-1] = 0.7276295833289623, best_loss = 0.7737268969416619, model saved at 3\n",
      "val_losses[-1] = 0.6585855390876532, best_loss = 0.7276295833289623, model saved at 4\n",
      "val_losses[-1] = 0.6414489848539233, best_loss = 0.6585855390876532, model saved at 5\n",
      "val_losses[-1] = 0.5883097518235445, best_loss = 0.6414489848539233, model saved at 6\n",
      "val_losses[-1] = 0.5137916320934892, best_loss = 0.5883097518235445, model saved at 8\n",
      "val_losses[-1] = 0.5091606019064784, best_loss = 0.5137916320934892, model saved at 9\n",
      "val_losses[-1] = 0.4965270522981882, best_loss = 0.5091606019064784, model saved at 10\n",
      "val_losses[-1] = 0.48575532604008914, best_loss = 0.4965270522981882, model saved at 12\n",
      "val_losses[-1] = 0.4639093725942075, best_loss = 0.48575532604008914, model saved at 14\n",
      "val_losses[-1] = 0.45702895177528263, best_loss = 0.4639093725942075, model saved at 15\n",
      "val_losses[-1] = 0.4412677283398807, best_loss = 0.45702895177528263, model saved at 17\n",
      "val_losses[-1] = 0.4388200841844082, best_loss = 0.4412677283398807, model saved at 25\n",
      "val_losses[-1] = 0.4387532946188003, best_loss = 0.4388200841844082, model saved at 26\n",
      "val_losses[-1] = 0.42909765476360917, best_loss = 0.4387532946188003, model saved at 27\n",
      "iter 1\n",
      "val_losses[-1] = 1.2497209165245295, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8271559979766607, best_loss = 1.2497209165245295, model saved at 1\n",
      "val_losses[-1] = 0.7687406282871961, best_loss = 0.8271559979766607, model saved at 3\n",
      "val_losses[-1] = 0.5988578401505947, best_loss = 0.7687406282871961, model saved at 4\n",
      "val_losses[-1] = 0.5801163677126169, best_loss = 0.5988578401505947, model saved at 6\n",
      "val_losses[-1] = 0.5413288188166916, best_loss = 0.5801163677126169, model saved at 7\n",
      "val_losses[-1] = 0.4743326151743531, best_loss = 0.5413288188166916, model saved at 8\n",
      "val_losses[-1] = 0.457181787211448, best_loss = 0.4743326151743531, model saved at 11\n",
      "val_losses[-1] = 0.44384250687435267, best_loss = 0.457181787211448, model saved at 13\n",
      "val_losses[-1] = 0.4368056905455887, best_loss = 0.44384250687435267, model saved at 18\n",
      "val_losses[-1] = 0.42795623978599906, best_loss = 0.4368056905455887, model saved at 20\n",
      "iter 2\n",
      "val_losses[-1] = 1.607857594639063, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.0826073084026575, best_loss = 1.607857594639063, model saved at 1\n",
      "val_losses[-1] = 0.8757015470415354, best_loss = 1.0826073084026575, model saved at 2\n",
      "val_losses[-1] = 0.7089599242433906, best_loss = 0.8757015470415354, model saved at 4\n",
      "val_losses[-1] = 0.6706059589982033, best_loss = 0.7089599242433906, model saved at 5\n",
      "val_losses[-1] = 0.6285196200013161, best_loss = 0.6706059589982033, model saved at 7\n",
      "val_losses[-1] = 0.600017799437046, best_loss = 0.6285196200013161, model saved at 8\n",
      "val_losses[-1] = 0.5428310973569751, best_loss = 0.600017799437046, model saved at 9\n",
      "val_losses[-1] = 0.5264931839890779, best_loss = 0.5428310973569751, model saved at 11\n",
      "val_losses[-1] = 0.48438114589080217, best_loss = 0.5264931839890779, model saved at 12\n",
      "val_losses[-1] = 0.4711502750404179, best_loss = 0.48438114589080217, model saved at 13\n",
      "val_losses[-1] = 0.46301424838602545, best_loss = 0.4711502750404179, model saved at 16\n",
      "val_losses[-1] = 0.45326772835105655, best_loss = 0.46301424838602545, model saved at 17\n",
      "val_losses[-1] = 0.43980701118707655, best_loss = 0.45326772835105655, model saved at 22\n",
      "val_losses[-1] = 0.4295516126789153, best_loss = 0.43980701118707655, model saved at 27\n",
      "iter 3\n",
      "val_losses[-1] = 1.5961070582270622, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.096156272664666, best_loss = 1.5961070582270622, model saved at 1\n",
      "val_losses[-1] = 0.7944150034338235, best_loss = 1.096156272664666, model saved at 2\n",
      "val_losses[-1] = 0.698326332308352, best_loss = 0.7944150034338235, model saved at 3\n",
      "val_losses[-1] = 0.6607150830328464, best_loss = 0.698326332308352, model saved at 4\n",
      "val_losses[-1] = 0.5783019740134477, best_loss = 0.6607150830328464, model saved at 6\n",
      "val_losses[-1] = 0.5639043901115656, best_loss = 0.5783019740134477, model saved at 7\n",
      "val_losses[-1] = 0.5209764142520725, best_loss = 0.5639043901115656, model saved at 8\n",
      "val_losses[-1] = 0.5152718957513571, best_loss = 0.5209764142520725, model saved at 10\n",
      "val_losses[-1] = 0.5000182037241757, best_loss = 0.5152718957513571, model saved at 11\n",
      "val_losses[-1] = 0.4930937680415809, best_loss = 0.5000182037241757, model saved at 12\n",
      "val_losses[-1] = 0.46153875924646853, best_loss = 0.4930937680415809, model saved at 15\n",
      "val_losses[-1] = 0.4488841426558793, best_loss = 0.46153875924646853, model saved at 17\n",
      "val_losses[-1] = 0.4296343468129635, best_loss = 0.4488841426558793, model saved at 20\n",
      "iter 4\n",
      "val_losses[-1] = 1.3419309005141258, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.9809491198509932, best_loss = 1.3419309005141258, model saved at 1\n",
      "val_losses[-1] = 0.7645765405148268, best_loss = 0.9809491198509932, model saved at 2\n",
      "val_losses[-1] = 0.7644649202004075, best_loss = 0.7645765405148268, model saved at 4\n",
      "val_losses[-1] = 0.6625872066244483, best_loss = 0.7644649202004075, model saved at 5\n",
      "val_losses[-1] = 0.5776323184370995, best_loss = 0.6625872066244483, model saved at 6\n",
      "val_losses[-1] = 0.5604400252923369, best_loss = 0.5776323184370995, model saved at 7\n",
      "val_losses[-1] = 0.5263772479258477, best_loss = 0.5604400252923369, model saved at 8\n",
      "val_losses[-1] = 0.4888208005577326, best_loss = 0.5263772479258477, model saved at 9\n",
      "val_losses[-1] = 0.4761270056478679, best_loss = 0.4888208005577326, model saved at 11\n",
      "val_losses[-1] = 0.45907400427386164, best_loss = 0.4761270056478679, model saved at 12\n",
      "val_losses[-1] = 0.43779598036780953, best_loss = 0.45907400427386164, model saved at 16\n",
      "(1, 0.7)\n",
      "iter 0\n",
      "val_losses[-1] = 1.2907054722309113, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8554642293602228, best_loss = 1.2907054722309113, model saved at 1\n",
      "val_losses[-1] = 0.7284914225339889, best_loss = 0.8554642293602228, model saved at 2\n",
      "val_losses[-1] = 0.6651306927204133, best_loss = 0.7284914225339889, model saved at 3\n",
      "val_losses[-1] = 0.6003310285508633, best_loss = 0.6651306927204133, model saved at 4\n",
      "val_losses[-1] = 0.5754546580836177, best_loss = 0.6003310285508633, model saved at 5\n",
      "val_losses[-1] = 0.5444762545637787, best_loss = 0.5754546580836177, model saved at 9\n",
      "val_losses[-1] = 0.4989858090877533, best_loss = 0.5444762545637787, model saved at 10\n",
      "val_losses[-1] = 0.49001417299732564, best_loss = 0.4989858090877533, model saved at 12\n",
      "val_losses[-1] = 0.45731857949867843, best_loss = 0.49001417299732564, model saved at 14\n",
      "val_losses[-1] = 0.44399119466543197, best_loss = 0.45731857949867843, model saved at 17\n",
      "val_losses[-1] = 0.43173595601692794, best_loss = 0.44399119466543197, model saved at 20\n",
      "val_losses[-1] = 0.4276348521001637, best_loss = 0.43173595601692794, model saved at 27\n",
      "iter 1\n",
      "val_losses[-1] = 1.1863127160817384, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.794898620992899, best_loss = 1.1863127160817384, model saved at 1\n",
      "val_losses[-1] = 0.7756557680666447, best_loss = 0.794898620992899, model saved at 2\n",
      "val_losses[-1] = 0.7154479704797267, best_loss = 0.7756557680666447, model saved at 3\n",
      "val_losses[-1] = 0.5927832534536719, best_loss = 0.7154479704797267, model saved at 4\n",
      "val_losses[-1] = 0.5446446004323662, best_loss = 0.5927832534536719, model saved at 7\n",
      "val_losses[-1] = 0.47523350091651084, best_loss = 0.5446446004323662, model saved at 8\n",
      "val_losses[-1] = 0.46388953030109403, best_loss = 0.47523350091651084, model saved at 13\n",
      "val_losses[-1] = 0.4578611541539431, best_loss = 0.46388953030109403, model saved at 14\n",
      "val_losses[-1] = 0.4480658731423318, best_loss = 0.4578611541539431, model saved at 17\n",
      "val_losses[-1] = 0.4427266623824835, best_loss = 0.4480658731423318, model saved at 18\n",
      "val_losses[-1] = 0.43514302335679533, best_loss = 0.4427266623824835, model saved at 20\n",
      "val_losses[-1] = 0.43434267938137056, best_loss = 0.43514302335679533, model saved at 22\n",
      "val_losses[-1] = 0.43343582963570954, best_loss = 0.43434267938137056, model saved at 24\n",
      "iter 2\n",
      "val_losses[-1] = 1.5943815685808658, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.0544136449694634, best_loss = 1.5943815685808658, model saved at 1\n",
      "val_losses[-1] = 0.8437908861786128, best_loss = 1.0544136449694634, model saved at 2\n",
      "val_losses[-1] = 0.7967570934444665, best_loss = 0.8437908861786128, model saved at 3\n",
      "val_losses[-1] = 0.7209209052845835, best_loss = 0.7967570934444665, model saved at 5\n",
      "val_losses[-1] = 0.7145916385576129, best_loss = 0.7209209052845835, model saved at 6\n",
      "val_losses[-1] = 0.6432431381195783, best_loss = 0.7145916385576129, model saved at 7\n",
      "val_losses[-1] = 0.5631861589848995, best_loss = 0.6432431381195783, model saved at 8\n",
      "val_losses[-1] = 0.5427997763268649, best_loss = 0.5631861589848995, model saved at 9\n",
      "val_losses[-1] = 0.5398919427767396, best_loss = 0.5427997763268649, model saved at 10\n",
      "val_losses[-1] = 0.509759517852217, best_loss = 0.5398919427767396, model saved at 11\n",
      "val_losses[-1] = 0.4975472575984895, best_loss = 0.509759517852217, model saved at 12\n",
      "val_losses[-1] = 0.45069860769435766, best_loss = 0.4975472575984895, model saved at 16\n",
      "val_losses[-1] = 0.4418502999469638, best_loss = 0.45069860769435766, model saved at 18\n",
      "val_losses[-1] = 0.43889031643047927, best_loss = 0.4418502999469638, model saved at 21\n",
      "val_losses[-1] = 0.42119640335440633, best_loss = 0.43889031643047927, model saved at 27\n",
      "iter 3\n",
      "val_losses[-1] = 1.6033273339271545, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.1884796783328055, best_loss = 1.6033273339271545, model saved at 1\n",
      "val_losses[-1] = 0.7594507083296775, best_loss = 1.1884796783328055, model saved at 2\n",
      "val_losses[-1] = 0.6569696245715022, best_loss = 0.7594507083296775, model saved at 4\n",
      "val_losses[-1] = 0.6435513058677316, best_loss = 0.6569696245715022, model saved at 5\n",
      "val_losses[-1] = 0.6399854118004441, best_loss = 0.6435513058677316, model saved at 6\n",
      "val_losses[-1] = 0.49116609590128063, best_loss = 0.6399854118004441, model saved at 8\n",
      "val_losses[-1] = 0.45249823229387404, best_loss = 0.49116609590128063, model saved at 11\n",
      "val_losses[-1] = 0.4517596159130335, best_loss = 0.45249823229387404, model saved at 12\n",
      "val_losses[-1] = 0.45150233563035724, best_loss = 0.4517596159130335, model saved at 18\n",
      "val_losses[-1] = 0.43870200403034687, best_loss = 0.45150233563035724, model saved at 19\n",
      "val_losses[-1] = 0.43434544978663325, best_loss = 0.43870200403034687, model saved at 21\n",
      "iter 4\n",
      "val_losses[-1] = 1.5428030021488666, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.0516868371516466, best_loss = 1.5428030021488666, model saved at 1\n",
      "val_losses[-1] = 0.8001990247517824, best_loss = 1.0516868371516466, model saved at 2\n",
      "val_losses[-1] = 0.7268178196623921, best_loss = 0.8001990247517824, model saved at 4\n",
      "val_losses[-1] = 0.6669844221323729, best_loss = 0.7268178196623921, model saved at 5\n",
      "val_losses[-1] = 0.6102093484252691, best_loss = 0.6669844221323729, model saved at 6\n",
      "val_losses[-1] = 0.6062527537345886, best_loss = 0.6102093484252691, model saved at 7\n",
      "val_losses[-1] = 0.5832010012120008, best_loss = 0.6062527537345886, model saved at 9\n",
      "val_losses[-1] = 0.5212496461346745, best_loss = 0.5832010012120008, model saved at 10\n",
      "val_losses[-1] = 0.4865068966522813, best_loss = 0.5212496461346745, model saved at 11\n",
      "val_losses[-1] = 0.4773798009380698, best_loss = 0.4865068966522813, model saved at 12\n",
      "val_losses[-1] = 0.4425034403800964, best_loss = 0.4773798009380698, model saved at 16\n",
      "val_losses[-1] = 0.4373517161235213, best_loss = 0.4425034403800964, model saved at 22\n",
      "(1, 0.6)\n",
      "iter 0\n",
      "val_losses[-1] = 1.4768140114843846, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8695493668317795, best_loss = 1.4768140114843846, model saved at 1\n",
      "val_losses[-1] = 0.7282904602587223, best_loss = 0.8695493668317795, model saved at 2\n",
      "val_losses[-1] = 0.6633706774562598, best_loss = 0.7282904602587223, model saved at 3\n",
      "val_losses[-1] = 0.5993785599246622, best_loss = 0.6633706774562598, model saved at 4\n",
      "val_losses[-1] = 0.5817475112155079, best_loss = 0.5993785599246622, model saved at 5\n",
      "val_losses[-1] = 0.5291757997125387, best_loss = 0.5817475112155079, model saved at 8\n",
      "val_losses[-1] = 0.5241466399282217, best_loss = 0.5291757997125387, model saved at 12\n",
      "val_losses[-1] = 0.49720113053917886, best_loss = 0.5241466399282217, model saved at 14\n",
      "val_losses[-1] = 0.4784591030329466, best_loss = 0.49720113053917886, model saved at 15\n",
      "val_losses[-1] = 0.4363953540101647, best_loss = 0.4784591030329466, model saved at 17\n",
      "val_losses[-1] = 0.4272456864826381, best_loss = 0.4363953540101647, model saved at 20\n",
      "iter 1\n",
      "val_losses[-1] = 1.2438910715281963, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8042093008756638, best_loss = 1.2438910715281963, model saved at 1\n",
      "val_losses[-1] = 0.7840180069208145, best_loss = 0.8042093008756638, model saved at 2\n",
      "val_losses[-1] = 0.7555942127481103, best_loss = 0.7840180069208145, model saved at 3\n",
      "val_losses[-1] = 0.6021492317318916, best_loss = 0.7555942127481103, model saved at 4\n",
      "val_losses[-1] = 0.5735416941344738, best_loss = 0.6021492317318916, model saved at 5\n",
      "val_losses[-1] = 0.5465623514726758, best_loss = 0.5735416941344738, model saved at 6\n",
      "val_losses[-1] = 0.47201331350952386, best_loss = 0.5465623514726758, model saved at 8\n",
      "val_losses[-1] = 0.45489881029352547, best_loss = 0.47201331350952386, model saved at 13\n",
      "val_losses[-1] = 0.44654743587598206, best_loss = 0.45489881029352547, model saved at 20\n",
      "val_losses[-1] = 0.436522579099983, best_loss = 0.44654743587598206, model saved at 22\n",
      "iter 2\n",
      "val_losses[-1] = 1.588253904134035, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.005722115561366, best_loss = 1.588253904134035, model saved at 1\n",
      "val_losses[-1] = 0.7984239842742682, best_loss = 1.005722115561366, model saved at 2\n",
      "val_losses[-1] = 0.7377461230382323, best_loss = 0.7984239842742682, model saved at 3\n",
      "val_losses[-1] = 0.638784715719521, best_loss = 0.7377461230382323, model saved at 5\n",
      "val_losses[-1] = 0.5758112464100122, best_loss = 0.638784715719521, model saved at 7\n",
      "val_losses[-1] = 0.5512853933498263, best_loss = 0.5758112464100122, model saved at 8\n",
      "val_losses[-1] = 0.4812866666354239, best_loss = 0.5512853933498263, model saved at 11\n",
      "val_losses[-1] = 0.46597179118543863, best_loss = 0.4812866666354239, model saved at 12\n",
      "val_losses[-1] = 0.4533499895595014, best_loss = 0.46597179118543863, model saved at 16\n",
      "val_losses[-1] = 0.44357240600511433, best_loss = 0.4533499895595014, model saved at 17\n",
      "val_losses[-1] = 0.4397764509543777, best_loss = 0.44357240600511433, model saved at 22\n",
      "val_losses[-1] = 0.4379371582530439, best_loss = 0.4397764509543777, model saved at 27\n",
      "val_losses[-1] = 0.4328438618220389, best_loss = 0.4379371582530439, model saved at 28\n",
      "iter 3\n",
      "val_losses[-1] = 1.5986555568873881, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 1.116657091677189, best_loss = 1.5986555568873881, model saved at 1\n",
      "val_losses[-1] = 0.8025508906692267, best_loss = 1.116657091677189, model saved at 2\n",
      "val_losses[-1] = 0.7159629138186574, best_loss = 0.8025508906692267, model saved at 3\n",
      "val_losses[-1] = 0.6881960498169064, best_loss = 0.7159629138186574, model saved at 4\n",
      "val_losses[-1] = 0.6505621211603284, best_loss = 0.6881960498169064, model saved at 5\n",
      "val_losses[-1] = 0.5906196920201182, best_loss = 0.6505621211603284, model saved at 8\n",
      "val_losses[-1] = 0.5348909288644791, best_loss = 0.5906196920201182, model saved at 9\n",
      "val_losses[-1] = 0.5228432768955826, best_loss = 0.5348909288644791, model saved at 10\n",
      "val_losses[-1] = 0.48723552096635103, best_loss = 0.5228432768955826, model saved at 11\n",
      "val_losses[-1] = 0.4622837288305163, best_loss = 0.48723552096635103, model saved at 15\n",
      "val_losses[-1] = 0.4589750592596829, best_loss = 0.4622837288305163, model saved at 17\n",
      "val_losses[-1] = 0.4557490233331919, best_loss = 0.4589750592596829, model saved at 18\n",
      "val_losses[-1] = 0.4372366017661989, best_loss = 0.4557490233331919, model saved at 19\n",
      "val_losses[-1] = 0.43547902405261996, best_loss = 0.4372366017661989, model saved at 24\n",
      "val_losses[-1] = 0.4324327633716166, best_loss = 0.43547902405261996, model saved at 28\n",
      "iter 4\n",
      "val_losses[-1] = 1.4980449378490448, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.9486049048602581, best_loss = 1.4980449378490448, model saved at 1\n",
      "val_losses[-1] = 0.7693939227610826, best_loss = 0.9486049048602581, model saved at 2\n",
      "val_losses[-1] = 0.7563053078949451, best_loss = 0.7693939227610826, model saved at 3\n",
      "val_losses[-1] = 0.6510076811537147, best_loss = 0.7563053078949451, model saved at 5\n",
      "val_losses[-1] = 0.5462837068364024, best_loss = 0.6510076811537147, model saved at 6\n",
      "val_losses[-1] = 0.5216960681602358, best_loss = 0.5462837068364024, model saved at 7\n",
      "val_losses[-1] = 0.5054853540845216, best_loss = 0.5216960681602358, model saved at 8\n",
      "val_losses[-1] = 0.46697071576491, best_loss = 0.5054853540845216, model saved at 11\n",
      "val_losses[-1] = 0.437948577478528, best_loss = 0.46697071576491, model saved at 16\n",
      "val_losses[-1] = 0.43072819206863644, best_loss = 0.437948577478528, model saved at 22\n",
      "(1, 0.5)\n",
      "iter 0\n",
      "val_losses[-1] = 1.1999758400022984, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8604850646108388, best_loss = 1.1999758400022984, model saved at 1\n",
      "val_losses[-1] = 0.7394623707979917, best_loss = 0.8604850646108388, model saved at 2\n",
      "val_losses[-1] = 0.7197407508268953, best_loss = 0.7394623707979917, model saved at 3\n",
      "val_losses[-1] = 0.6420361930504441, best_loss = 0.7197407508268953, model saved at 4\n",
      "val_losses[-1] = 0.5808433344587683, best_loss = 0.6420361930504441, model saved at 5\n",
      "val_losses[-1] = 0.5715053433552384, best_loss = 0.5808433344587683, model saved at 6\n",
      "val_losses[-1] = 0.5419884972274304, best_loss = 0.5715053433552384, model saved at 8\n",
      "val_losses[-1] = 0.5383196251466871, best_loss = 0.5419884972274304, model saved at 10\n",
      "val_losses[-1] = 0.5296639047563076, best_loss = 0.5383196251466871, model saved at 12\n",
      "val_losses[-1] = 0.4929968249984086, best_loss = 0.5296639047563076, model saved at 13\n",
      "val_losses[-1] = 0.4675685680471361, best_loss = 0.4929968249984086, model saved at 14\n",
      "val_losses[-1] = 0.4334148283582181, best_loss = 0.4675685680471361, model saved at 17\n",
      "val_losses[-1] = 0.4333939134143293, best_loss = 0.4334148283582181, model saved at 20\n",
      "iter 1\n",
      "val_losses[-1] = 1.1715235650539397, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.7640367384999991, best_loss = 1.1715235650539397, model saved at 1\n",
      "val_losses[-1] = 0.6903813101351262, best_loss = 0.7640367384999991, model saved at 2\n",
      "val_losses[-1] = 0.5881766783073544, best_loss = 0.6903813101351262, model saved at 3\n",
      "val_losses[-1] = 0.5529162865132093, best_loss = 0.5881766783073544, model saved at 5\n",
      "val_losses[-1] = 0.4976512002758682, best_loss = 0.5529162865132093, model saved at 6\n",
      "val_losses[-1] = 0.47143634157255293, best_loss = 0.4976512002758682, model saved at 8\n",
      "val_losses[-1] = 0.4702199251390994, best_loss = 0.47143634157255293, model saved at 11\n",
      "val_losses[-1] = 0.4501906828954816, best_loss = 0.4702199251390994, model saved at 13\n",
      "val_losses[-1] = 0.43990449300035833, best_loss = 0.4501906828954816, model saved at 18\n",
      "val_losses[-1] = 0.4300642741844058, best_loss = 0.43990449300035833, model saved at 20\n",
      "val_losses[-1] = 0.42990520875900984, best_loss = 0.4300642741844058, model saved at 22\n",
      "iter 2\n",
      "val_losses[-1] = 1.573471763730049, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.9008479963988065, best_loss = 1.573471763730049, model saved at 1\n",
      "val_losses[-1] = 0.7487388983368873, best_loss = 0.9008479963988065, model saved at 2\n",
      "val_losses[-1] = 0.7175025790929794, best_loss = 0.7487388983368873, model saved at 3\n",
      "val_losses[-1] = 0.6915601003915072, best_loss = 0.7175025790929794, model saved at 5\n",
      "val_losses[-1] = 0.6335286276414991, best_loss = 0.6915601003915072, model saved at 6\n",
      "val_losses[-1] = 0.6183409815654158, best_loss = 0.6335286276414991, model saved at 8\n",
      "val_losses[-1] = 0.6053137823939323, best_loss = 0.6183409815654158, model saved at 9\n",
      "val_losses[-1] = 0.5420135647058487, best_loss = 0.6053137823939323, model saved at 10\n",
      "val_losses[-1] = 0.5174089422449469, best_loss = 0.5420135647058487, model saved at 12\n",
      "val_losses[-1] = 0.4877439050003886, best_loss = 0.5174089422449469, model saved at 13\n",
      "val_losses[-1] = 0.45173775386065246, best_loss = 0.4877439050003886, model saved at 16\n",
      "val_losses[-1] = 0.4399210757575929, best_loss = 0.45173775386065246, model saved at 18\n",
      "val_losses[-1] = 0.43636632785201074, best_loss = 0.4399210757575929, model saved at 26\n",
      "iter 3\n",
      "val_losses[-1] = 1.6003626592457294, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8986427567899227, best_loss = 1.6003626592457294, model saved at 1\n",
      "val_losses[-1] = 0.7137599118053913, best_loss = 0.8986427567899227, model saved at 2\n",
      "val_losses[-1] = 0.6795535204932094, best_loss = 0.7137599118053913, model saved at 3\n",
      "val_losses[-1] = 0.6330074349418282, best_loss = 0.6795535204932094, model saved at 5\n",
      "val_losses[-1] = 0.6160289432853461, best_loss = 0.6330074349418282, model saved at 8\n",
      "val_losses[-1] = 0.5361721839755773, best_loss = 0.6160289432853461, model saved at 9\n",
      "val_losses[-1] = 0.5220496795140207, best_loss = 0.5361721839755773, model saved at 10\n",
      "val_losses[-1] = 0.5054246472194791, best_loss = 0.5220496795140207, model saved at 11\n",
      "val_losses[-1] = 0.4701610899530351, best_loss = 0.5054246472194791, model saved at 14\n",
      "val_losses[-1] = 0.46092149131000043, best_loss = 0.4701610899530351, model saved at 15\n",
      "val_losses[-1] = 0.445628859475255, best_loss = 0.46092149131000043, model saved at 18\n",
      "val_losses[-1] = 0.44362457701936364, best_loss = 0.445628859475255, model saved at 19\n",
      "val_losses[-1] = 0.43200999069958923, best_loss = 0.44362457701936364, model saved at 20\n",
      "val_losses[-1] = 0.42814120147377255, best_loss = 0.43200999069958923, model saved at 24\n",
      "iter 4\n",
      "val_losses[-1] = 1.5130249731242658, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.868880956619978, best_loss = 1.5130249731242658, model saved at 1\n",
      "val_losses[-1] = 0.8578091651201248, best_loss = 0.868880956619978, model saved at 2\n",
      "val_losses[-1] = 0.7303278243169189, best_loss = 0.8578091651201248, model saved at 3\n",
      "val_losses[-1] = 0.6831633413210512, best_loss = 0.7303278243169189, model saved at 4\n",
      "val_losses[-1] = 0.6726312348619103, best_loss = 0.6831633413210512, model saved at 6\n",
      "val_losses[-1] = 0.5917709955945611, best_loss = 0.6726312348619103, model saved at 7\n",
      "val_losses[-1] = 0.5514232005923987, best_loss = 0.5917709955945611, model saved at 8\n",
      "val_losses[-1] = 0.4938367505557835, best_loss = 0.5514232005923987, model saved at 10\n",
      "val_losses[-1] = 0.47720830179750917, best_loss = 0.4938367505557835, model saved at 15\n",
      "val_losses[-1] = 0.4589005048386753, best_loss = 0.47720830179750917, model saved at 16\n",
      "val_losses[-1] = 0.4489583007059991, best_loss = 0.4589005048386753, model saved at 21\n",
      "val_losses[-1] = 0.43125605611130596, best_loss = 0.4489583007059991, model saved at 24\n",
      "(1, 0.4)\n",
      "iter 0\n",
      "val_losses[-1] = 1.4316595003008843, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8564289659261703, best_loss = 1.4316595003008843, model saved at 1\n",
      "val_losses[-1] = 0.7065546410158277, best_loss = 0.8564289659261703, model saved at 2\n",
      "val_losses[-1] = 0.6553077146410942, best_loss = 0.7065546410158277, model saved at 3\n",
      "val_losses[-1] = 0.6062411023303866, best_loss = 0.6553077146410942, model saved at 4\n",
      "val_losses[-1] = 0.5526043245568871, best_loss = 0.6062411023303866, model saved at 6\n",
      "val_losses[-1] = 0.534866739064455, best_loss = 0.5526043245568871, model saved at 7\n",
      "val_losses[-1] = 0.4918419120833278, best_loss = 0.534866739064455, model saved at 8\n",
      "val_losses[-1] = 0.4917201424017549, best_loss = 0.4918419120833278, model saved at 9\n",
      "val_losses[-1] = 0.46923230215907097, best_loss = 0.4917201424017549, model saved at 10\n",
      "val_losses[-1] = 0.45124903954565526, best_loss = 0.46923230215907097, model saved at 12\n",
      "val_losses[-1] = 0.4440667930059135, best_loss = 0.45124903954565526, model saved at 15\n",
      "val_losses[-1] = 0.43472765320912005, best_loss = 0.4440667930059135, model saved at 17\n",
      "val_losses[-1] = 0.43231221614405513, best_loss = 0.43472765320912005, model saved at 20\n",
      "iter 1\n",
      "val_losses[-1] = 1.1024845223873854, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.7340316975489258, best_loss = 1.1024845223873854, model saved at 1\n",
      "val_losses[-1] = 0.6465463377535343, best_loss = 0.7340316975489258, model saved at 2\n",
      "val_losses[-1] = 0.5900852346792818, best_loss = 0.6465463377535343, model saved at 3\n",
      "val_losses[-1] = 0.5661014759913087, best_loss = 0.5900852346792818, model saved at 4\n",
      "val_losses[-1] = 0.5465752905234694, best_loss = 0.5661014759913087, model saved at 5\n",
      "val_losses[-1] = 0.5045953663997352, best_loss = 0.5465752905234694, model saved at 6\n",
      "val_losses[-1] = 0.4667869320139289, best_loss = 0.5045953663997352, model saved at 8\n",
      "val_losses[-1] = 0.46627130433917047, best_loss = 0.4667869320139289, model saved at 11\n",
      "val_losses[-1] = 0.4384445299394429, best_loss = 0.46627130433917047, model saved at 13\n",
      "val_losses[-1] = 0.4337527511641383, best_loss = 0.4384445299394429, model saved at 18\n",
      "val_losses[-1] = 0.4288146557286382, best_loss = 0.4337527511641383, model saved at 20\n",
      "iter 2\n",
      "val_losses[-1] = 1.5537490755319596, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.9007306519895792, best_loss = 1.5537490755319596, model saved at 1\n",
      "val_losses[-1] = 0.7292093308642507, best_loss = 0.9007306519895792, model saved at 2\n",
      "val_losses[-1] = 0.6876844929531216, best_loss = 0.7292093308642507, model saved at 3\n",
      "val_losses[-1] = 0.6349776804447174, best_loss = 0.6876844929531216, model saved at 6\n",
      "val_losses[-1] = 0.6304047098383307, best_loss = 0.6349776804447174, model saved at 7\n",
      "val_losses[-1] = 0.617265654169023, best_loss = 0.6304047098383307, model saved at 8\n",
      "val_losses[-1] = 0.5638893416151405, best_loss = 0.617265654169023, model saved at 10\n",
      "val_losses[-1] = 0.5398552162572742, best_loss = 0.5638893416151405, model saved at 11\n",
      "val_losses[-1] = 0.4912574216723442, best_loss = 0.5398552162572742, model saved at 12\n",
      "val_losses[-1] = 0.48511957079172136, best_loss = 0.4912574216723442, model saved at 14\n",
      "val_losses[-1] = 0.4640684735029936, best_loss = 0.48511957079172136, model saved at 17\n",
      "val_losses[-1] = 0.44198607336729767, best_loss = 0.4640684735029936, model saved at 18\n",
      "val_losses[-1] = 0.4374499723315239, best_loss = 0.44198607336729767, model saved at 19\n",
      "val_losses[-1] = 0.42518293792381884, best_loss = 0.4374499723315239, model saved at 22\n",
      "val_losses[-1] = 0.42512360513210296, best_loss = 0.42518293792381884, model saved at 25\n",
      "iter 3\n",
      "val_losses[-1] = 1.593394249677658, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8616172082722187, best_loss = 1.593394249677658, model saved at 1\n",
      "val_losses[-1] = 0.7051631970331073, best_loss = 0.8616172082722187, model saved at 2\n",
      "val_losses[-1] = 0.6572657706215977, best_loss = 0.7051631970331073, model saved at 3\n",
      "val_losses[-1] = 0.6259596835821867, best_loss = 0.6572657706215977, model saved at 5\n",
      "val_losses[-1] = 0.5230926391668618, best_loss = 0.6259596835821867, model saved at 8\n",
      "val_losses[-1] = 0.478959733247757, best_loss = 0.5230926391668618, model saved at 11\n",
      "val_losses[-1] = 0.45123033225536346, best_loss = 0.478959733247757, model saved at 12\n",
      "val_losses[-1] = 0.43962531005963684, best_loss = 0.45123033225536346, model saved at 14\n",
      "val_losses[-1] = 0.43196608321741226, best_loss = 0.43962531005963684, model saved at 18\n",
      "val_losses[-1] = 0.4268761033192277, best_loss = 0.43196608321741226, model saved at 20\n",
      "iter 4\n",
      "val_losses[-1] = 1.4759423598647117, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8431803401559591, best_loss = 1.4759423598647117, model saved at 1\n",
      "val_losses[-1] = 0.8293193999677897, best_loss = 0.8431803401559591, model saved at 2\n",
      "val_losses[-1] = 0.7084689565002918, best_loss = 0.8293193999677897, model saved at 3\n",
      "val_losses[-1] = 0.6734102223068476, best_loss = 0.7084689565002918, model saved at 4\n",
      "val_losses[-1] = 0.5837686978280544, best_loss = 0.6734102223068476, model saved at 6\n",
      "val_losses[-1] = 0.5254750125110149, best_loss = 0.5837686978280544, model saved at 8\n",
      "val_losses[-1] = 0.49951738622039554, best_loss = 0.5254750125110149, model saved at 10\n",
      "val_losses[-1] = 0.49243616349995134, best_loss = 0.49951738622039554, model saved at 11\n",
      "val_losses[-1] = 0.4846643225289881, best_loss = 0.49243616349995134, model saved at 14\n",
      "val_losses[-1] = 0.4600740996189415, best_loss = 0.4846643225289881, model saved at 16\n",
      "val_losses[-1] = 0.4534001985564828, best_loss = 0.4600740996189415, model saved at 17\n",
      "val_losses[-1] = 0.4469737684354186, best_loss = 0.4534001985564828, model saved at 20\n",
      "val_losses[-1] = 0.44379724534228443, best_loss = 0.4469737684354186, model saved at 21\n",
      "val_losses[-1] = 0.4322081750258803, best_loss = 0.44379724534228443, model saved at 24\n",
      "(1, 0.3)\n",
      "iter 0\n",
      "val_losses[-1] = 1.2550515152513981, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.7861737467348575, best_loss = 1.2550515152513981, model saved at 1\n",
      "val_losses[-1] = 0.7036243714392185, best_loss = 0.7861737467348575, model saved at 2\n",
      "val_losses[-1] = 0.66279037296772, best_loss = 0.7036243714392185, model saved at 4\n",
      "val_losses[-1] = 0.6327415781095624, best_loss = 0.66279037296772, model saved at 5\n",
      "val_losses[-1] = 0.5774560578167438, best_loss = 0.6327415781095624, model saved at 7\n",
      "val_losses[-1] = 0.5500221038237214, best_loss = 0.5774560578167438, model saved at 8\n",
      "val_losses[-1] = 0.5403210861608386, best_loss = 0.5500221038237214, model saved at 9\n",
      "val_losses[-1] = 0.47961323950439694, best_loss = 0.5403210861608386, model saved at 12\n",
      "val_losses[-1] = 0.45731317438185215, best_loss = 0.47961323950439694, model saved at 13\n",
      "val_losses[-1] = 0.4477591373026371, best_loss = 0.45731317438185215, model saved at 15\n",
      "val_losses[-1] = 0.4292188096791506, best_loss = 0.4477591373026371, model saved at 17\n",
      "val_losses[-1] = 0.4261560533195734, best_loss = 0.4292188096791506, model saved at 20\n",
      "val_losses[-1] = 0.4255925739184022, best_loss = 0.4261560533195734, model saved at 21\n",
      "iter 1\n",
      "val_losses[-1] = 0.9544778499752283, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.7598591627553105, best_loss = 0.9544778499752283, model saved at 1\n",
      "val_losses[-1] = 0.6733132261782885, best_loss = 0.7598591627553105, model saved at 2\n",
      "val_losses[-1] = 0.6571056403219699, best_loss = 0.6733132261782885, model saved at 3\n",
      "val_losses[-1] = 0.5853967629373074, best_loss = 0.6571056403219699, model saved at 6\n",
      "val_losses[-1] = 0.5706194280646741, best_loss = 0.5853967629373074, model saved at 7\n",
      "val_losses[-1] = 0.535170341655612, best_loss = 0.5706194280646741, model saved at 8\n",
      "val_losses[-1] = 0.5196262658573687, best_loss = 0.535170341655612, model saved at 9\n",
      "val_losses[-1] = 0.5009631056338548, best_loss = 0.5196262658573687, model saved at 11\n",
      "val_losses[-1] = 0.4712334033101797, best_loss = 0.5009631056338548, model saved at 12\n",
      "val_losses[-1] = 0.4454436399973929, best_loss = 0.4712334033101797, model saved at 13\n",
      "val_losses[-1] = 0.43556001912802456, best_loss = 0.4454436399973929, model saved at 15\n",
      "val_losses[-1] = 0.4256265826523304, best_loss = 0.43556001912802456, model saved at 16\n",
      "val_losses[-1] = 0.41923418249934913, best_loss = 0.4256265826523304, model saved at 20\n",
      "iter 2\n",
      "val_losses[-1] = 1.4936201333999635, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8107225771993398, best_loss = 1.4936201333999635, model saved at 1\n",
      "val_losses[-1] = 0.6952481029555202, best_loss = 0.8107225771993398, model saved at 2\n",
      "val_losses[-1] = 0.6605757512152195, best_loss = 0.6952481029555202, model saved at 4\n",
      "val_losses[-1] = 0.6299790786579251, best_loss = 0.6605757512152195, model saved at 6\n",
      "val_losses[-1] = 0.5738604268059134, best_loss = 0.6299790786579251, model saved at 7\n",
      "val_losses[-1] = 0.5364387161098421, best_loss = 0.5738604268059134, model saved at 8\n",
      "val_losses[-1] = 0.4988890695385635, best_loss = 0.5364387161098421, model saved at 10\n",
      "val_losses[-1] = 0.4815130107104778, best_loss = 0.4988890695385635, model saved at 12\n",
      "val_losses[-1] = 0.47898276820778846, best_loss = 0.4815130107104778, model saved at 13\n",
      "val_losses[-1] = 0.45938065871596334, best_loss = 0.47898276820778846, model saved at 17\n",
      "val_losses[-1] = 0.4438792520202696, best_loss = 0.45938065871596334, model saved at 18\n",
      "val_losses[-1] = 0.4314508226700127, best_loss = 0.4438792520202696, model saved at 20\n",
      "iter 3\n",
      "val_losses[-1] = 1.5558393061161042, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8029631916433573, best_loss = 1.5558393061161042, model saved at 1\n",
      "val_losses[-1] = 0.7921391369774937, best_loss = 0.8029631916433573, model saved at 2\n",
      "val_losses[-1] = 0.6177983485162258, best_loss = 0.7921391369774937, model saved at 3\n",
      "val_losses[-1] = 0.5920447945594788, best_loss = 0.6177983485162258, model saved at 5\n",
      "val_losses[-1] = 0.5801229840144515, best_loss = 0.5920447945594788, model saved at 6\n",
      "val_losses[-1] = 0.5556128270924091, best_loss = 0.5801229840144515, model saved at 7\n",
      "val_losses[-1] = 0.48654518704861405, best_loss = 0.5556128270924091, model saved at 8\n",
      "val_losses[-1] = 0.476732474565506, best_loss = 0.48654518704861405, model saved at 10\n",
      "val_losses[-1] = 0.4548662981018424, best_loss = 0.476732474565506, model saved at 11\n",
      "val_losses[-1] = 0.4495101112872362, best_loss = 0.4548662981018424, model saved at 12\n",
      "val_losses[-1] = 0.44769482063129545, best_loss = 0.4495101112872362, model saved at 17\n",
      "val_losses[-1] = 0.44611465353518726, best_loss = 0.44769482063129545, model saved at 19\n",
      "val_losses[-1] = 0.4400518083013594, best_loss = 0.44611465353518726, model saved at 22\n",
      "iter 4\n",
      "val_losses[-1] = 1.3024132043123244, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8173848755657673, best_loss = 1.3024132043123244, model saved at 1\n",
      "val_losses[-1] = 0.7117285031825304, best_loss = 0.8173848755657673, model saved at 3\n",
      "val_losses[-1] = 0.663430542871356, best_loss = 0.7117285031825304, model saved at 4\n",
      "val_losses[-1] = 0.5620602613314987, best_loss = 0.663430542871356, model saved at 7\n",
      "val_losses[-1] = 0.5487268142402172, best_loss = 0.5620602613314987, model saved at 8\n",
      "val_losses[-1] = 0.5090046186000109, best_loss = 0.5487268142402172, model saved at 11\n",
      "val_losses[-1] = 0.5018395565450191, best_loss = 0.5090046186000109, model saved at 13\n",
      "val_losses[-1] = 0.49097130419686436, best_loss = 0.5018395565450191, model saved at 14\n",
      "val_losses[-1] = 0.4821671137586236, best_loss = 0.49097130419686436, model saved at 15\n",
      "val_losses[-1] = 0.4698475764133036, best_loss = 0.4821671137586236, model saved at 16\n",
      "val_losses[-1] = 0.46349929375573995, best_loss = 0.4698475764133036, model saved at 17\n",
      "val_losses[-1] = 0.4606118707917631, best_loss = 0.46349929375573995, model saved at 20\n",
      "val_losses[-1] = 0.45649118833243846, best_loss = 0.4606118707917631, model saved at 21\n",
      "val_losses[-1] = 0.45237328894436357, best_loss = 0.45649118833243846, model saved at 22\n",
      "(1, 0.2)\n",
      "iter 0\n",
      "val_losses[-1] = 1.5098180256783962, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8246096648275852, best_loss = 1.5098180256783962, model saved at 1\n",
      "val_losses[-1] = 0.657747071608901, best_loss = 0.8246096648275852, model saved at 2\n",
      "val_losses[-1] = 0.6250922141596675, best_loss = 0.657747071608901, model saved at 3\n",
      "val_losses[-1] = 0.5651839535683394, best_loss = 0.6250922141596675, model saved at 4\n",
      "val_losses[-1] = 0.5264289146289229, best_loss = 0.5651839535683394, model saved at 5\n",
      "val_losses[-1] = 0.5220318282023072, best_loss = 0.5264289146289229, model saved at 7\n",
      "val_losses[-1] = 0.5058717619627714, best_loss = 0.5220318282023072, model saved at 8\n",
      "val_losses[-1] = 0.4753257575444877, best_loss = 0.5058717619627714, model saved at 9\n",
      "val_losses[-1] = 0.45844377456232904, best_loss = 0.4753257575444877, model saved at 10\n",
      "val_losses[-1] = 0.4379448024556041, best_loss = 0.45844377456232904, model saved at 12\n",
      "val_losses[-1] = 0.43485300252214076, best_loss = 0.4379448024556041, model saved at 14\n",
      "val_losses[-1] = 0.42384688509628177, best_loss = 0.43485300252214076, model saved at 15\n",
      "val_losses[-1] = 0.4219062357209623, best_loss = 0.42384688509628177, model saved at 20\n",
      "iter 1\n",
      "val_losses[-1] = 0.9077404279261827, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8234680410474539, best_loss = 0.9077404279261827, model saved at 1\n",
      "val_losses[-1] = 0.6378079313784838, best_loss = 0.8234680410474539, model saved at 2\n",
      "val_losses[-1] = 0.6185653364285827, best_loss = 0.6378079313784838, model saved at 3\n",
      "val_losses[-1] = 0.5247618824243545, best_loss = 0.6185653364285827, model saved at 6\n",
      "val_losses[-1] = 0.4855195766314864, best_loss = 0.5247618824243545, model saved at 9\n",
      "val_losses[-1] = 0.466311283223331, best_loss = 0.4855195766314864, model saved at 13\n",
      "val_losses[-1] = 0.4624979356303811, best_loss = 0.466311283223331, model saved at 14\n",
      "val_losses[-1] = 0.4379571573808789, best_loss = 0.4624979356303811, model saved at 15\n",
      "val_losses[-1] = 0.42877402873709797, best_loss = 0.4379571573808789, model saved at 16\n",
      "val_losses[-1] = 0.42767805103212597, best_loss = 0.42877402873709797, model saved at 20\n",
      "iter 2\n",
      "val_losses[-1] = 1.1683843746781348, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.7276855625212193, best_loss = 1.1683843746781348, model saved at 1\n",
      "val_losses[-1] = 0.6521827334538102, best_loss = 0.7276855625212193, model saved at 2\n",
      "val_losses[-1] = 0.6515025030821562, best_loss = 0.6521827334538102, model saved at 3\n",
      "val_losses[-1] = 0.5862034499645233, best_loss = 0.6515025030821562, model saved at 7\n",
      "val_losses[-1] = 0.5313726782798767, best_loss = 0.5862034499645233, model saved at 9\n",
      "val_losses[-1] = 0.527664958871901, best_loss = 0.5313726782798767, model saved at 11\n",
      "val_losses[-1] = 0.4570469303056598, best_loss = 0.527664958871901, model saved at 12\n",
      "val_losses[-1] = 0.4447295186109841, best_loss = 0.4570469303056598, model saved at 13\n",
      "val_losses[-1] = 0.43499060650356114, best_loss = 0.4447295186109841, model saved at 14\n",
      "val_losses[-1] = 0.4264367653056979, best_loss = 0.43499060650356114, model saved at 18\n",
      "iter 3\n",
      "val_losses[-1] = 1.1065995041280985, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.7262962494045496, best_loss = 1.1065995041280985, model saved at 1\n",
      "val_losses[-1] = 0.6112205633893609, best_loss = 0.7262962494045496, model saved at 3\n",
      "val_losses[-1] = 0.5531234597787261, best_loss = 0.6112205633893609, model saved at 5\n",
      "val_losses[-1] = 0.5309214504435659, best_loss = 0.5531234597787261, model saved at 6\n",
      "val_losses[-1] = 0.5211415102705359, best_loss = 0.5309214504435659, model saved at 7\n",
      "val_losses[-1] = 0.501161727681756, best_loss = 0.5211415102705359, model saved at 8\n",
      "val_losses[-1] = 0.44275190867483616, best_loss = 0.501161727681756, model saved at 11\n",
      "val_losses[-1] = 0.43555290773510935, best_loss = 0.44275190867483616, model saved at 12\n",
      "val_losses[-1] = 0.4344357387162745, best_loss = 0.43555290773510935, model saved at 18\n",
      "iter 4\n",
      "val_losses[-1] = 1.1142489906400441, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.751651206240058, best_loss = 1.1142489906400441, model saved at 1\n",
      "val_losses[-1] = 0.6676935585215688, best_loss = 0.751651206240058, model saved at 3\n",
      "val_losses[-1] = 0.5951355449855328, best_loss = 0.6676935585215688, model saved at 4\n",
      "val_losses[-1] = 0.5918369924649596, best_loss = 0.5951355449855328, model saved at 6\n",
      "val_losses[-1] = 0.5310123501345515, best_loss = 0.5918369924649596, model saved at 7\n",
      "val_losses[-1] = 0.5280129205435514, best_loss = 0.5310123501345515, model saved at 8\n",
      "val_losses[-1] = 0.5143752971664071, best_loss = 0.5280129205435514, model saved at 10\n",
      "val_losses[-1] = 0.5037457251921296, best_loss = 0.5143752971664071, model saved at 13\n",
      "val_losses[-1] = 0.4717073844745755, best_loss = 0.5037457251921296, model saved at 15\n",
      "val_losses[-1] = 0.4367164551280439, best_loss = 0.4717073844745755, model saved at 16\n",
      "val_losses[-1] = 0.4281574328429997, best_loss = 0.4367164551280439, model saved at 20\n",
      "val_losses[-1] = 0.4250943576917052, best_loss = 0.4281574328429997, model saved at 23\n",
      "(1, 0.1)\n",
      "iter 0\n",
      "val_losses[-1] = 0.8722225870937109, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.7322237897664309, best_loss = 0.8722225870937109, model saved at 1\n",
      "val_losses[-1] = 0.6254505241289735, best_loss = 0.7322237897664309, model saved at 2\n",
      "val_losses[-1] = 0.5777869462966919, best_loss = 0.6254505241289735, model saved at 3\n",
      "val_losses[-1] = 0.5673693416640162, best_loss = 0.5777869462966919, model saved at 4\n",
      "val_losses[-1] = 0.4813052279874682, best_loss = 0.5673693416640162, model saved at 5\n",
      "val_losses[-1] = 0.4736628090962768, best_loss = 0.4813052279874682, model saved at 8\n",
      "val_losses[-1] = 0.442474537435919, best_loss = 0.4736628090962768, model saved at 9\n",
      "val_losses[-1] = 0.4253745533525944, best_loss = 0.442474537435919, model saved at 12\n",
      "val_losses[-1] = 0.41925059352070093, best_loss = 0.4253745533525944, model saved at 15\n",
      "iter 1\n",
      "val_losses[-1] = 0.836071702465415, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.7243393452838063, best_loss = 0.836071702465415, model saved at 1\n",
      "val_losses[-1] = 0.5626094313338399, best_loss = 0.7243393452838063, model saved at 2\n",
      "val_losses[-1] = 0.5541031841188669, best_loss = 0.5626094313338399, model saved at 3\n",
      "val_losses[-1] = 0.5531722731888294, best_loss = 0.5541031841188669, model saved at 4\n",
      "val_losses[-1] = 0.5172038923949003, best_loss = 0.5531722731888294, model saved at 5\n",
      "val_losses[-1] = 0.5088467126712203, best_loss = 0.5172038923949003, model saved at 6\n",
      "val_losses[-1] = 0.4596604355610907, best_loss = 0.5088467126712203, model saved at 8\n",
      "val_losses[-1] = 0.4547427867539227, best_loss = 0.4596604355610907, model saved at 10\n",
      "val_losses[-1] = 0.44551531653851273, best_loss = 0.4547427867539227, model saved at 11\n",
      "val_losses[-1] = 0.4318152753636241, best_loss = 0.44551531653851273, model saved at 13\n",
      "val_losses[-1] = 0.4285311132669449, best_loss = 0.4318152753636241, model saved at 16\n",
      "val_losses[-1] = 0.42833178099244834, best_loss = 0.4285311132669449, model saved at 18\n",
      "iter 2\n",
      "val_losses[-1] = 1.044763894751668, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.726567855104804, best_loss = 1.044763894751668, model saved at 1\n",
      "val_losses[-1] = 0.624158531986177, best_loss = 0.726567855104804, model saved at 2\n",
      "val_losses[-1] = 0.5932236554101109, best_loss = 0.624158531986177, model saved at 3\n",
      "val_losses[-1] = 0.49169578300788996, best_loss = 0.5932236554101109, model saved at 7\n",
      "val_losses[-1] = 0.45123776206746696, best_loss = 0.49169578300788996, model saved at 10\n",
      "val_losses[-1] = 0.4485892021097243, best_loss = 0.45123776206746696, model saved at 11\n",
      "val_losses[-1] = 0.4302759850397706, best_loss = 0.4485892021097243, model saved at 12\n",
      "val_losses[-1] = 0.41970400330610574, best_loss = 0.4302759850397706, model saved at 14\n",
      "val_losses[-1] = 0.41817008759826424, best_loss = 0.41970400330610574, model saved at 17\n",
      "iter 3\n",
      "val_losses[-1] = 0.9866696011275053, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.6524211639538408, best_loss = 0.9866696011275053, model saved at 1\n",
      "val_losses[-1] = 0.6369204960763455, best_loss = 0.6524211639538408, model saved at 3\n",
      "val_losses[-1] = 0.5099290504120291, best_loss = 0.6369204960763455, model saved at 4\n",
      "val_losses[-1] = 0.5000296326354146, best_loss = 0.5099290504120291, model saved at 5\n",
      "val_losses[-1] = 0.49649519119411706, best_loss = 0.5000296326354146, model saved at 6\n",
      "val_losses[-1] = 0.46177500654011966, best_loss = 0.49649519119411706, model saved at 7\n",
      "val_losses[-1] = 0.45284937517717483, best_loss = 0.46177500654011966, model saved at 8\n",
      "val_losses[-1] = 0.4435133713297546, best_loss = 0.45284937517717483, model saved at 9\n",
      "val_losses[-1] = 0.4268165735527873, best_loss = 0.4435133713297546, model saved at 11\n",
      "iter 4\n",
      "val_losses[-1] = 1.0650304209440946, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.7711244560778141, best_loss = 1.0650304209440946, model saved at 1\n",
      "val_losses[-1] = 0.7710850559175014, best_loss = 0.7711244560778141, model saved at 2\n",
      "val_losses[-1] = 0.6029807221144438, best_loss = 0.7710850559175014, model saved at 3\n",
      "val_losses[-1] = 0.5543652914464474, best_loss = 0.6029807221144438, model saved at 5\n",
      "val_losses[-1] = 0.5062132770195603, best_loss = 0.5543652914464474, model saved at 6\n",
      "val_losses[-1] = 0.48654646389186385, best_loss = 0.5062132770195603, model saved at 7\n",
      "val_losses[-1] = 0.4455286206677556, best_loss = 0.48654646389186385, model saved at 8\n",
      "val_losses[-1] = 0.4227793101221323, best_loss = 0.4455286206677556, model saved at 11\n",
      "val_losses[-1] = 0.4175859985873103, best_loss = 0.4227793101221323, model saved at 17\n"
     ]
    }
   ],
   "source": [
    "loss_fns = (nn.CrossEntropyLoss(), nn.MSELoss())\n",
    "\n",
    "for loss_ratios in all_loss_ratios:\n",
    "    print(loss_ratios)\n",
    "    \n",
    "    models = []\n",
    "    for j in range(num_iter):\n",
    "        print(f\"iter {j}\")\n",
    "        dataset = torch.load(f\"data/amc_data_512_all_cases.pt\")\n",
    "        train_loader = dataset['train_loader']\n",
    "        val_loader = dataset['val_loader']\n",
    "        \n",
    "        torch.manual_seed(j)\n",
    "        model_mtl = amc_model_mtl(case=1)\n",
    "        model_mtl.cuda()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model_mtl.parameters(), lr=0.0005, weight_decay=0.0001)\n",
    "\n",
    "        model, losses, losses_mod, losses_snr, val_losses, val_losses_snr = train_mtl(model_mtl, optimizer, train_loader, \n",
    "                                                                                      val_loader, loss_fns, \n",
    "                                                                                      num_epochs=num_epochs, verbose=False, \n",
    "                                                                                      loss_ratios=loss_ratios, case=1)\n",
    "        \n",
    "        model_config = {\"weights\": model_mtl.state_dict(),\n",
    "                        \"losses\": losses,\n",
    "                        \"val_losses\": val_losses,\n",
    "                        \"losses_mod\": losses_mod,\n",
    "                        \"losses_snr\": losses_snr,\n",
    "                        \"val_losses_snr\": val_losses_snr}\n",
    "        \n",
    "        models.append(model_config)\n",
    "    torch.save(models, f'models/case_1/amc_{loss_ratios[0]}_{loss_ratios[1]}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "caf851a2-887c-47c6-9941-bff31f1a3810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.9)\n",
      "(1, 0.8)\n",
      "(1, 0.7)\n",
      "(1, 0.6)\n",
      "(1, 0.5)\n",
      "(1, 0.4)\n",
      "(1, 0.3)\n",
      "(1, 0.2)\n",
      "(1, 0.1)\n"
     ]
    }
   ],
   "source": [
    "# for models in sorted(os.listdir('models')):\n",
    "for loss_ratios in all_loss_ratios:\n",
    "    print(loss_ratios)\n",
    "    results = []\n",
    "    for model in torch.load(f'models/case_1/amc_{loss_ratios[0]}_{loss_ratios[1]}.pt'):\n",
    "        model_mtl = amc_model_mtl(case=1)\n",
    "        model_mtl.load_state_dict(model['weights'])\n",
    "        accs_mod = test_model_mtl(model_mtl, snr_range, num_frames=128)\n",
    "        \n",
    "        result = {\"accs_mod\": accs_mod,\n",
    "                   \"snr_range\": snr_range,\n",
    "                   \"model\": model}\n",
    "        results.append(result)\n",
    "    torch.save(results, f'results/case_1/amc_{loss_ratios[0]}_{loss_ratios[1]}.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed4843ef-bfc7-40b3-89fb-c985dd0394de",
   "metadata": {},
   "source": [
    "## Add MTL, case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "976ad751-287e-4a52-860e-b3d4c802d4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.9)\n",
      "iter 0\n",
      "val_losses[-1] = 1.0015151225030423, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.811079940572381, best_loss = 1.0015151225030423, model saved at 1\n",
      "val_losses[-1] = 0.6611665023490787, best_loss = 0.811079940572381, model saved at 2\n",
      "val_losses[-1] = 0.5986242212355137, best_loss = 0.6611665023490787, model saved at 3\n",
      "val_losses[-1] = 0.5393058797344565, best_loss = 0.5986242212355137, model saved at 4\n",
      "val_losses[-1] = 0.5179628105834126, best_loss = 0.5393058797344565, model saved at 5\n",
      "val_losses[-1] = 0.5138407708145678, best_loss = 0.5179628105834126, model saved at 6\n",
      "val_losses[-1] = 0.49173193480819466, best_loss = 0.5138407708145678, model saved at 7\n",
      "val_losses[-1] = 0.47604854963719845, best_loss = 0.49173193480819466, model saved at 9\n",
      "val_losses[-1] = 0.465094197448343, best_loss = 0.47604854963719845, model saved at 11\n",
      "val_losses[-1] = 0.4457358210347593, best_loss = 0.465094197448343, model saved at 12\n",
      "val_losses[-1] = 0.4426851633936167, best_loss = 0.4457358210347593, model saved at 13\n",
      "val_losses[-1] = 0.43217147802934053, best_loss = 0.4426851633936167, model saved at 14\n",
      "val_losses[-1] = 0.4305131183937192, best_loss = 0.43217147802934053, model saved at 20\n",
      "iter 1\n",
      "val_losses[-1] = 1.0379096824675798, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.904950513318181, best_loss = 1.0379096824675798, model saved at 1\n",
      "val_losses[-1] = 0.6256540285423398, best_loss = 0.904950513318181, model saved at 2\n",
      "val_losses[-1] = 0.5864451816305518, best_loss = 0.6256540285423398, model saved at 3\n",
      "val_losses[-1] = 0.5377299321815372, best_loss = 0.5864451816305518, model saved at 4\n",
      "val_losses[-1] = 0.49076636815443636, best_loss = 0.5377299321815372, model saved at 6\n",
      "val_losses[-1] = 0.4807636925019324, best_loss = 0.49076636815443636, model saved at 7\n",
      "val_losses[-1] = 0.4685513813048601, best_loss = 0.4807636925019324, model saved at 10\n",
      "val_losses[-1] = 0.46549666291102765, best_loss = 0.4685513813048601, model saved at 12\n",
      "val_losses[-1] = 0.4437729173339903, best_loss = 0.46549666291102765, model saved at 13\n",
      "val_losses[-1] = 0.43609134312719106, best_loss = 0.4437729173339903, model saved at 18\n",
      "val_losses[-1] = 0.42888028649613263, best_loss = 0.43609134312719106, model saved at 27\n",
      "iter 2\n",
      "val_losses[-1] = 1.0643432039767504, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8830436747521162, best_loss = 1.0643432039767504, model saved at 1\n",
      "val_losses[-1] = 0.7290787473320961, best_loss = 0.8830436747521162, model saved at 2\n",
      "val_losses[-1] = 0.6607498247176409, best_loss = 0.7290787473320961, model saved at 3\n",
      "val_losses[-1] = 0.6310772048309445, best_loss = 0.6607498247176409, model saved at 4\n",
      "val_losses[-1] = 0.5273866727948189, best_loss = 0.6310772048309445, model saved at 5\n",
      "val_losses[-1] = 0.49052608301863077, best_loss = 0.5273866727948189, model saved at 7\n",
      "val_losses[-1] = 0.46565643036738036, best_loss = 0.49052608301863077, model saved at 12\n",
      "val_losses[-1] = 0.459729774389416, best_loss = 0.46565643036738036, model saved at 15\n",
      "val_losses[-1] = 0.4454863560386002, best_loss = 0.459729774389416, model saved at 16\n",
      "val_losses[-1] = 0.43407008284702897, best_loss = 0.4454863560386002, model saved at 19\n",
      "val_losses[-1] = 0.4332552248612046, best_loss = 0.43407008284702897, model saved at 23\n",
      "val_losses[-1] = 0.42975471494719386, best_loss = 0.4332552248612046, model saved at 26\n",
      "iter 3\n",
      "val_losses[-1] = 0.9978930730372667, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.9837628126144409, best_loss = 0.9978930730372667, model saved at 1\n",
      "val_losses[-1] = 0.852485840767622, best_loss = 0.9837628126144409, model saved at 2\n",
      "val_losses[-1] = 0.6194604733958841, best_loss = 0.852485840767622, model saved at 3\n",
      "val_losses[-1] = 0.5670187011361122, best_loss = 0.6194604733958841, model saved at 4\n",
      "val_losses[-1] = 0.559209593385458, best_loss = 0.5670187011361122, model saved at 5\n",
      "val_losses[-1] = 0.49761911304667594, best_loss = 0.559209593385458, model saved at 7\n",
      "val_losses[-1] = 0.49721382595598695, best_loss = 0.49761911304667594, model saved at 10\n",
      "val_losses[-1] = 0.48608601503074167, best_loss = 0.49721382595598695, model saved at 11\n",
      "val_losses[-1] = 0.4593519037589431, best_loss = 0.48608601503074167, model saved at 12\n",
      "val_losses[-1] = 0.4462007631547749, best_loss = 0.4593519037589431, model saved at 17\n",
      "val_losses[-1] = 0.4435173384845257, best_loss = 0.4462007631547749, model saved at 18\n",
      "val_losses[-1] = 0.43426914932206273, best_loss = 0.4435173384845257, model saved at 19\n",
      "iter 4\n",
      "val_losses[-1] = 1.0206447578966618, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.9258669033646584, best_loss = 1.0206447578966618, model saved at 1\n",
      "val_losses[-1] = 0.6886273505166173, best_loss = 0.9258669033646584, model saved at 2\n",
      "val_losses[-1] = 0.6162386637181043, best_loss = 0.6886273505166173, model saved at 3\n",
      "val_losses[-1] = 0.6102897007018327, best_loss = 0.6162386637181043, model saved at 4\n",
      "val_losses[-1] = 0.5926786422729492, best_loss = 0.6102897007018327, model saved at 5\n",
      "val_losses[-1] = 0.5204282950609922, best_loss = 0.5926786422729492, model saved at 6\n",
      "val_losses[-1] = 0.49655320700258015, best_loss = 0.5204282950609922, model saved at 7\n",
      "val_losses[-1] = 0.4957863723859191, best_loss = 0.49655320700258015, model saved at 10\n",
      "val_losses[-1] = 0.46792423631995916, best_loss = 0.4957863723859191, model saved at 14\n",
      "val_losses[-1] = 0.45591711457818745, best_loss = 0.46792423631995916, model saved at 15\n",
      "val_losses[-1] = 0.44557528393343093, best_loss = 0.45591711457818745, model saved at 18\n",
      "val_losses[-1] = 0.4361227057874203, best_loss = 0.44557528393343093, model saved at 20\n",
      "val_losses[-1] = 0.4323207008652389, best_loss = 0.4361227057874203, model saved at 25\n",
      "(1, 0.8)\n",
      "iter 0\n",
      "val_losses[-1] = 1.0031853385269642, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.6858878327533603, best_loss = 1.0031853385269642, model saved at 1\n",
      "val_losses[-1] = 0.6075827179476618, best_loss = 0.6858878327533603, model saved at 2\n",
      "val_losses[-1] = 0.5652193760499358, best_loss = 0.6075827179476618, model saved at 3\n",
      "val_losses[-1] = 0.5507061235606671, best_loss = 0.5652193760499358, model saved at 4\n",
      "val_losses[-1] = 0.49828218715265393, best_loss = 0.5507061235606671, model saved at 5\n",
      "val_losses[-1] = 0.4811838342808187, best_loss = 0.49828218715265393, model saved at 7\n",
      "val_losses[-1] = 0.46731970058754085, best_loss = 0.4811838342808187, model saved at 8\n",
      "val_losses[-1] = 0.45935318861156704, best_loss = 0.46731970058754085, model saved at 9\n",
      "val_losses[-1] = 0.4577503501437604, best_loss = 0.45935318861156704, model saved at 10\n",
      "val_losses[-1] = 0.4530590086244047, best_loss = 0.4577503501437604, model saved at 13\n",
      "val_losses[-1] = 0.4452699082903564, best_loss = 0.4530590086244047, model saved at 14\n",
      "val_losses[-1] = 0.436701700091362, best_loss = 0.4452699082903564, model saved at 15\n",
      "val_losses[-1] = 0.4260879478417337, best_loss = 0.436701700091362, model saved at 20\n",
      "iter 1\n",
      "val_losses[-1] = 1.0306668922305107, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8978384077548981, best_loss = 1.0306668922305107, model saved at 1\n",
      "val_losses[-1] = 0.6380529027432204, best_loss = 0.8978384077548981, model saved at 2\n",
      "val_losses[-1] = 0.5636195095255971, best_loss = 0.6380529027432204, model saved at 4\n",
      "val_losses[-1] = 0.5201264807954431, best_loss = 0.5636195095255971, model saved at 6\n",
      "val_losses[-1] = 0.49344378523528576, best_loss = 0.5201264807954431, model saved at 7\n",
      "val_losses[-1] = 0.48419631374999883, best_loss = 0.49344378523528576, model saved at 10\n",
      "val_losses[-1] = 0.46612866343930365, best_loss = 0.48419631374999883, model saved at 12\n",
      "val_losses[-1] = 0.46201446279883385, best_loss = 0.46612866343930365, model saved at 15\n",
      "val_losses[-1] = 0.4559826192446053, best_loss = 0.46201446279883385, model saved at 17\n",
      "val_losses[-1] = 0.4484389521181583, best_loss = 0.4559826192446053, model saved at 18\n",
      "val_losses[-1] = 0.4483329742215574, best_loss = 0.4484389521181583, model saved at 24\n",
      "val_losses[-1] = 0.44171319296583533, best_loss = 0.4483329742215574, model saved at 27\n",
      "iter 2\n",
      "val_losses[-1] = 1.0422220140695573, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.9650164425373078, best_loss = 1.0422220140695573, model saved at 1\n",
      "val_losses[-1] = 0.6935426980257035, best_loss = 0.9650164425373078, model saved at 2\n",
      "val_losses[-1] = 0.6516435423865914, best_loss = 0.6935426980257035, model saved at 3\n",
      "val_losses[-1] = 0.586225695721805, best_loss = 0.6516435423865914, model saved at 4\n",
      "val_losses[-1] = 0.5392907032743096, best_loss = 0.586225695721805, model saved at 5\n",
      "val_losses[-1] = 0.5295090297237038, best_loss = 0.5392907032743096, model saved at 6\n",
      "val_losses[-1] = 0.5057563431560993, best_loss = 0.5295090297237038, model saved at 7\n",
      "val_losses[-1] = 0.49609322343021633, best_loss = 0.5057563431560993, model saved at 11\n",
      "val_losses[-1] = 0.48466620203107597, best_loss = 0.49609322343021633, model saved at 12\n",
      "val_losses[-1] = 0.4687495117075741, best_loss = 0.48466620203107597, model saved at 14\n",
      "val_losses[-1] = 0.4524925664998591, best_loss = 0.4687495117075741, model saved at 16\n",
      "val_losses[-1] = 0.4473980965092778, best_loss = 0.4524925664998591, model saved at 20\n",
      "val_losses[-1] = 0.43845057133585214, best_loss = 0.4473980965092778, model saved at 21\n",
      "val_losses[-1] = 0.43275176556780937, best_loss = 0.43845057133585214, model saved at 24\n",
      "iter 3\n",
      "val_losses[-1] = 1.0000932816416026, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.9859174344688654, best_loss = 1.0000932816416026, model saved at 1\n",
      "val_losses[-1] = 0.8530106123536825, best_loss = 0.9859174344688654, model saved at 2\n",
      "val_losses[-1] = 0.754028745740652, best_loss = 0.8530106123536825, model saved at 3\n",
      "val_losses[-1] = 0.5943230329081416, best_loss = 0.754028745740652, model saved at 4\n",
      "val_losses[-1] = 0.590431860089302, best_loss = 0.5943230329081416, model saved at 5\n",
      "val_losses[-1] = 0.5560305677354336, best_loss = 0.590431860089302, model saved at 6\n",
      "val_losses[-1] = 0.49879165142774584, best_loss = 0.5560305677354336, model saved at 7\n",
      "val_losses[-1] = 0.47532063433900473, best_loss = 0.49879165142774584, model saved at 10\n",
      "val_losses[-1] = 0.4558803976513445, best_loss = 0.47532063433900473, model saved at 12\n",
      "val_losses[-1] = 0.4458890907466412, best_loss = 0.4558803976513445, model saved at 14\n",
      "val_losses[-1] = 0.43900524349883197, best_loss = 0.4458890907466412, model saved at 17\n",
      "val_losses[-1] = 0.4302634520456195, best_loss = 0.43900524349883197, model saved at 22\n",
      "iter 4\n",
      "val_losses[-1] = 0.9948287196457386, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8312718905508518, best_loss = 0.9948287196457386, model saved at 1\n",
      "val_losses[-1] = 0.5993751826696098, best_loss = 0.8312718905508518, model saved at 2\n",
      "val_losses[-1] = 0.5177676327526569, best_loss = 0.5993751826696098, model saved at 4\n",
      "val_losses[-1] = 0.4987902401946485, best_loss = 0.5177676327526569, model saved at 7\n",
      "val_losses[-1] = 0.49586004968732594, best_loss = 0.4987902401946485, model saved at 8\n",
      "val_losses[-1] = 0.47633247412741186, best_loss = 0.49586004968732594, model saved at 10\n",
      "val_losses[-1] = 0.44202471943572164, best_loss = 0.47633247412741186, model saved at 15\n",
      "val_losses[-1] = 0.4333046774379909, best_loss = 0.44202471943572164, model saved at 18\n",
      "val_losses[-1] = 0.42687602723017337, best_loss = 0.4333046774379909, model saved at 25\n",
      "(1, 0.7)\n",
      "iter 0\n",
      "val_losses[-1] = 0.9949928242713213, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.7659328706562519, best_loss = 0.9949928242713213, model saved at 1\n",
      "val_losses[-1] = 0.640381726063788, best_loss = 0.7659328706562519, model saved at 2\n",
      "val_losses[-1] = 0.567121846973896, best_loss = 0.640381726063788, model saved at 3\n",
      "val_losses[-1] = 0.50705794878304, best_loss = 0.567121846973896, model saved at 4\n",
      "val_losses[-1] = 0.4997312449850142, best_loss = 0.50705794878304, model saved at 5\n",
      "val_losses[-1] = 0.47536970106884835, best_loss = 0.4997312449850142, model saved at 7\n",
      "val_losses[-1] = 0.46917686313390733, best_loss = 0.47536970106884835, model saved at 8\n",
      "val_losses[-1] = 0.45373041527345775, best_loss = 0.46917686313390733, model saved at 9\n",
      "val_losses[-1] = 0.45144724948331716, best_loss = 0.45373041527345775, model saved at 13\n",
      "val_losses[-1] = 0.44059839528054, best_loss = 0.45144724948331716, model saved at 15\n",
      "val_losses[-1] = 0.44057049266994, best_loss = 0.44059839528054, model saved at 16\n",
      "val_losses[-1] = 0.44024014566093683, best_loss = 0.44057049266994, model saved at 18\n",
      "val_losses[-1] = 0.4292564237490296, best_loss = 0.44024014566093683, model saved at 20\n",
      "iter 1\n",
      "val_losses[-1] = 1.018730791285634, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.901996024698019, best_loss = 1.018730791285634, model saved at 1\n",
      "val_losses[-1] = 0.6744869222864509, best_loss = 0.901996024698019, model saved at 2\n",
      "val_losses[-1] = 0.6347103366628289, best_loss = 0.6744869222864509, model saved at 3\n",
      "val_losses[-1] = 0.5400250365957617, best_loss = 0.6347103366628289, model saved at 4\n",
      "val_losses[-1] = 0.4940574369393289, best_loss = 0.5400250365957617, model saved at 6\n",
      "val_losses[-1] = 0.473107897490263, best_loss = 0.4940574369393289, model saved at 10\n",
      "val_losses[-1] = 0.4551404149271548, best_loss = 0.473107897490263, model saved at 17\n",
      "val_losses[-1] = 0.4418927991762757, best_loss = 0.4551404149271548, model saved at 18\n",
      "val_losses[-1] = 0.43625439880415795, best_loss = 0.4418927991762757, model saved at 24\n",
      "iter 2\n",
      "val_losses[-1] = 1.0052058406174182, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.9863715641200542, best_loss = 1.0052058406174182, model saved at 1\n",
      "val_losses[-1] = 0.6823223013430834, best_loss = 0.9863715641200542, model saved at 2\n",
      "val_losses[-1] = 0.6385556494817137, best_loss = 0.6823223013430834, model saved at 3\n",
      "val_losses[-1] = 0.5604955883696675, best_loss = 0.6385556494817137, model saved at 4\n",
      "val_losses[-1] = 0.5471997000277042, best_loss = 0.5604955883696675, model saved at 5\n",
      "val_losses[-1] = 0.5148365344852209, best_loss = 0.5471997000277042, model saved at 6\n",
      "val_losses[-1] = 0.508092388510704, best_loss = 0.5148365344852209, model saved at 8\n",
      "val_losses[-1] = 0.48180392226204277, best_loss = 0.508092388510704, model saved at 9\n",
      "val_losses[-1] = 0.4752412336878479, best_loss = 0.48180392226204277, model saved at 11\n",
      "val_losses[-1] = 0.4569658299908042, best_loss = 0.4752412336878479, model saved at 14\n",
      "val_losses[-1] = 0.4417315434664488, best_loss = 0.4569658299908042, model saved at 17\n",
      "val_losses[-1] = 0.43621806241571903, best_loss = 0.4417315434664488, model saved at 20\n",
      "val_losses[-1] = 0.42845880417153237, best_loss = 0.43621806241571903, model saved at 21\n",
      "iter 3\n",
      "val_losses[-1] = 1.0034446019679308, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.9760127365589142, best_loss = 1.0034446019679308, model saved at 1\n",
      "val_losses[-1] = 0.6917118931189179, best_loss = 0.9760127365589142, model saved at 2\n",
      "val_losses[-1] = 0.5764470007270575, best_loss = 0.6917118931189179, model saved at 3\n",
      "val_losses[-1] = 0.5224708697758615, best_loss = 0.5764470007270575, model saved at 6\n",
      "val_losses[-1] = 0.4672716801986098, best_loss = 0.5224708697758615, model saved at 7\n",
      "val_losses[-1] = 0.46577372755855323, best_loss = 0.4672716801986098, model saved at 9\n",
      "val_losses[-1] = 0.4593997864983976, best_loss = 0.46577372755855323, model saved at 11\n",
      "val_losses[-1] = 0.45541885159909723, best_loss = 0.4593997864983976, model saved at 12\n",
      "val_losses[-1] = 0.43744776025414467, best_loss = 0.45541885159909723, model saved at 13\n",
      "val_losses[-1] = 0.43463897574692967, best_loss = 0.43744776025414467, model saved at 22\n",
      "val_losses[-1] = 0.4319229026325047, best_loss = 0.43463897574692967, model saved at 23\n",
      "iter 4\n",
      "val_losses[-1] = 0.9951414868235589, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8383913990110159, best_loss = 0.9951414868235589, model saved at 1\n",
      "val_losses[-1] = 0.7147636128589511, best_loss = 0.8383913990110159, model saved at 2\n",
      "val_losses[-1] = 0.6131275800988079, best_loss = 0.7147636128589511, model saved at 3\n",
      "val_losses[-1] = 0.5370818443596364, best_loss = 0.6131275800988079, model saved at 4\n",
      "val_losses[-1] = 0.52293102145195, best_loss = 0.5370818443596364, model saved at 6\n",
      "val_losses[-1] = 0.5066880943253637, best_loss = 0.52293102145195, model saved at 7\n",
      "val_losses[-1] = 0.4759481785818934, best_loss = 0.5066880943253637, model saved at 8\n",
      "val_losses[-1] = 0.4757293378934264, best_loss = 0.4759481785818934, model saved at 10\n",
      "val_losses[-1] = 0.44612180199474094, best_loss = 0.4757293378934264, model saved at 15\n",
      "val_losses[-1] = 0.4338040415197611, best_loss = 0.44612180199474094, model saved at 18\n",
      "(1, 0.6)\n",
      "iter 0\n",
      "val_losses[-1] = 0.9999421369284391, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.7586379572749138, best_loss = 0.9999421369284391, model saved at 1\n",
      "val_losses[-1] = 0.711085525713861, best_loss = 0.7586379572749138, model saved at 2\n",
      "val_losses[-1] = 0.5449653547257185, best_loss = 0.711085525713861, model saved at 3\n",
      "val_losses[-1] = 0.5320183612406254, best_loss = 0.5449653547257185, model saved at 4\n",
      "val_losses[-1] = 0.4964141996577382, best_loss = 0.5320183612406254, model saved at 5\n",
      "val_losses[-1] = 0.48791853403672575, best_loss = 0.4964141996577382, model saved at 7\n",
      "val_losses[-1] = 0.48607500195503234, best_loss = 0.48791853403672575, model saved at 8\n",
      "val_losses[-1] = 0.46954589802771807, best_loss = 0.48607500195503234, model saved at 9\n",
      "val_losses[-1] = 0.4482660569250584, best_loss = 0.46954589802771807, model saved at 10\n",
      "val_losses[-1] = 0.4430320647545159, best_loss = 0.4482660569250584, model saved at 13\n",
      "val_losses[-1] = 0.4322832123376429, best_loss = 0.4430320647545159, model saved at 16\n",
      "val_losses[-1] = 0.42715318584814665, best_loss = 0.4322832123376429, model saved at 20\n",
      "iter 1\n",
      "val_losses[-1] = 1.0126411877572536, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.7437058717012406, best_loss = 1.0126411877572536, model saved at 1\n",
      "val_losses[-1] = 0.5862276691943407, best_loss = 0.7437058717012406, model saved at 2\n",
      "val_losses[-1] = 0.4995027519762516, best_loss = 0.5862276691943407, model saved at 4\n",
      "val_losses[-1] = 0.4927458534017205, best_loss = 0.4995027519762516, model saved at 6\n",
      "val_losses[-1] = 0.48848008215427396, best_loss = 0.4927458534017205, model saved at 7\n",
      "val_losses[-1] = 0.47674884777516124, best_loss = 0.48848008215427396, model saved at 8\n",
      "val_losses[-1] = 0.4499371529556811, best_loss = 0.47674884777516124, model saved at 10\n",
      "val_losses[-1] = 0.43186737298965455, best_loss = 0.4499371529556811, model saved at 17\n",
      "iter 2\n",
      "val_losses[-1] = 1.0284458123147489, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8515675570815802, best_loss = 1.0284458123147489, model saved at 1\n",
      "val_losses[-1] = 0.6037647230550647, best_loss = 0.8515675570815802, model saved at 2\n",
      "val_losses[-1] = 0.5591682584956288, best_loss = 0.6037647230550647, model saved at 3\n",
      "val_losses[-1] = 0.5147058619186282, best_loss = 0.5591682584956288, model saved at 4\n",
      "val_losses[-1] = 0.49782486278563737, best_loss = 0.5147058619186282, model saved at 5\n",
      "val_losses[-1] = 0.4944428185001016, best_loss = 0.49782486278563737, model saved at 6\n",
      "val_losses[-1] = 0.47314217071980236, best_loss = 0.4944428185001016, model saved at 9\n",
      "val_losses[-1] = 0.4662074063904583, best_loss = 0.47314217071980236, model saved at 11\n",
      "val_losses[-1] = 0.4381509837694466, best_loss = 0.4662074063904583, model saved at 14\n",
      "val_losses[-1] = 0.4331874806899577, best_loss = 0.4381509837694466, model saved at 16\n",
      "val_losses[-1] = 0.4316769514232874, best_loss = 0.4331874806899577, model saved at 17\n",
      "iter 3\n",
      "val_losses[-1] = 0.9953809048980474, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8880606926977634, best_loss = 0.9953809048980474, model saved at 1\n",
      "val_losses[-1] = 0.6592913581058383, best_loss = 0.8880606926977634, model saved at 2\n",
      "val_losses[-1] = 0.5559342790395021, best_loss = 0.6592913581058383, model saved at 3\n",
      "val_losses[-1] = 0.5272928042337298, best_loss = 0.5559342790395021, model saved at 4\n",
      "val_losses[-1] = 0.5048057059757411, best_loss = 0.5272928042337298, model saved at 6\n",
      "val_losses[-1] = 0.48257101979106665, best_loss = 0.5048057059757411, model saved at 7\n",
      "val_losses[-1] = 0.4820665896870196, best_loss = 0.48257101979106665, model saved at 11\n",
      "val_losses[-1] = 0.45507705099880696, best_loss = 0.4820665896870196, model saved at 13\n",
      "val_losses[-1] = 0.4460683261975646, best_loss = 0.45507705099880696, model saved at 14\n",
      "val_losses[-1] = 0.4302402399480343, best_loss = 0.4460683261975646, model saved at 18\n",
      "iter 4\n",
      "val_losses[-1] = 0.8954116251319647, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.6847445664927363, best_loss = 0.8954116251319647, model saved at 1\n",
      "val_losses[-1] = 0.5604288842529058, best_loss = 0.6847445664927363, model saved at 2\n",
      "val_losses[-1] = 0.5331534825265407, best_loss = 0.5604288842529058, model saved at 3\n",
      "val_losses[-1] = 0.4944844926707447, best_loss = 0.5331534825265407, model saved at 4\n",
      "val_losses[-1] = 0.47559662330895663, best_loss = 0.4944844926707447, model saved at 7\n",
      "val_losses[-1] = 0.4679472676478326, best_loss = 0.47559662330895663, model saved at 8\n",
      "val_losses[-1] = 0.4643103441223502, best_loss = 0.4679472676478326, model saved at 10\n",
      "val_losses[-1] = 0.4617341392673552, best_loss = 0.4643103441223502, model saved at 11\n",
      "val_losses[-1] = 0.43969644941389563, best_loss = 0.4617341392673552, model saved at 18\n",
      "val_losses[-1] = 0.4294984009116888, best_loss = 0.43969644941389563, model saved at 20\n",
      "val_losses[-1] = 0.42657942036166785, best_loss = 0.4294984009116888, model saved at 28\n",
      "(1, 0.5)\n",
      "iter 0\n",
      "val_losses[-1] = 0.9926689188927412, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.6012774296104908, best_loss = 0.9926689188927412, model saved at 1\n",
      "val_losses[-1] = 0.5535941235721111, best_loss = 0.6012774296104908, model saved at 2\n",
      "val_losses[-1] = 0.5118525406345725, best_loss = 0.5535941235721111, model saved at 3\n",
      "val_losses[-1] = 0.5014664059504866, best_loss = 0.5118525406345725, model saved at 4\n",
      "val_losses[-1] = 0.44872881639748813, best_loss = 0.5014664059504866, model saved at 7\n",
      "val_losses[-1] = 0.4481773448176682, best_loss = 0.44872881639748813, model saved at 8\n",
      "val_losses[-1] = 0.44220904894173146, best_loss = 0.4481773448176682, model saved at 9\n",
      "val_losses[-1] = 0.44218805376440284, best_loss = 0.44220904894173146, model saved at 12\n",
      "val_losses[-1] = 0.4264571962878108, best_loss = 0.44218805376440284, model saved at 15\n",
      "val_losses[-1] = 0.4262392809614539, best_loss = 0.4264571962878108, model saved at 20\n",
      "iter 1\n",
      "val_losses[-1] = 1.0199479285627604, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8627983495593071, best_loss = 1.0199479285627604, model saved at 1\n",
      "val_losses[-1] = 0.5825682356953621, best_loss = 0.8627983495593071, model saved at 2\n",
      "val_losses[-1] = 0.5430413631722331, best_loss = 0.5825682356953621, model saved at 3\n",
      "val_losses[-1] = 0.5017389267683029, best_loss = 0.5430413631722331, model saved at 4\n",
      "val_losses[-1] = 0.4725684148259461, best_loss = 0.5017389267683029, model saved at 7\n",
      "val_losses[-1] = 0.4556045450270176, best_loss = 0.4725684148259461, model saved at 10\n",
      "val_losses[-1] = 0.4506909463554621, best_loss = 0.4556045450270176, model saved at 17\n",
      "val_losses[-1] = 0.430991315189749, best_loss = 0.4506909463554621, model saved at 18\n",
      "iter 2\n",
      "val_losses[-1] = 1.0133640520274638, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.7995658650994301, best_loss = 1.0133640520274638, model saved at 1\n",
      "val_losses[-1] = 0.6207520283758641, best_loss = 0.7995658650994301, model saved at 2\n",
      "val_losses[-1] = 0.5892249066382647, best_loss = 0.6207520283758641, model saved at 3\n",
      "val_losses[-1] = 0.5172074700705707, best_loss = 0.5892249066382647, model saved at 4\n",
      "val_losses[-1] = 0.5005646066740155, best_loss = 0.5172074700705707, model saved at 5\n",
      "val_losses[-1] = 0.4916367942467332, best_loss = 0.5005646066740155, model saved at 8\n",
      "val_losses[-1] = 0.46653790334239603, best_loss = 0.4916367942467332, model saved at 9\n",
      "val_losses[-1] = 0.4594075673259795, best_loss = 0.46653790334239603, model saved at 12\n",
      "val_losses[-1] = 0.45050982548855245, best_loss = 0.4594075673259795, model saved at 14\n",
      "val_losses[-1] = 0.4323995146434754, best_loss = 0.45050982548855245, model saved at 16\n",
      "val_losses[-1] = 0.4319802281446755, best_loss = 0.4323995146434754, model saved at 21\n",
      "iter 3\n",
      "val_losses[-1] = 0.9918678618967534, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.8510844495147467, best_loss = 0.9918678618967534, model saved at 1\n",
      "val_losses[-1] = 0.6295766182243824, best_loss = 0.8510844495147467, model saved at 2\n",
      "val_losses[-1] = 0.5816822273656725, best_loss = 0.6295766182243824, model saved at 3\n",
      "val_losses[-1] = 0.530840375367552, best_loss = 0.5816822273656725, model saved at 4\n",
      "val_losses[-1] = 0.5158980356529355, best_loss = 0.530840375367552, model saved at 5\n",
      "val_losses[-1] = 0.5061593492515385, best_loss = 0.5158980356529355, model saved at 6\n",
      "val_losses[-1] = 0.4769588798284531, best_loss = 0.5061593492515385, model saved at 7\n",
      "val_losses[-1] = 0.4691300648264587, best_loss = 0.4769588798284531, model saved at 10\n",
      "val_losses[-1] = 0.46795523250475524, best_loss = 0.4691300648264587, model saved at 12\n",
      "val_losses[-1] = 0.4482444618828595, best_loss = 0.46795523250475524, model saved at 13\n",
      "val_losses[-1] = 0.44499674504622816, best_loss = 0.4482444618828595, model saved at 15\n",
      "val_losses[-1] = 0.43850062107667326, best_loss = 0.44499674504622816, model saved at 20\n",
      "val_losses[-1] = 0.4281707129441202, best_loss = 0.43850062107667326, model saved at 22\n",
      "iter 4\n",
      "val_losses[-1] = 1.0095272157341242, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.7790786761790514, best_loss = 1.0095272157341242, model saved at 1\n",
      "val_losses[-1] = 0.6052716318517923, best_loss = 0.7790786761790514, model saved at 2\n",
      "val_losses[-1] = 0.5331600144505501, best_loss = 0.6052716318517923, model saved at 4\n",
      "val_losses[-1] = 0.5269567197188735, best_loss = 0.5331600144505501, model saved at 6\n",
      "val_losses[-1] = 0.48391417115926744, best_loss = 0.5269567197188735, model saved at 7\n",
      "val_losses[-1] = 0.4799094564281404, best_loss = 0.48391417115926744, model saved at 8\n",
      "val_losses[-1] = 0.4787367507815361, best_loss = 0.4799094564281404, model saved at 10\n",
      "val_losses[-1] = 0.47210399676114323, best_loss = 0.4787367507815361, model saved at 11\n",
      "val_losses[-1] = 0.46782171651721, best_loss = 0.47210399676114323, model saved at 12\n",
      "val_losses[-1] = 0.445518156979233, best_loss = 0.46782171651721, model saved at 14\n",
      "val_losses[-1] = 0.4396150015294552, best_loss = 0.445518156979233, model saved at 15\n",
      "val_losses[-1] = 0.43714469112455845, best_loss = 0.4396150015294552, model saved at 18\n",
      "val_losses[-1] = 0.42436802266165613, best_loss = 0.43714469112455845, model saved at 20\n",
      "(1, 0.4)\n",
      "iter 0\n",
      "val_losses[-1] = 0.9825082007795573, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.695926908031106, best_loss = 0.9825082007795573, model saved at 1\n",
      "val_losses[-1] = 0.5605081919580698, best_loss = 0.695926908031106, model saved at 2\n",
      "val_losses[-1] = 0.5119019752368331, best_loss = 0.5605081919580698, model saved at 3\n",
      "val_losses[-1] = 0.455223991908133, best_loss = 0.5119019752368331, model saved at 7\n",
      "val_losses[-1] = 0.4459017083980143, best_loss = 0.455223991908133, model saved at 9\n",
      "val_losses[-1] = 0.4439073711633682, best_loss = 0.4459017083980143, model saved at 10\n",
      "val_losses[-1] = 0.43337507648393514, best_loss = 0.4439073711633682, model saved at 13\n",
      "val_losses[-1] = 0.43242815881967545, best_loss = 0.43337507648393514, model saved at 15\n",
      "val_losses[-1] = 0.42705163415521386, best_loss = 0.43242815881967545, model saved at 18\n",
      "iter 1\n",
      "val_losses[-1] = 1.0055025205016137, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.6581411227583885, best_loss = 1.0055025205016137, model saved at 1\n",
      "val_losses[-1] = 0.5589557940140366, best_loss = 0.6581411227583885, model saved at 2\n",
      "val_losses[-1] = 0.5093861913308502, best_loss = 0.5589557940140366, model saved at 4\n",
      "val_losses[-1] = 0.4834126309491694, best_loss = 0.5093861913308502, model saved at 5\n",
      "val_losses[-1] = 0.4829234270378947, best_loss = 0.4834126309491694, model saved at 7\n",
      "val_losses[-1] = 0.4709947590716183, best_loss = 0.4829234270378947, model saved at 9\n",
      "val_losses[-1] = 0.46219308087602257, best_loss = 0.4709947590716183, model saved at 10\n",
      "val_losses[-1] = 0.4616912627592683, best_loss = 0.46219308087602257, model saved at 12\n",
      "val_losses[-1] = 0.4513117576949298, best_loss = 0.4616912627592683, model saved at 13\n",
      "val_losses[-1] = 0.43832782916724683, best_loss = 0.4513117576949298, model saved at 15\n",
      "iter 2\n",
      "val_losses[-1] = 1.0621049497276545, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.741503118723631, best_loss = 1.0621049497276545, model saved at 1\n",
      "val_losses[-1] = 0.5818014046177268, best_loss = 0.741503118723631, model saved at 2\n",
      "val_losses[-1] = 0.5507381148636341, best_loss = 0.5818014046177268, model saved at 3\n",
      "val_losses[-1] = 0.5001968506723642, best_loss = 0.5507381148636341, model saved at 4\n",
      "val_losses[-1] = 0.47637638337910176, best_loss = 0.5001968506723642, model saved at 5\n",
      "val_losses[-1] = 0.46933406721800563, best_loss = 0.47637638337910176, model saved at 7\n",
      "val_losses[-1] = 0.45127634359523655, best_loss = 0.46933406721800563, model saved at 10\n",
      "val_losses[-1] = 0.4512311778962612, best_loss = 0.45127634359523655, model saved at 12\n",
      "val_losses[-1] = 0.42699264977127316, best_loss = 0.4512311778962612, model saved at 14\n",
      "iter 3\n",
      "val_losses[-1] = 0.990305607393384, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.7095352988690138, best_loss = 0.990305607393384, model saved at 1\n",
      "val_losses[-1] = 0.6079110080376268, best_loss = 0.7095352988690138, model saved at 2\n",
      "val_losses[-1] = 0.5651496279984712, best_loss = 0.6079110080376268, model saved at 3\n",
      "val_losses[-1] = 0.5192136140540242, best_loss = 0.5651496279984712, model saved at 4\n",
      "val_losses[-1] = 0.5093119172379375, best_loss = 0.5192136140540242, model saved at 5\n",
      "val_losses[-1] = 0.4777505214326084, best_loss = 0.5093119172379375, model saved at 7\n",
      "val_losses[-1] = 0.4560969345271587, best_loss = 0.4777505214326084, model saved at 9\n",
      "val_losses[-1] = 0.4351922403089702, best_loss = 0.4560969345271587, model saved at 13\n",
      "val_losses[-1] = 0.43363024964928626, best_loss = 0.4351922403089702, model saved at 14\n",
      "val_losses[-1] = 0.432949660345912, best_loss = 0.43363024964928626, model saved at 18\n",
      "iter 4\n",
      "val_losses[-1] = 0.910605913773179, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.6191936610266566, best_loss = 0.910605913773179, model saved at 1\n",
      "val_losses[-1] = 0.5494868231005967, best_loss = 0.6191936610266566, model saved at 2\n",
      "val_losses[-1] = 0.5146007925271988, best_loss = 0.5494868231005967, model saved at 4\n",
      "val_losses[-1] = 0.48211480062454937, best_loss = 0.5146007925271988, model saved at 7\n",
      "val_losses[-1] = 0.4539167100563645, best_loss = 0.48211480062454937, model saved at 8\n",
      "val_losses[-1] = 0.4483903945423663, best_loss = 0.4539167100563645, model saved at 11\n",
      "val_losses[-1] = 0.4454265839420259, best_loss = 0.4483903945423663, model saved at 13\n",
      "val_losses[-1] = 0.44105643769726155, best_loss = 0.4454265839420259, model saved at 14\n",
      "val_losses[-1] = 0.43008126337081193, best_loss = 0.44105643769726155, model saved at 15\n",
      "val_losses[-1] = 0.42162552047520874, best_loss = 0.43008126337081193, model saved at 20\n",
      "(1, 0.3)\n",
      "iter 0\n",
      "val_losses[-1] = 0.8057912062853575, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.6391269179061055, best_loss = 0.8057912062853575, model saved at 1\n",
      "val_losses[-1] = 0.5482409063726663, best_loss = 0.6391269179061055, model saved at 2\n",
      "val_losses[-1] = 0.49710310585796835, best_loss = 0.5482409063726663, model saved at 3\n",
      "val_losses[-1] = 0.48061276841908696, best_loss = 0.49710310585796835, model saved at 4\n",
      "val_losses[-1] = 0.46769217737019064, best_loss = 0.48061276841908696, model saved at 5\n",
      "val_losses[-1] = 0.44376742420718074, best_loss = 0.46769217737019064, model saved at 7\n",
      "val_losses[-1] = 0.4379441598430276, best_loss = 0.44376742420718074, model saved at 9\n",
      "val_losses[-1] = 0.43347065784037114, best_loss = 0.4379441598430276, model saved at 16\n",
      "val_losses[-1] = 0.4256633071228862, best_loss = 0.43347065784037114, model saved at 18\n",
      "iter 1\n",
      "val_losses[-1] = 0.9990867156535387, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.9327968414872885, best_loss = 0.9990867156535387, model saved at 1\n",
      "val_losses[-1] = 0.5522134078666567, best_loss = 0.9327968414872885, model saved at 2\n",
      "val_losses[-1] = 0.5457765348255634, best_loss = 0.5522134078666567, model saved at 3\n",
      "val_losses[-1] = 0.48379742009565235, best_loss = 0.5457765348255634, model saved at 4\n",
      "val_losses[-1] = 0.47078052079305055, best_loss = 0.48379742009565235, model saved at 5\n",
      "val_losses[-1] = 0.44846184775233267, best_loss = 0.47078052079305055, model saved at 8\n",
      "val_losses[-1] = 0.4373390944674611, best_loss = 0.44846184775233267, model saved at 9\n",
      "val_losses[-1] = 0.4339767556171864, best_loss = 0.4373390944674611, model saved at 15\n",
      "val_losses[-1] = 0.4323432410135865, best_loss = 0.4339767556171864, model saved at 17\n",
      "iter 2\n",
      "val_losses[-1] = 0.9091170191764831, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.6578982518985868, best_loss = 0.9091170191764831, model saved at 1\n",
      "val_losses[-1] = 0.5509609723463654, best_loss = 0.6578982518985868, model saved at 2\n",
      "val_losses[-1] = 0.494684353005141, best_loss = 0.5509609723463654, model saved at 4\n",
      "val_losses[-1] = 0.48453770065680146, best_loss = 0.494684353005141, model saved at 5\n",
      "val_losses[-1] = 0.4752022860571742, best_loss = 0.48453770065680146, model saved at 6\n",
      "val_losses[-1] = 0.4592311906628311, best_loss = 0.4752022860571742, model saved at 7\n",
      "val_losses[-1] = 0.4454384732991457, best_loss = 0.4592311906628311, model saved at 9\n",
      "val_losses[-1] = 0.4286028824746609, best_loss = 0.4454384732991457, model saved at 14\n",
      "iter 3\n",
      "val_losses[-1] = 0.7907027497887611, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.5737683121114969, best_loss = 0.7907027497887611, model saved at 1\n",
      "val_losses[-1] = 0.541448247525841, best_loss = 0.5737683121114969, model saved at 3\n",
      "val_losses[-1] = 0.4821090348064899, best_loss = 0.541448247525841, model saved at 5\n",
      "val_losses[-1] = 0.45320050390437244, best_loss = 0.4821090348064899, model saved at 7\n",
      "val_losses[-1] = 0.45232780892401936, best_loss = 0.45320050390437244, model saved at 12\n",
      "val_losses[-1] = 0.43333143116906286, best_loss = 0.45232780892401936, model saved at 13\n",
      "val_losses[-1] = 0.431919508241117, best_loss = 0.43333143116906286, model saved at 16\n",
      "val_losses[-1] = 0.4258898883126676, best_loss = 0.431919508241117, model saved at 18\n",
      "iter 4\n",
      "val_losses[-1] = 0.8131927412003279, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.5967554037459195, best_loss = 0.8131927412003279, model saved at 1\n",
      "val_losses[-1] = 0.5359382044523955, best_loss = 0.5967554037459195, model saved at 2\n",
      "val_losses[-1] = 0.4759470638819039, best_loss = 0.5359382044523955, model saved at 4\n",
      "val_losses[-1] = 0.4573175299912691, best_loss = 0.4759470638819039, model saved at 8\n",
      "val_losses[-1] = 0.45551821645349266, best_loss = 0.4573175299912691, model saved at 12\n",
      "val_losses[-1] = 0.45013253558427097, best_loss = 0.45551821645349266, model saved at 14\n",
      "val_losses[-1] = 0.42930459799245, best_loss = 0.45013253558427097, model saved at 15\n",
      "val_losses[-1] = 0.42671195026487113, best_loss = 0.42930459799245, model saved at 18\n",
      "(1, 0.2)\n",
      "iter 0\n",
      "val_losses[-1] = 0.7239080384373665, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.5742317374795676, best_loss = 0.7239080384373665, model saved at 1\n",
      "val_losses[-1] = 0.5305246230214834, best_loss = 0.5742317374795676, model saved at 2\n",
      "val_losses[-1] = 0.4966600310057402, best_loss = 0.5305246230214834, model saved at 3\n",
      "val_losses[-1] = 0.48387824948877095, best_loss = 0.4966600310057402, model saved at 5\n",
      "val_losses[-1] = 0.47281043995171784, best_loss = 0.48387824948877095, model saved at 7\n",
      "val_losses[-1] = 0.435705927759409, best_loss = 0.47281043995171784, model saved at 8\n",
      "val_losses[-1] = 0.4255750616081059, best_loss = 0.435705927759409, model saved at 13\n",
      "iter 1\n",
      "val_losses[-1] = 0.8364124841988086, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.5616735272109509, best_loss = 0.8364124841988086, model saved at 1\n",
      "val_losses[-1] = 0.524117368645966, best_loss = 0.5616735272109509, model saved at 2\n",
      "val_losses[-1] = 0.5131068270653486, best_loss = 0.524117368645966, model saved at 3\n",
      "val_losses[-1] = 0.48729617409408094, best_loss = 0.5131068270653486, model saved at 4\n",
      "val_losses[-1] = 0.461888138577342, best_loss = 0.48729617409408094, model saved at 5\n",
      "val_losses[-1] = 0.445507528539747, best_loss = 0.461888138577342, model saved at 8\n",
      "val_losses[-1] = 0.435267127212137, best_loss = 0.445507528539747, model saved at 11\n",
      "val_losses[-1] = 0.43435833202674984, best_loss = 0.435267127212137, model saved at 12\n",
      "val_losses[-1] = 0.426878316141665, best_loss = 0.43435833202674984, model saved at 13\n",
      "iter 2\n",
      "val_losses[-1] = 0.917498328909278, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.5869204236194492, best_loss = 0.917498328909278, model saved at 1\n",
      "val_losses[-1] = 0.5146658455953002, best_loss = 0.5869204236194492, model saved at 2\n",
      "val_losses[-1] = 0.5133256881497801, best_loss = 0.5146658455953002, model saved at 4\n",
      "val_losses[-1] = 0.4927233605645597, best_loss = 0.5133256881497801, model saved at 5\n",
      "val_losses[-1] = 0.4617372704669833, best_loss = 0.4927233605645597, model saved at 6\n",
      "val_losses[-1] = 0.4551738264039159, best_loss = 0.4617372704669833, model saved at 8\n",
      "val_losses[-1] = 0.4536720708012581, best_loss = 0.4551738264039159, model saved at 9\n",
      "val_losses[-1] = 0.43997920653782785, best_loss = 0.4536720708012581, model saved at 10\n",
      "val_losses[-1] = 0.4274399540387094, best_loss = 0.43997920653782785, model saved at 14\n",
      "iter 3\n",
      "val_losses[-1] = 0.7974414970725775, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.5793415738269687, best_loss = 0.7974414970725775, model saved at 1\n",
      "val_losses[-1] = 0.5216076850891114, best_loss = 0.5793415738269687, model saved at 2\n",
      "val_losses[-1] = 0.4907045751810074, best_loss = 0.5216076850891114, model saved at 5\n",
      "val_losses[-1] = 0.48413918698206543, best_loss = 0.4907045751810074, model saved at 6\n",
      "val_losses[-1] = 0.4556906968355179, best_loss = 0.48413918698206543, model saved at 7\n",
      "val_losses[-1] = 0.44014839101582764, best_loss = 0.4556906968355179, model saved at 10\n",
      "val_losses[-1] = 0.43454576851800086, best_loss = 0.44014839101582764, model saved at 13\n",
      "val_losses[-1] = 0.42869726549834014, best_loss = 0.43454576851800086, model saved at 14\n",
      "iter 4\n",
      "val_losses[-1] = 0.9326250713318587, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.5598766570910811, best_loss = 0.9326250713318587, model saved at 1\n",
      "val_losses[-1] = 0.5251151047646999, best_loss = 0.5598766570910811, model saved at 2\n",
      "val_losses[-1] = 0.4788589529693127, best_loss = 0.5251151047646999, model saved at 4\n",
      "val_losses[-1] = 0.46289161033928394, best_loss = 0.4788589529693127, model saved at 6\n",
      "val_losses[-1] = 0.45873150397092105, best_loss = 0.46289161033928394, model saved at 7\n",
      "val_losses[-1] = 0.4525600723922253, best_loss = 0.45873150397092105, model saved at 8\n",
      "val_losses[-1] = 0.4473629671148956, best_loss = 0.4525600723922253, model saved at 11\n",
      "val_losses[-1] = 0.4376908648759127, best_loss = 0.4473629671148956, model saved at 15\n",
      "(1, 0.1)\n",
      "iter 0\n",
      "val_losses[-1] = 0.6903558764606714, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.5031814344227314, best_loss = 0.6903558764606714, model saved at 1\n",
      "val_losses[-1] = 0.48689186898991466, best_loss = 0.5031814344227314, model saved at 3\n",
      "val_losses[-1] = 0.47775352345779537, best_loss = 0.48689186898991466, model saved at 4\n",
      "val_losses[-1] = 0.4776718565262854, best_loss = 0.47775352345779537, model saved at 5\n",
      "val_losses[-1] = 0.443462329916656, best_loss = 0.4776718565262854, model saved at 7\n",
      "val_losses[-1] = 0.4371579776518047, best_loss = 0.443462329916656, model saved at 8\n",
      "val_losses[-1] = 0.42635180661454797, best_loss = 0.4371579776518047, model saved at 9\n",
      "iter 1\n",
      "val_losses[-1] = 0.831503090262413, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.540063445456326, best_loss = 0.831503090262413, model saved at 1\n",
      "val_losses[-1] = 0.49534109476953747, best_loss = 0.540063445456326, model saved at 2\n",
      "val_losses[-1] = 0.46737970523536204, best_loss = 0.49534109476953747, model saved at 4\n",
      "val_losses[-1] = 0.46607717787846925, best_loss = 0.46737970523536204, model saved at 5\n",
      "val_losses[-1] = 0.4402416697703302, best_loss = 0.46607717787846925, model saved at 7\n",
      "val_losses[-1] = 0.4343208880163729, best_loss = 0.4402416697703302, model saved at 8\n",
      "val_losses[-1] = 0.43041061013937, best_loss = 0.4343208880163729, model saved at 17\n",
      "iter 2\n",
      "val_losses[-1] = 0.8494739782065153, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.6122001335024834, best_loss = 0.8494739782065153, model saved at 1\n",
      "val_losses[-1] = 0.5020592276006937, best_loss = 0.6122001335024834, model saved at 2\n",
      "val_losses[-1] = 0.48524347906932236, best_loss = 0.5020592276006937, model saved at 5\n",
      "val_losses[-1] = 0.4241284569725394, best_loss = 0.48524347906932236, model saved at 8\n",
      "iter 3\n",
      "val_losses[-1] = 0.7798367317765951, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.5457992199808359, best_loss = 0.7798367317765951, model saved at 1\n",
      "val_losses[-1] = 0.5358130855485796, best_loss = 0.5457992199808359, model saved at 2\n",
      "val_losses[-1] = 0.49672027472406627, best_loss = 0.5358130855485796, model saved at 3\n",
      "val_losses[-1] = 0.4881035923026502, best_loss = 0.49672027472406627, model saved at 4\n",
      "val_losses[-1] = 0.45243903091177345, best_loss = 0.4881035923026502, model saved at 5\n",
      "val_losses[-1] = 0.44967788066715003, best_loss = 0.45243903091177345, model saved at 6\n",
      "val_losses[-1] = 0.43857534509152174, best_loss = 0.44967788066715003, model saved at 9\n",
      "val_losses[-1] = 0.4350011674221605, best_loss = 0.43857534509152174, model saved at 10\n",
      "val_losses[-1] = 0.4272839281708002, best_loss = 0.4350011674221605, model saved at 14\n",
      "iter 4\n",
      "val_losses[-1] = 0.6964698756113649, best_loss = inf, model saved at 0\n",
      "val_losses[-1] = 0.5295440805144608, best_loss = 0.6964698756113649, model saved at 1\n",
      "val_losses[-1] = 0.5287038337439298, best_loss = 0.5295440805144608, model saved at 2\n",
      "val_losses[-1] = 0.526158255059272, best_loss = 0.5287038337439298, model saved at 3\n",
      "val_losses[-1] = 0.46853611171245574, best_loss = 0.526158255059272, model saved at 4\n",
      "val_losses[-1] = 0.45866435999050736, best_loss = 0.46853611171245574, model saved at 6\n",
      "val_losses[-1] = 0.4440821622498333, best_loss = 0.45866435999050736, model saved at 7\n",
      "val_losses[-1] = 0.4267634339630604, best_loss = 0.4440821622498333, model saved at 10\n",
      "val_losses[-1] = 0.4168817989528179, best_loss = 0.4267634339630604, model saved at 12\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "num_iter = 5\n",
    "\n",
    "loss_fns = (nn.CrossEntropyLoss(), nn.CrossEntropyLoss())\n",
    "\n",
    "for loss_ratios in all_loss_ratios:\n",
    "    print(loss_ratios)\n",
    "    \n",
    "    models = []\n",
    "    for j in range(num_iter):\n",
    "        print(f\"iter {j}\")\n",
    "        dataset = torch.load(f\"data/amc_data_512_all_cases.pt\")\n",
    "        train_loader = dataset['train_loader']\n",
    "        val_loader = dataset['val_loader']\n",
    "        \n",
    "        torch.manual_seed(j)\n",
    "        model_mtl = amc_model_mtl(case=2)\n",
    "        model_mtl.cuda()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model_mtl.parameters(), lr=0.0005, weight_decay=0.0001)\n",
    "\n",
    "        model, losses, losses_mod, losses_snr, val_losses, val_losses_snr = train_mtl(model_mtl, optimizer, train_loader, \n",
    "                                                                                      val_loader, loss_fns, \n",
    "                                                                                      num_epochs=num_epochs, verbose=False, \n",
    "                                                                                      loss_ratios=loss_ratios, case=2)\n",
    "        \n",
    "        model_config = {\"weights\": model_mtl.state_dict(),\n",
    "                        \"losses\": losses,\n",
    "                        \"val_losses\": val_losses,\n",
    "                        \"losses_mod\": losses_mod,\n",
    "                        \"losses_snr\": losses_snr,\n",
    "                        \"val_losses_snr\": val_losses_snr}\n",
    "        \n",
    "        models.append(model_config)\n",
    "    torch.save(models, f'models/case_2/amc_{loss_ratios[0]}_{loss_ratios[1]}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d31bc7e-498f-4037-9082-253b0e3e3255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.9)\n",
      "(1, 0.8)\n",
      "(1, 0.7)\n",
      "(1, 0.6)\n",
      "(1, 0.5)\n",
      "(1, 0.4)\n",
      "(1, 0.3)\n",
      "(1, 0.2)\n",
      "(1, 0.1)\n"
     ]
    }
   ],
   "source": [
    "snr_range = np.arange(-15,16,2)\n",
    "\n",
    "# for models in sorted(os.listdir('models')):\n",
    "for loss_ratios in all_loss_ratios:\n",
    "    print(loss_ratios)\n",
    "    results = []\n",
    "    for model in torch.load(f'models/case_2/amc_{loss_ratios[0]}_{loss_ratios[1]}.pt'):\n",
    "        model_mtl = amc_model_mtl(case=2)\n",
    "        model_mtl.load_state_dict(model['weights'])\n",
    "        accs_mod = test_model_mtl(model_mtl, snr_range, num_frames=128)\n",
    "        \n",
    "        result = {\"accs_mod\": accs_mod,\n",
    "                   \"snr_range\": snr_range,\n",
    "                   \"model\": model}\n",
    "        results.append(result)\n",
    "    torch.save(results, f'results/case_2/amc_{loss_ratios[0]}_{loss_ratios[1]}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4b75a9-27cc-447a-99c9-32807931d0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
