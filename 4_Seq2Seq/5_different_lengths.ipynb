{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Seq2Seq AMC+Demodulation on different length sequences\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from comms import awgn, make_rrc, modulate, demodulate, pulse_shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_baseline(snr, num_iter=512, M=2, sps = 4, span = 10):\n",
    "    \n",
    "    num_weights = sps*span+1\n",
    "    hrrc = make_rrc(num_weights=num_weights, fs=sps)\n",
    "    \n",
    "    ints = np.random.randint(low=0, high=M, size=(num_iter))\n",
    "    samples = modulate(ints, M=M)\n",
    "    \n",
    "    samples = pulse_shape(samples, hrrc, sps=sps)\n",
    "    samples = awgn(samples, snr, measured=True)\n",
    "\n",
    "    syms_filtered = np.convolve(samples, hrrc, mode='same')\n",
    "    extract_symbols = syms_filtered[np.arange(0,len(samples),sps)]\n",
    "    extract_symbols /= np.sqrt(np.mean(np.abs(extract_symbols)**2)) # normalize to unit power\n",
    "    \n",
    "    num_correct = sum(demodulate(extract_symbols, M=M) == ints)\n",
    "    \n",
    "    return num_correct/num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "snr_range = np.arange(-30,30)\n",
    "\n",
    "bpsk_baseline, qpsk_baseline, psk_baseline = [], [], []\n",
    "for snr in snr_range:\n",
    "    bpsk_baseline.append(calc_baseline(snr=snr, M=2, num_iter=1024*16))\n",
    "    qpsk_baseline.append(calc_baseline(snr=snr, M=4, num_iter=1024*16))\n",
    "#     psk_baseline.append(calc_baseline(snr=snr, M=8, num_iter=2048))\n",
    "\n",
    "bpsk_baseline = np.array(bpsk_baseline)\n",
    "qpsk_baseline = np.array(qpsk_baseline)\n",
    "# psk_baseline = np.array(psk_baseline)\n",
    "    \n",
    "plt.plot(snr_range, bpsk_baseline)\n",
    "plt.plot(snr_range, qpsk_baseline)\n",
    "# plt.plot(snr_range, psk_baseline)\n",
    "# plt.plot(snr_range, (bpsk_baseline+qpsk_baseline+psk_baseline)/3, '--')\n",
    "plt.grid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data\n",
    "\n",
    "We will need pairs of sequences of pulse shaped symbols and labels. \n",
    "\n",
    "E.g. when training for 5 pulse shaped symbols at sps=4, we will have a x_train inputs be of shape batch x 20 x 2, where the last dimension represents real and imaginary channels, and the corresponding y_train will be a sequence of batch x 5, for the 5 classes reprented in 5 pulse shaped symbols taking up 20 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(num_examples, seq_length, mod_order=4, sps=4, span=10, snr=50, normalize=True):\n",
    "    '''\n",
    "    This function produces a num_examples number of pulse shaped M-QAM waveforms, each\n",
    "    example is made up from seq_length*sps samples.\n",
    "    '''\n",
    "    \n",
    "    # Calculate number of weights based on sps and span, and create an RRC filter\n",
    "    num_weights= sps*span+1\n",
    "    hrrc = make_rrc(num_weights=num_weights, fs=sps)\n",
    "    \n",
    "    # Generate random bits\n",
    "    ints = np.random.randint(low=0, high=mod_order, size=(num_examples * seq_length))\n",
    "    \n",
    "    # Modulate according to mod order\n",
    "    samples = modulate(ints, M=mod_order)\n",
    "    \n",
    "    # Pulse shape / upsample the symbols\n",
    "    samples_pulse_shaped = pulse_shape(samples, hrrc, sps=sps)\n",
    "    \n",
    "    # Add noise. IMPORTANT -> measured=True (keep note of this when eveluating)\n",
    "    samples_noisy = awgn(samples_pulse_shaped, snr, measured=True)\n",
    "    \n",
    "    # Reshape the vectors into a new array of shape [num_examples, seq_length]\n",
    "    data = samples_noisy.reshape(num_examples,-1)\n",
    "    labels = ints.reshape(num_examples,-1)\n",
    "    \n",
    "    # Optionally normalize the waveform (helps with training)\n",
    "    if normalize:\n",
    "        data = (data/np.max(np.abs(data),axis=1)[:,None])\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "def to_tensors(x,y, onehot=False, gpu=False, batch_first=False, normalize=True, num_classes=None):\n",
    "    '''\n",
    "    This function takes the outputs from gen_data and does the necessary formatting\n",
    "    to make the data compatible with torch nn layers.\n",
    "    '''\n",
    "    \n",
    "    # Split complex x into real/imaginary along 3rd axis\n",
    "    # 2d -> 3d tensor\n",
    "    x = torch.FloatTensor(np.stack((x.real, x.imag), axis=2))\n",
    "    \n",
    "    # One hot encoding if doing MSE/CrossEntropy\n",
    "    # Otherwise NLLLoss will take the LongTensor\n",
    "    if onehot:\n",
    "        y = torch.LongTensor(y)\n",
    "        if num_classes:\n",
    "            y = F.one_hot(y, num_classes=num_classes)\n",
    "        else:\n",
    "            y = F.one_hot(y, num_classes=len(np.unique(y)))\n",
    "    else:\n",
    "        y = torch.LongTensor(y)\n",
    "        \n",
    "    # LSTM expects (seq, batch, feats) input shape\n",
    "    if not batch_first:\n",
    "        x = x.permute((1,0,2))\n",
    "        if onehot:\n",
    "            y = y.permute((1,0,2))\n",
    "        else:\n",
    "            y = y.unsqueeze(2).permute((1,0,2))\n",
    "    \n",
    "    # Move to cuda device memory if gpu is used\n",
    "    if gpu:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the encoder and decoder for our autoencoder structure. There's no init function for the decoder as it will only ever be using the one that the encoder provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=1, device='cpu', dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self, batch_size=32, device='cpu'):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device), \n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device))\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=1, device='cpu', dropout=0):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        rnn_output, hidden = self.rnn(input, hidden)\n",
    "        \n",
    "        rnn_output = self.dropout(rnn_output)\n",
    "        y = self.out(rnn_output)\n",
    "        \n",
    "        return y, hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Define the training and validation functions for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_train, y_train, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, tf_ratio=0, gpu=True, num_classes=6):\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # format to (seq x batch x feats) format\n",
    "    x_train = x_train.permute((1,0,2))\n",
    "    y_train = y_train.permute((1,0,2))\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    encoder_hidden = encoder.initHidden(batch_size=x_train.shape[1], device = 'cuda' if gpu else 'cpu')\n",
    "    _, encoder_hidden = encoder(x_train, encoder_hidden)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoder_input = torch.zeros((1,x_train.shape[1],num_classes))\n",
    "    if gpu:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    for i in range(y_train.shape[0]):\n",
    "        y, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "#         loss += criterion(y.squeeze(), y_train[i].squeeze().float()) # MSE\n",
    "        loss += criterion(y.squeeze(), y_train[i].argmax(axis=1)) # cross entropy\n",
    "#         loss += criterion(F.log_softmax(y.squeeze(),dim=1), y_train[i].argmax(axis=1)) #NLLLoss\n",
    "\n",
    "        # the higher the ratio the higher the chance of forcing\n",
    "        teacher_forcing = np.random.random() < tf_ratio\n",
    "\n",
    "        if teacher_forcing:\n",
    "            decoder_input = y_train[i].float().unsqueeze(0)\n",
    "        else:\n",
    "            decoder_input = y\n",
    "\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item()/y_train.shape[0]\n",
    "\n",
    "def val_step(x_val, y_val, encoder, decoder, criterion, gpu=True, num_classes=6):\n",
    "\n",
    "    x_val = x_val.permute((1,0,2))\n",
    "    y_val = y_val.permute((1,0,2))\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    encoder_hidden = encoder.initHidden(batch_size=x_val.shape[1], device = 'cuda' if gpu else 'cpu')\n",
    "    _, encoder_hidden = encoder(x_val, encoder_hidden)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoder_input = torch.zeros((1,x_val.shape[1],num_classes))\n",
    "    if gpu:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    for i in range(y_val.shape[0]):\n",
    "        y, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "#         loss += criterion(y.squeeze(), y_val[i].squeeze().float()) #mse\n",
    "        loss += criterion(y.squeeze(), y_val[i].argmax(axis=1)) # cross-entropy\n",
    "        decoder_input = y\n",
    "    \n",
    "    return loss.item()/y_val.shape[0]\n",
    "\n",
    "def gen_dataset(data_config, train_config):\n",
    "    \n",
    "    # if multiple SNR\n",
    "    if type(data_config['snr']) == np.ndarray:\n",
    "        datas, labels = [], []\n",
    "        for sn in data_config['snr']:\n",
    "            if data_config['num_examples_bpsk'] > 0:\n",
    "                x_bpsk, y_bpsk = gen_data(data_config['num_examples_bpsk'],\n",
    "                                data_config['seq_length'],\n",
    "                                sps=data_config['sps'],\n",
    "                                mod_order=2,\n",
    "                                snr=sn)\n",
    "                datas.append(x_bpsk)\n",
    "                labels.append(y_bpsk)\n",
    "            \n",
    "            if data_config['num_examples_qpsk'] > 0:\n",
    "                x_qpsk, y_qpsk = gen_data(data_config['num_examples_qpsk'],\n",
    "                                data_config['seq_length'],\n",
    "                                sps=data_config['sps'],\n",
    "                                mod_order=4,\n",
    "                                snr=sn)\n",
    "                \n",
    "                y_qpsk += 2\n",
    "                \n",
    "                datas.append(x_qpsk)\n",
    "                labels.append(y_qpsk)\n",
    "\n",
    "        x = np.concatenate((datas))\n",
    "        y = np.concatenate((labels))\n",
    "    \n",
    "    # if single SNR\n",
    "    else:\n",
    "        x_bpsk, y_bpsk = gen_data(data_config['num_examples_bpsk'], \n",
    "                                  data_config['seq_length'], \n",
    "                                  mod_order=2, sps=data_config['sps'], \n",
    "                                  snr=data_config['snr'])\n",
    "\n",
    "        x_qpsk, y_qpsk = gen_data(data_config['num_examples_qpsk'], \n",
    "                                  data_config['seq_length'], \n",
    "                                  mod_order=4, sps=data_config['sps'], \n",
    "                                  snr=data_config['snr'])\n",
    "        y_qpsk += 2\n",
    "        x = np.concatenate((x_bpsk, x_qpsk))\n",
    "        y = np.concatenate((y_bpsk, y_qpsk))\n",
    "\n",
    "    x, y = to_tensors(x, y, batch_first=True, gpu=True, onehot=True, num_classes=6)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(x, y)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=train_config['batch_size'], shuffle=True)\n",
    "    \n",
    "    return train_dataloader\n",
    "\n",
    "def inference(x_test, encoder, decoder, num_classes=6, seq_length=5, batch_size=32, device='cpu'):\n",
    "\n",
    "    encoder_hidden = encoder.initHidden(batch_size=batch_size, device=device)\n",
    "    _, encoder_hidden = encoder(x_test, encoder_hidden)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoder_outputs = torch.zeros((seq_length,batch_size,num_classes), device=device)\n",
    "    decoder_input = torch.zeros((1,batch_size,num_classes), device=device)\n",
    "    \n",
    "    for i in range(seq_length):\n",
    "        y, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        decoder_outputs[i] = y\n",
    "        decoder_input = y\n",
    "        \n",
    "    return decoder_outputs\n",
    "\n",
    "def eval_accuracy(encoder, decoder, snr_range, mod_order=2, num_examples=256, seq_length=5, sps=4, num_classes=6, batch_size=32, gpu=True):\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for snr in snr_range:\n",
    "\n",
    "        x, y = gen_data(num_examples, seq_length, mod_order=mod_order, sps=sps, snr=snr)\n",
    "        if mod_order == 4:\n",
    "            y += 2\n",
    "        if mod_order == 8:\n",
    "            y += 6\n",
    "        x, y = to_tensors(x, y, batch_first=True, gpu=gpu)\n",
    "\n",
    "        # create dataset and dataloader\n",
    "        test_dataset = torch.utils.data.TensorDataset(x, y)\n",
    "        test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        corrects, totals = 0, 0\n",
    "        for x,y in test_dataloader:\n",
    "            x = x.permute((1,0,2))\n",
    "            y = y.unsqueeze(2).permute((1,0,2))\n",
    "            y_hat = inference(x, encoder, decoder, num_classes=num_classes, seq_length=seq_length, device='cuda' if gpu else 'cpu')\n",
    "\n",
    "            corrects += torch.sum(y_hat.argmax(axis=2) == y.squeeze())\n",
    "\n",
    "            totals += y.numel()\n",
    "\n",
    "        accuracy = np.array(corrects)/totals\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    return accuracies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_validation_dataloader(snrs, seq_length, examples_per_snr=32):\n",
    "    ## bpsk\n",
    "    datas, labels = [], []\n",
    "    for snr in snrs:\n",
    "        x_bpsk, y_bpsk = gen_data(examples_per_snr, seq_length, mod_order=2, sps=4, snr=snr)\n",
    "        datas.append(x_bpsk)\n",
    "        labels.append(y_bpsk)\n",
    "        \n",
    "    x_bpsk = np.concatenate((datas))\n",
    "    y_bpsk = np.concatenate((labels))\n",
    "\n",
    "    x_val_bpsk, y_val_bpsk = to_tensors(x_bpsk, y_bpsk, batch_first=True, gpu=True, onehot=True, num_classes=6)\n",
    "    val_dataloader_bpsk = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_val_bpsk, y_val_bpsk), batch_size=32, shuffle=True)\n",
    "\n",
    "    ## qpsk\n",
    "    datas, labels = [], []\n",
    "    for snr in snrs:\n",
    "        x_qpsk, y_qpsk = gen_data(examples_per_snr, seq_length, mod_order=4, sps=4, snr=snr)\n",
    "        datas.append(x_qpsk)\n",
    "        labels.append(y_qpsk)\n",
    "        \n",
    "    x_qpsk = np.concatenate((datas))\n",
    "    y_qpsk = np.concatenate((labels))\n",
    "    y_qpsk += 2\n",
    "\n",
    "    x_val_qpsk, y_val_qpsk = to_tensors(x_qpsk, y_qpsk, batch_first=True, gpu=True, onehot=True, num_classes=6)\n",
    "    val_dataloader_qpsk = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_val_qpsk, y_val_qpsk), batch_size=32, shuffle=True)\n",
    "\n",
    "    return val_dataloader_bpsk, val_dataloader_qpsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(data_config, train_config, model_config=(16,3), gpu=True, dropout=0, tf_rate=0, numpy_seed=0, torch_seed=0, savepath='models/bpsk_qpsk_seq2seq.pt'):        \n",
    "        np.random.seed(numpy_seed)\n",
    "        train_dataloader = gen_dataset(data_config, train_config)\n",
    "        val_dataloader_bpsk, val_dataloader_qpsk = gen_validation_dataloader(data_config['snr'], data_config['seq_length'])\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        torch.manual_seed(torch_seed)\n",
    "        \n",
    "        # Create the encoder and decoder models\n",
    "        encoder = EncoderRNN(hidden_size=model_config[0], input_size=2, num_layers=model_config[1], dropout=dropout, device = 'cuda' if gpu else 'cpu')\n",
    "        decoder = DecoderRNN(hidden_size=model_config[0], input_size=6, num_layers=model_config[1], dropout=dropout, device = 'cuda' if gpu else 'cpu')\n",
    "\n",
    "        encoder.cuda()\n",
    "        decoder.cuda()\n",
    "\n",
    "        # Initialize the optimizers\n",
    "        encoder_optimizer = optim.Adam(encoder.parameters(), lr=train_config['learning_rate'], weight_decay=train_config['weight_decay'])\n",
    "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=train_config['learning_rate'], weight_decay=train_config['weight_decay'])\n",
    "\n",
    "        # Define empty lists where loss progress will be stored\n",
    "        losses, val_losses, val_losses_bpsk, val_losses_qpsk = [], [], [], []\n",
    "\n",
    "        best_loss = np.inf\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(train_config['num_epochs']):\n",
    "            train_loss = 0\n",
    "            for x_train,y_train in train_dataloader:\n",
    "\n",
    "                train_loss += train_step(x_train, y_train, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, tf_ratio=tf_rate, gpu=gpu)\n",
    "\n",
    "            losses.append(train_loss/len(train_dataloader))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for x_val,y_val in val_dataloader_bpsk:\n",
    "                    val_loss += val_step(x_val, y_val, encoder, decoder, criterion, gpu=gpu)\n",
    "                val_losses_bpsk.append(val_loss/len(val_dataloader_bpsk))\n",
    "\n",
    "                val_loss = 0\n",
    "                for x_val,y_val in val_dataloader_qpsk:\n",
    "                    val_loss += val_step(x_val, y_val, encoder, decoder, criterion, gpu=gpu)\n",
    "                val_losses_qpsk.append(val_loss/len(val_dataloader_qpsk))\n",
    "\n",
    "                val_loss = (val_losses_bpsk[-1] + val_losses_qpsk[-1])/2\n",
    "                val_losses.append(val_loss)\n",
    "\n",
    "                if val_loss < best_loss:\n",
    "                    encoder_weights = encoder.state_dict()\n",
    "                    decoder_weights = decoder.state_dict()\n",
    "                    best_loss = val_losses[-1]\n",
    "                    print(f\"{epoch}: Best loss {best_loss}, saving...\")\n",
    "\n",
    "        encoder.load_state_dict(encoder_weights)\n",
    "        decoder.load_state_dict(decoder_weights)\n",
    "        \n",
    "        train_results = {'encoder': encoder_weights,\n",
    "                        'decoder': decoder_weights,\n",
    "                        'losses': losses,\n",
    "                        'val_losses': val_losses,\n",
    "                        'val_losses_bpsk': val_losses_bpsk,\n",
    "                        'val_losses_qpsk': val_losses_qpsk}\n",
    "        \n",
    "        print(f\"Saving training results to {savepath}\")\n",
    "        torch.save(train_results, savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {'hidden_size': 16,\n",
    "                'num_layers': 3,\n",
    "                'input_size': 2,\n",
    "                'output_size': 6}\n",
    "\n",
    "# 'snr': np.array([-10, -5, 0, 5, 10, 15])\n",
    "data_config = {'snr': np.arange(-5,21,5),\n",
    "               'num_examples_bpsk': 2048,\n",
    "               'num_examples_qpsk': 2048,\n",
    "               'seq_length': 5,\n",
    "               'sps': 4,\n",
    "               'span': 10,\n",
    "               'num_examples_val': 256}\n",
    "\n",
    "train_config = {'num_epochs': 250,\n",
    "                'batch_size': 64,\n",
    "                'learning_rate': 3e-4,\n",
    "                'weight_decay': 0.0001,\n",
    "                'device': 'cuda',\n",
    "                'tf_ratio': 0}\n",
    "\n",
    "test_config = {'snr_range': np.arange(-30,30),\n",
    "               'num_iter': 1024}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Best loss 1.7323810895284018, saving...\n",
      "1: Best loss 1.2137800415356952, saving...\n",
      "2: Best loss 1.18321586449941, saving...\n",
      "3: Best loss 1.1574120759963988, saving...\n",
      "4: Best loss 1.151079249382019, saving...\n",
      "5: Best loss 1.1509033997853597, saving...\n",
      "6: Best loss 1.1347309350967407, saving...\n",
      "8: Best loss 1.110803731282552, saving...\n",
      "9: Best loss 1.1030313769976297, saving...\n",
      "10: Best loss 1.0963377912839252, saving...\n",
      "11: Best loss 1.084485948085785, saving...\n",
      "12: Best loss 1.0686754743258158, saving...\n",
      "13: Best loss 1.0517144759496053, saving...\n",
      "15: Best loss 1.035528286298116, saving...\n",
      "16: Best loss 1.0274872263272603, saving...\n",
      "17: Best loss 1.0199763695398967, saving...\n",
      "18: Best loss 1.0167801102002463, saving...\n",
      "19: Best loss 1.005366305510203, saving...\n",
      "20: Best loss 0.9922241886456807, saving...\n",
      "21: Best loss 0.9746516386667887, saving...\n",
      "22: Best loss 0.9609691977500916, saving...\n",
      "23: Best loss 0.9544221997261048, saving...\n",
      "24: Best loss 0.9458236932754517, saving...\n",
      "25: Best loss 0.9335791031519571, saving...\n",
      "26: Best loss 0.9137797117233277, saving...\n",
      "27: Best loss 0.9053871830304463, saving...\n",
      "28: Best loss 0.8912223537762959, saving...\n",
      "29: Best loss 0.8886382857958475, saving...\n",
      "30: Best loss 0.8757683316866558, saving...\n",
      "31: Best loss 0.8712171673774718, saving...\n",
      "32: Best loss 0.863546582063039, saving...\n",
      "33: Best loss 0.8590710957845052, saving...\n",
      "35: Best loss 0.8465537985165914, saving...\n",
      "37: Best loss 0.8373188853263853, saving...\n",
      "38: Best loss 0.8246853431065877, saving...\n",
      "39: Best loss 0.816366704305013, saving...\n",
      "40: Best loss 0.8080922961235046, saving...\n",
      "41: Best loss 0.800702585776647, saving...\n",
      "42: Best loss 0.7966808716456094, saving...\n",
      "44: Best loss 0.787383751074473, saving...\n",
      "45: Best loss 0.786056923866272, saving...\n",
      "46: Best loss 0.7765513221422832, saving...\n",
      "47: Best loss 0.7709214727083842, saving...\n",
      "48: Best loss 0.7637842615445455, saving...\n",
      "53: Best loss 0.7577912628650666, saving...\n",
      "54: Best loss 0.7497939209143321, saving...\n",
      "55: Best loss 0.7434532761573791, saving...\n",
      "56: Best loss 0.7376631458600362, saving...\n",
      "57: Best loss 0.7273574531078338, saving...\n",
      "59: Best loss 0.7223652640978495, saving...\n",
      "60: Best loss 0.7079722245534261, saving...\n",
      "61: Best loss 0.7024639189243316, saving...\n",
      "62: Best loss 0.6904018799463907, saving...\n",
      "63: Best loss 0.6855548838774363, saving...\n",
      "64: Best loss 0.6791022499402364, saving...\n",
      "65: Best loss 0.6707460304101308, saving...\n",
      "67: Best loss 0.6547800242900849, saving...\n",
      "68: Best loss 0.6499620536963144, saving...\n",
      "69: Best loss 0.6434341172377268, saving...\n",
      "70: Best loss 0.6373090962568919, saving...\n",
      "71: Best loss 0.6288996458053588, saving...\n",
      "73: Best loss 0.6176518857479096, saving...\n",
      "75: Best loss 0.6015583475430806, saving...\n",
      "76: Best loss 0.5902077456315358, saving...\n",
      "77: Best loss 0.5786624729633331, saving...\n",
      "78: Best loss 0.5768641511599223, saving...\n",
      "79: Best loss 0.5695753246545792, saving...\n",
      "80: Best loss 0.5622683664162954, saving...\n",
      "81: Best loss 0.5585822810729344, saving...\n",
      "82: Best loss 0.5540240089098613, saving...\n",
      "84: Best loss 0.5407550096511842, saving...\n",
      "86: Best loss 0.5384248058001201, saving...\n",
      "87: Best loss 0.530340881148974, saving...\n",
      "89: Best loss 0.5175123939911525, saving...\n",
      "90: Best loss 0.5144214739402135, saving...\n",
      "91: Best loss 0.5011043071746827, saving...\n",
      "93: Best loss 0.49945870737234754, saving...\n",
      "95: Best loss 0.4911778678496678, saving...\n",
      "96: Best loss 0.48562397658824924, saving...\n",
      "97: Best loss 0.47965615689754487, saving...\n",
      "98: Best loss 0.47341973980267843, saving...\n",
      "99: Best loss 0.4703508267800014, saving...\n",
      "100: Best loss 0.47010950644810995, saving...\n",
      "101: Best loss 0.4662932485342025, saving...\n",
      "102: Best loss 0.4627400606870652, saving...\n",
      "103: Best loss 0.45731678207715354, saving...\n",
      "106: Best loss 0.44814416468143464, saving...\n",
      "107: Best loss 0.4393848806619644, saving...\n",
      "108: Best loss 0.4381098171075185, saving...\n",
      "110: Best loss 0.431432557106018, saving...\n",
      "111: Best loss 0.43085104624430337, saving...\n",
      "113: Best loss 0.4266567587852478, saving...\n",
      "114: Best loss 0.42304679850737253, saving...\n",
      "115: Best loss 0.4162125289440155, saving...\n",
      "116: Best loss 0.4130902469158173, saving...\n",
      "117: Best loss 0.40931745270888015, saving...\n",
      "118: Best loss 0.4056897814075152, saving...\n",
      "119: Best loss 0.39740330576896665, saving...\n",
      "122: Best loss 0.3924760336677233, saving...\n",
      "124: Best loss 0.3838336234291395, saving...\n",
      "125: Best loss 0.3838107327620188, saving...\n",
      "126: Best loss 0.3740621358156205, saving...\n",
      "128: Best loss 0.37404631326595944, saving...\n",
      "129: Best loss 0.37302329738934836, saving...\n",
      "130: Best loss 0.3674926300843557, saving...\n",
      "131: Best loss 0.3553891013065974, saving...\n",
      "136: Best loss 0.3434726377328237, saving...\n",
      "139: Best loss 0.34093378980954486, saving...\n",
      "140: Best loss 0.33988190293312076, saving...\n",
      "141: Best loss 0.33135677402218183, saving...\n",
      "143: Best loss 0.3290037419646979, saving...\n",
      "147: Best loss 0.3189170822501182, saving...\n",
      "148: Best loss 0.3181959768136342, saving...\n",
      "152: Best loss 0.31499504546324414, saving...\n",
      "153: Best loss 0.31378685782353083, saving...\n",
      "154: Best loss 0.31030501127243043, saving...\n",
      "155: Best loss 0.30483979483445484, saving...\n",
      "156: Best loss 0.2978729233145714, saving...\n",
      "157: Best loss 0.2939881217976411, saving...\n",
      "159: Best loss 0.2864619995156924, saving...\n",
      "160: Best loss 0.2857735320925712, saving...\n",
      "161: Best loss 0.28473479449748995, saving...\n",
      "162: Best loss 0.2765968576073647, saving...\n",
      "163: Best loss 0.2729719827572505, saving...\n",
      "166: Best loss 0.2618654524286588, saving...\n",
      "169: Best loss 0.2612400762736798, saving...\n",
      "170: Best loss 0.2561604266365369, saving...\n",
      "171: Best loss 0.2520871269206206, saving...\n",
      "174: Best loss 0.251742118348678, saving...\n",
      "176: Best loss 0.2465117434660594, saving...\n",
      "177: Best loss 0.2436655024687449, saving...\n",
      "179: Best loss 0.24228961865107215, saving...\n",
      "180: Best loss 0.24116542239983874, saving...\n",
      "181: Best loss 0.24063727060953777, saving...\n",
      "182: Best loss 0.23973836600780485, saving...\n",
      "183: Best loss 0.23667046874761583, saving...\n",
      "185: Best loss 0.22841417690118154, saving...\n",
      "191: Best loss 0.2275683100024859, saving...\n",
      "195: Best loss 0.22740272631247838, saving...\n",
      "196: Best loss 0.22248037358125053, saving...\n",
      "202: Best loss 0.22206290935476622, saving...\n",
      "203: Best loss 0.21618392616510396, saving...\n",
      "204: Best loss 0.2134564017256101, saving...\n",
      "206: Best loss 0.2112854778766632, saving...\n",
      "211: Best loss 0.2090696021914482, saving...\n",
      "213: Best loss 0.20831133673588437, saving...\n",
      "217: Best loss 0.2016008629153172, saving...\n",
      "227: Best loss 0.19999853074550628, saving...\n",
      "228: Best loss 0.19886059910058976, saving...\n",
      "230: Best loss 0.19867355947693185, saving...\n",
      "231: Best loss 0.19774749179681142, saving...\n",
      "232: Best loss 0.19726190095146495, saving...\n",
      "240: Best loss 0.19409526387850445, saving...\n",
      "244: Best loss 0.1938723862171173, saving...\n",
      "246: Best loss 0.19144452114899954, saving...\n",
      "250: Best loss 0.18925391882658005, saving...\n",
      "251: Best loss 0.18873918006817497, saving...\n",
      "263: Best loss 0.18837705800930657, saving...\n",
      "265: Best loss 0.18717471361160276, saving...\n",
      "266: Best loss 0.1794088073074818, saving...\n",
      "297: Best loss 0.17864446043968202, saving...\n",
      "305: Best loss 0.17616943443814911, saving...\n",
      "308: Best loss 0.17417102803786597, saving...\n",
      "318: Best loss 0.17298978914817176, saving...\n",
      "328: Best loss 0.17196317116419474, saving...\n",
      "351: Best loss 0.1707417522867521, saving...\n",
      "370: Best loss 0.16979150424400963, saving...\n",
      "380: Best loss 0.16880137746532758, saving...\n",
      "386: Best loss 0.16761418854196866, saving...\n",
      "395: Best loss 0.16681329011917115, saving...\n",
      "409: Best loss 0.1658711835741997, saving...\n",
      "415: Best loss 0.16560915789256492, saving...\n",
      "432: Best loss 0.16533773293097814, saving...\n",
      "439: Best loss 0.16268024295568467, saving...\n",
      "450: Best loss 0.16175730166335903, saving...\n",
      "Saving training results to models/bpsk_qpsk_seq_16_3_10_trained_on_2048_500_epochs.pt\n",
      "0: Best loss 1.7328537835015192, saving...\n",
      "1: Best loss 1.187253517574734, saving...\n",
      "2: Best loss 1.1444037119547528, saving...\n",
      "3: Best loss 1.140741655561659, saving...\n",
      "4: Best loss 1.1272290494706896, saving...\n",
      "5: Best loss 1.121926392449273, saving...\n",
      "6: Best loss 1.1133129914601643, saving...\n",
      "7: Best loss 1.1029837767283122, saving...\n",
      "8: Best loss 1.0999345461527508, saving...\n",
      "9: Best loss 1.0934941132863363, saving...\n",
      "10: Best loss 1.0807967397901748, saving...\n",
      "12: Best loss 1.0750749958886041, saving...\n",
      "13: Best loss 1.0712836583455403, saving...\n",
      "14: Best loss 1.0621996455722387, saving...\n",
      "15: Best loss 1.0558845678965252, saving...\n",
      "16: Best loss 1.044388755162557, saving...\n",
      "17: Best loss 1.0383472018771702, saving...\n",
      "18: Best loss 1.0312775558895533, saving...\n",
      "20: Best loss 1.0218313588036432, saving...\n",
      "21: Best loss 1.017849244011773, saving...\n",
      "23: Best loss 1.0117026964823406, saving...\n",
      "24: Best loss 1.011250040266249, saving...\n",
      "25: Best loss 1.0082337167527942, saving...\n",
      "27: Best loss 1.002686876720852, saving...\n",
      "30: Best loss 0.9977324061923558, saving...\n",
      "31: Best loss 0.9971750895182292, saving...\n",
      "32: Best loss 0.9952582306332058, saving...\n",
      "33: Best loss 0.9917719629075792, saving...\n",
      "35: Best loss 0.9806694030761719, saving...\n",
      "36: Best loss 0.9804497136010064, saving...\n",
      "38: Best loss 0.9748911380767824, saving...\n",
      "40: Best loss 0.9690140618218315, saving...\n",
      "43: Best loss 0.9603006733788384, saving...\n",
      "44: Best loss 0.9572795126173232, saving...\n",
      "45: Best loss 0.9568054887983535, saving...\n",
      "46: Best loss 0.9510224713219537, saving...\n",
      "48: Best loss 0.9485708289676242, saving...\n",
      "49: Best loss 0.9473300404018827, saving...\n",
      "51: Best loss 0.9385328981611463, saving...\n",
      "52: Best loss 0.9364400492774115, saving...\n",
      "56: Best loss 0.9299843523237441, saving...\n",
      "58: Best loss 0.9247064643436007, saving...\n",
      "61: Best loss 0.923082865609063, saving...\n",
      "62: Best loss 0.921218834982978, saving...\n",
      "63: Best loss 0.9185018910302056, saving...\n",
      "64: Best loss 0.9171601798799303, saving...\n",
      "65: Best loss 0.9163578510284425, saving...\n",
      "67: Best loss 0.9152465264002483, saving...\n",
      "68: Best loss 0.91191975540585, saving...\n",
      "69: Best loss 0.9077714337242972, saving...\n",
      "70: Best loss 0.9049453602896795, saving...\n",
      "72: Best loss 0.899125501844618, saving...\n",
      "74: Best loss 0.8986006789737278, saving...\n",
      "75: Best loss 0.8930897209379409, saving...\n",
      "77: Best loss 0.8860341866811116, saving...\n",
      "78: Best loss 0.8799740711847941, saving...\n",
      "79: Best loss 0.8789191643397012, saving...\n",
      "80: Best loss 0.8710547606150312, saving...\n",
      "82: Best loss 0.8636933247248333, saving...\n",
      "83: Best loss 0.8629269123077392, saving...\n",
      "84: Best loss 0.8580139451556736, saving...\n",
      "86: Best loss 0.8502792729271782, saving...\n",
      "90: Best loss 0.8379035578833686, saving...\n",
      "91: Best loss 0.8358381986618042, saving...\n",
      "94: Best loss 0.8309711721208359, saving...\n",
      "95: Best loss 0.8285185628467135, saving...\n",
      "96: Best loss 0.8230200158225166, saving...\n",
      "98: Best loss 0.8186579333411322, saving...\n",
      "102: Best loss 0.8094259818394979, saving...\n",
      "103: Best loss 0.806651939286126, saving...\n",
      "104: Best loss 0.8010493781831529, saving...\n",
      "108: Best loss 0.7962492783864339, saving...\n",
      "109: Best loss 0.7883589824040731, saving...\n",
      "110: Best loss 0.7871504942576091, saving...\n",
      "111: Best loss 0.7867523484759861, saving...\n",
      "112: Best loss 0.7835871828926935, saving...\n",
      "113: Best loss 0.7768477280934653, saving...\n",
      "114: Best loss 0.7727692868974474, saving...\n",
      "115: Best loss 0.7720335827933418, saving...\n",
      "117: Best loss 0.7651201062732272, saving...\n",
      "118: Best loss 0.7581057760450575, saving...\n",
      "121: Best loss 0.7535467677646213, saving...\n",
      "122: Best loss 0.7504590961668226, saving...\n",
      "123: Best loss 0.7487483845816718, saving...\n",
      "124: Best loss 0.7434761338763767, saving...\n",
      "126: Best loss 0.7393886274761624, saving...\n",
      "128: Best loss 0.7349242819680109, saving...\n",
      "130: Best loss 0.7301959726545546, saving...\n",
      "131: Best loss 0.7283718638949924, saving...\n",
      "133: Best loss 0.7245682160059611, saving...\n",
      "135: Best loss 0.7202054791980319, saving...\n",
      "141: Best loss 0.7135008467568291, saving...\n",
      "145: Best loss 0.7103210926055908, saving...\n",
      "146: Best loss 0.7096059931649101, saving...\n",
      "147: Best loss 0.6992918835745917, saving...\n",
      "155: Best loss 0.6972587611940173, saving...\n",
      "157: Best loss 0.6927074988683064, saving...\n",
      "159: Best loss 0.6860555330912271, saving...\n",
      "164: Best loss 0.677477667066786, saving...\n",
      "169: Best loss 0.6751443041695488, saving...\n",
      "172: Best loss 0.6734869162241618, saving...\n",
      "176: Best loss 0.6717622677485149, saving...\n",
      "178: Best loss 0.671482687526279, saving...\n",
      "179: Best loss 0.6679235855738322, saving...\n",
      "182: Best loss 0.6647966305414835, saving...\n",
      "185: Best loss 0.6633332755830552, saving...\n",
      "189: Best loss 0.6539159456888833, saving...\n",
      "230: Best loss 0.648226965798272, saving...\n",
      "233: Best loss 0.6461260292265151, saving...\n",
      "237: Best loss 0.6348820050557454, saving...\n",
      "242: Best loss 0.6297833098305596, saving...\n",
      "244: Best loss 0.6232313540246752, saving...\n",
      "245: Best loss 0.6202244997024536, saving...\n",
      "247: Best loss 0.6186589545673794, saving...\n",
      "248: Best loss 0.6155824979146322, saving...\n",
      "252: Best loss 0.6127912958463033, saving...\n",
      "254: Best loss 0.6096751266055637, saving...\n",
      "256: Best loss 0.6091710872120327, saving...\n",
      "257: Best loss 0.6047187937630548, saving...\n",
      "259: Best loss 0.6032318313916525, saving...\n",
      "260: Best loss 0.6009085628721449, saving...\n",
      "263: Best loss 0.6004011352856955, saving...\n",
      "264: Best loss 0.5933576583862304, saving...\n",
      "269: Best loss 0.5916762351989746, saving...\n",
      "273: Best loss 0.5906940937042237, saving...\n",
      "275: Best loss 0.5891516274876064, saving...\n",
      "277: Best loss 0.589090883731842, saving...\n",
      "278: Best loss 0.5886122875743441, saving...\n",
      "280: Best loss 0.5868409872055054, saving...\n",
      "284: Best loss 0.5807985689904955, saving...\n",
      "287: Best loss 0.5797688351737128, saving...\n",
      "290: Best loss 0.5793244414859348, saving...\n",
      "292: Best loss 0.5731198747952779, saving...\n",
      "303: Best loss 0.5697420027520921, saving...\n",
      "307: Best loss 0.5693908373514811, saving...\n",
      "313: Best loss 0.5679328931702509, saving...\n",
      "315: Best loss 0.5627282526757982, saving...\n",
      "316: Best loss 0.5613055401378207, saving...\n",
      "317: Best loss 0.5594678958257039, saving...\n",
      "326: Best loss 0.5579575048552619, saving...\n",
      "329: Best loss 0.557702213525772, saving...\n",
      "331: Best loss 0.5541515641742283, saving...\n",
      "342: Best loss 0.5511432488759358, saving...\n",
      "345: Best loss 0.5500488519668579, saving...\n",
      "347: Best loss 0.5485537330309549, saving...\n",
      "349: Best loss 0.5439153843455844, saving...\n",
      "355: Best loss 0.5429496222072178, saving...\n",
      "366: Best loss 0.542187790075938, saving...\n",
      "368: Best loss 0.5382851680119833, saving...\n",
      "372: Best loss 0.5365313602818383, saving...\n",
      "374: Best loss 0.5341332448853386, saving...\n",
      "377: Best loss 0.5323495666186014, saving...\n",
      "379: Best loss 0.5311432573530408, saving...\n",
      "380: Best loss 0.5278618454933166, saving...\n",
      "391: Best loss 0.5254919158087836, saving...\n",
      "392: Best loss 0.5218305183781518, saving...\n",
      "400: Best loss 0.5213523785273235, saving...\n",
      "402: Best loss 0.5152411434385512, saving...\n",
      "406: Best loss 0.5151857727103764, saving...\n",
      "407: Best loss 0.5119081775347393, saving...\n",
      "408: Best loss 0.5114847587214576, saving...\n",
      "410: Best loss 0.5090657922956678, saving...\n",
      "411: Best loss 0.5076344887415568, saving...\n",
      "412: Best loss 0.5056259777810838, saving...\n",
      "414: Best loss 0.5019923382335238, saving...\n",
      "420: Best loss 0.501250500149197, saving...\n",
      "421: Best loss 0.5003791928291321, saving...\n",
      "425: Best loss 0.49958120716942683, saving...\n",
      "428: Best loss 0.4987944152620104, saving...\n",
      "430: Best loss 0.497155671649509, saving...\n",
      "431: Best loss 0.4931282069947984, saving...\n",
      "432: Best loss 0.4916450924343533, saving...\n",
      "438: Best loss 0.4896482898129357, saving...\n",
      "444: Best loss 0.48584349685245093, saving...\n",
      "448: Best loss 0.48518113162782456, saving...\n",
      "449: Best loss 0.47853042152192854, saving...\n",
      "456: Best loss 0.4784407522943285, saving...\n",
      "457: Best loss 0.4783387588130103, saving...\n",
      "460: Best loss 0.4725773824585809, saving...\n",
      "467: Best loss 0.47254928880267677, saving...\n",
      "468: Best loss 0.4696721540557014, saving...\n",
      "479: Best loss 0.4665659864743551, saving...\n",
      "480: Best loss 0.46301455365286937, saving...\n",
      "483: Best loss 0.46063643362787027, saving...\n",
      "486: Best loss 0.4584822363323635, saving...\n",
      "Saving training results to models/bpsk_qpsk_seq_16_3_15_trained_on_2048_500_epochs.pt\n",
      "0: Best loss 1.7330749273300172, saving...\n",
      "1: Best loss 1.1932793060938516, saving...\n",
      "2: Best loss 1.1471654017766317, saving...\n",
      "3: Best loss 1.132132605711619, saving...\n",
      "4: Best loss 1.1171611944834392, saving...\n",
      "6: Best loss 1.1117610176404318, saving...\n",
      "7: Best loss 1.1013627767562868, saving...\n",
      "8: Best loss 1.0959982077280679, saving...\n",
      "9: Best loss 1.0875415007273357, saving...\n",
      "11: Best loss 1.0838590343793235, saving...\n",
      "12: Best loss 1.0690102656682332, saving...\n",
      "14: Best loss 1.0682990550994873, saving...\n",
      "15: Best loss 1.0662215550740561, saving...\n",
      "18: Best loss 1.0574074188868203, saving...\n",
      "19: Best loss 1.0569398840268451, saving...\n",
      "25: Best loss 1.052031155427297, saving...\n",
      "29: Best loss 1.0516785661379495, saving...\n",
      "33: Best loss 1.0409163991610209, saving...\n",
      "34: Best loss 1.0385502537091573, saving...\n",
      "36: Best loss 1.0349265774091085, saving...\n",
      "39: Best loss 1.0287269790967306, saving...\n",
      "41: Best loss 1.024542764822642, saving...\n",
      "43: Best loss 1.022580595811208, saving...\n",
      "46: Best loss 1.0199911832809447, saving...\n",
      "48: Best loss 1.0046592632929485, saving...\n",
      "49: Best loss 1.0032187461853026, saving...\n",
      "53: Best loss 1.0009467522303264, saving...\n",
      "54: Best loss 0.9981483459472655, saving...\n",
      "55: Best loss 0.9931962966918946, saving...\n",
      "56: Best loss 0.987982201576233, saving...\n",
      "57: Best loss 0.9865578691164653, saving...\n",
      "58: Best loss 0.9843656102816265, saving...\n",
      "60: Best loss 0.9836939334869386, saving...\n",
      "62: Best loss 0.9800880153973897, saving...\n",
      "63: Best loss 0.9748967329661051, saving...\n",
      "64: Best loss 0.9742024978001913, saving...\n",
      "67: Best loss 0.9721439957618714, saving...\n",
      "69: Best loss 0.9656651576360065, saving...\n",
      "71: Best loss 0.962203633785248, saving...\n",
      "74: Best loss 0.9553659081459045, saving...\n",
      "75: Best loss 0.953368361790975, saving...\n",
      "77: Best loss 0.9477304498354594, saving...\n",
      "78: Best loss 0.9464105447133382, saving...\n",
      "79: Best loss 0.9438484072685243, saving...\n",
      "80: Best loss 0.9392216126124064, saving...\n",
      "82: Best loss 0.9338679671287538, saving...\n",
      "84: Best loss 0.932601050535838, saving...\n",
      "86: Best loss 0.9274900237719219, saving...\n",
      "87: Best loss 0.922179897626241, saving...\n",
      "88: Best loss 0.9217429041862489, saving...\n",
      "89: Best loss 0.9206385533014934, saving...\n",
      "90: Best loss 0.9171521782875061, saving...\n",
      "91: Best loss 0.9107595642407735, saving...\n",
      "95: Best loss 0.9107373873392742, saving...\n",
      "96: Best loss 0.9070097804069519, saving...\n",
      "97: Best loss 0.9061696151892346, saving...\n",
      "100: Best loss 0.9026099721590679, saving...\n",
      "101: Best loss 0.8963091989358265, saving...\n",
      "107: Best loss 0.8928012251853942, saving...\n",
      "108: Best loss 0.889126686255137, saving...\n",
      "109: Best loss 0.882747898499171, saving...\n",
      "111: Best loss 0.881822419166565, saving...\n",
      "113: Best loss 0.881035751104355, saving...\n",
      "115: Best loss 0.8794354319572448, saving...\n",
      "116: Best loss 0.8768000940481822, saving...\n",
      "119: Best loss 0.8706978122393292, saving...\n",
      "124: Best loss 0.8643851300080617, saving...\n",
      "127: Best loss 0.8594931145509085, saving...\n",
      "128: Best loss 0.8561670184135438, saving...\n",
      "130: Best loss 0.8532679696877797, saving...\n",
      "131: Best loss 0.8514475027720134, saving...\n",
      "133: Best loss 0.8465698182582855, saving...\n",
      "135: Best loss 0.8444938858350117, saving...\n",
      "136: Best loss 0.8430218875408172, saving...\n",
      "139: Best loss 0.8384403248627981, saving...\n",
      "140: Best loss 0.8381677707036337, saving...\n",
      "141: Best loss 0.8326137244701385, saving...\n",
      "143: Best loss 0.8313596487045288, saving...\n",
      "145: Best loss 0.8263061245282491, saving...\n",
      "146: Best loss 0.8229695638020834, saving...\n",
      "148: Best loss 0.8226534903049468, saving...\n",
      "149: Best loss 0.8199123640855154, saving...\n",
      "153: Best loss 0.8181243062019348, saving...\n",
      "154: Best loss 0.8168008685111999, saving...\n",
      "155: Best loss 0.8163263539473216, saving...\n",
      "159: Best loss 0.8140452822049459, saving...\n",
      "160: Best loss 0.810145596663157, saving...\n",
      "162: Best loss 0.8064277152220408, saving...\n",
      "165: Best loss 0.8047683914502461, saving...\n",
      "166: Best loss 0.803930429617564, saving...\n",
      "167: Best loss 0.8018457313378653, saving...\n",
      "168: Best loss 0.8001952071984609, saving...\n",
      "169: Best loss 0.7991728186607361, saving...\n",
      "170: Best loss 0.7990025262037913, saving...\n",
      "171: Best loss 0.7958200136820476, saving...\n",
      "172: Best loss 0.7925321678320567, saving...\n",
      "173: Best loss 0.7903737346331279, saving...\n",
      "176: Best loss 0.7896084308624268, saving...\n",
      "178: Best loss 0.7874316374460856, saving...\n",
      "184: Best loss 0.7785115639368694, saving...\n",
      "188: Best loss 0.7776044984658559, saving...\n",
      "189: Best loss 0.7754904945691428, saving...\n",
      "190: Best loss 0.7749907453854878, saving...\n",
      "192: Best loss 0.7747166633605957, saving...\n",
      "194: Best loss 0.7728796124458311, saving...\n",
      "198: Best loss 0.7676363408565522, saving...\n",
      "199: Best loss 0.7661175270875296, saving...\n",
      "200: Best loss 0.7635854721069335, saving...\n",
      "203: Best loss 0.7613974869251252, saving...\n",
      "204: Best loss 0.7548408806324005, saving...\n",
      "209: Best loss 0.7502503802378971, saving...\n",
      "213: Best loss 0.7493022680282593, saving...\n",
      "217: Best loss 0.7442155222098032, saving...\n",
      "218: Best loss 0.7415456225474675, saving...\n",
      "220: Best loss 0.7390376736720403, saving...\n",
      "225: Best loss 0.7379004985094071, saving...\n",
      "228: Best loss 0.737245633204778, saving...\n",
      "229: Best loss 0.7332992990811665, saving...\n",
      "232: Best loss 0.7293458888928094, saving...\n",
      "236: Best loss 0.7289133578538893, saving...\n",
      "240: Best loss 0.7270829111337662, saving...\n",
      "241: Best loss 0.7232121626536052, saving...\n",
      "242: Best loss 0.7183395127455393, saving...\n",
      "243: Best loss 0.7176538646221161, saving...\n",
      "247: Best loss 0.7109866281350454, saving...\n",
      "257: Best loss 0.7108344316482543, saving...\n",
      "260: Best loss 0.7088928172985713, saving...\n",
      "262: Best loss 0.7057782133420308, saving...\n",
      "264: Best loss 0.703247853120168, saving...\n",
      "265: Best loss 0.6993026425441106, saving...\n",
      "272: Best loss 0.6989213118950527, saving...\n",
      "274: Best loss 0.6955082565546036, saving...\n",
      "276: Best loss 0.6948194752136866, saving...\n",
      "277: Best loss 0.6927004983027776, saving...\n",
      "280: Best loss 0.6913247932990393, saving...\n",
      "283: Best loss 0.6904748151699702, saving...\n",
      "284: Best loss 0.6902419636646908, saving...\n",
      "285: Best loss 0.6889366368452708, saving...\n",
      "287: Best loss 0.6827890505393347, saving...\n",
      "289: Best loss 0.6813091496626537, saving...\n",
      "293: Best loss 0.6813061386346817, saving...\n",
      "295: Best loss 0.6798982361952464, saving...\n",
      "296: Best loss 0.6787956903378168, saving...\n",
      "301: Best loss 0.6764163047075271, saving...\n",
      "303: Best loss 0.6750783801078796, saving...\n",
      "304: Best loss 0.6705215533574422, saving...\n",
      "305: Best loss 0.669491054614385, saving...\n",
      "307: Best loss 0.6670641978581746, saving...\n",
      "310: Best loss 0.6665025452772776, saving...\n",
      "311: Best loss 0.6662935366233189, saving...\n",
      "315: Best loss 0.6632506122191747, saving...\n",
      "316: Best loss 0.6627056519190471, saving...\n",
      "318: Best loss 0.6592780898014704, saving...\n",
      "321: Best loss 0.6585953851540883, saving...\n",
      "326: Best loss 0.6561469813187918, saving...\n",
      "329: Best loss 0.6520328635970751, saving...\n",
      "339: Best loss 0.649803246061007, saving...\n",
      "342: Best loss 0.6483844478925069, saving...\n",
      "345: Best loss 0.6482875833908717, saving...\n",
      "346: Best loss 0.6448552906513214, saving...\n",
      "351: Best loss 0.6440348784128824, saving...\n",
      "355: Best loss 0.6430311312278113, saving...\n",
      "361: Best loss 0.6402132719755173, saving...\n",
      "366: Best loss 0.6376694326599439, saving...\n",
      "367: Best loss 0.6357348700364431, saving...\n",
      "372: Best loss 0.6349982688824336, saving...\n",
      "373: Best loss 0.6332949817180633, saving...\n",
      "375: Best loss 0.6332781434059143, saving...\n",
      "377: Best loss 0.6323245738943417, saving...\n",
      "378: Best loss 0.632047097881635, saving...\n",
      "382: Best loss 0.6293014392256737, saving...\n",
      "394: Best loss 0.6283514966567357, saving...\n",
      "395: Best loss 0.6235031336545944, saving...\n",
      "397: Best loss 0.6234550476074219, saving...\n",
      "403: Best loss 0.6232195546229681, saving...\n",
      "408: Best loss 0.6203119397163391, saving...\n",
      "410: Best loss 0.6182777533928553, saving...\n",
      "418: Best loss 0.6182640969753265, saving...\n",
      "419: Best loss 0.6147570719321568, saving...\n",
      "425: Best loss 0.6115409970283507, saving...\n",
      "427: Best loss 0.6103587706883749, saving...\n",
      "436: Best loss 0.6085150430599847, saving...\n",
      "442: Best loss 0.608439335723718, saving...\n",
      "449: Best loss 0.6082877869407335, saving...\n",
      "450: Best loss 0.6062073628107707, saving...\n",
      "451: Best loss 0.6043247366944949, saving...\n",
      "456: Best loss 0.5987432127197584, saving...\n",
      "464: Best loss 0.5962036276857059, saving...\n",
      "471: Best loss 0.5960084130366643, saving...\n",
      "474: Best loss 0.5950714210669199, saving...\n",
      "479: Best loss 0.593415933350722, saving...\n",
      "482: Best loss 0.5898579592506091, saving...\n",
      "489: Best loss 0.5890315388639769, saving...\n",
      "494: Best loss 0.5864639063676197, saving...\n",
      "495: Best loss 0.5862615803877512, saving...\n",
      "498: Best loss 0.5860536510745684, saving...\n",
      "499: Best loss 0.5849713345368703, saving...\n",
      "Saving training results to models/bpsk_qpsk_seq_16_3_20_trained_on_2048_500_epochs.pt\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = [10,15,20]\n",
    "model_configs = [(16,3), (128,3)]\n",
    "train_config['num_epochs'] = 250\n",
    "for seq_length in seq_lengths:\n",
    "    for model_config in model_configs:\n",
    "        data_config['seq_length'] = seq_length\n",
    "\n",
    "        train_network(data_config, train_config, model_config=(16,3), gpu=True, dropout=0, tf_rate=0, numpy_seed=0, torch_seed=0, \n",
    "                      savepath=f\"models/bpsk_qpsk_seq_{model_config[0]}_{model_config[1]}_{data_config['seq_length']}_trained_on_2048_{train_config['num_epochs']}_epochs.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Best loss 1.7268521467844646, saving...\n",
      "1: Best loss 1.252894691626231, saving...\n",
      "2: Best loss 1.1950663288434347, saving...\n",
      "3: Best loss 1.1888446966807047, saving...\n",
      "4: Best loss 1.1660877307256063, saving...\n",
      "5: Best loss 1.1419851541519166, saving...\n",
      "6: Best loss 1.1371471444765726, saving...\n",
      "7: Best loss 1.131692659854889, saving...\n",
      "8: Best loss 1.1121546188990274, saving...\n",
      "9: Best loss 1.1084344903628032, saving...\n",
      "12: Best loss 1.0831317901611328, saving...\n",
      "13: Best loss 1.0748254855473833, saving...\n",
      "14: Best loss 1.0606040636698404, saving...\n",
      "16: Best loss 1.0397382656733194, saving...\n",
      "18: Best loss 1.036305022239685, saving...\n",
      "19: Best loss 1.0294407963752745, saving...\n",
      "20: Best loss 1.0269316236178079, saving...\n",
      "21: Best loss 1.016840887069702, saving...\n",
      "22: Best loss 1.008637837568919, saving...\n",
      "23: Best loss 0.9971877694129943, saving...\n",
      "26: Best loss 0.9900003393491109, saving...\n",
      "27: Best loss 0.9794383247693381, saving...\n",
      "29: Best loss 0.9611142436663309, saving...\n",
      "30: Best loss 0.9577998916308086, saving...\n",
      "31: Best loss 0.9482747316360474, saving...\n",
      "33: Best loss 0.9383051514625549, saving...\n",
      "34: Best loss 0.9374305844306945, saving...\n",
      "35: Best loss 0.9216992994149527, saving...\n",
      "36: Best loss 0.9158818483352661, saving...\n",
      "37: Best loss 0.9064106384913126, saving...\n",
      "38: Best loss 0.8795894881089529, saving...\n",
      "40: Best loss 0.8642293473084768, saving...\n",
      "41: Best loss 0.8528820474942526, saving...\n",
      "42: Best loss 0.8444919069608052, saving...\n",
      "43: Best loss 0.8440709034601848, saving...\n",
      "44: Best loss 0.8227718492348989, saving...\n",
      "45: Best loss 0.802989141146342, saving...\n",
      "46: Best loss 0.7828771054744721, saving...\n",
      "47: Best loss 0.7774131377538046, saving...\n",
      "48: Best loss 0.760367621978124, saving...\n",
      "49: Best loss 0.7517811169226963, saving...\n",
      "50: Best loss 0.7371169924736024, saving...\n",
      "51: Best loss 0.7183602203925451, saving...\n",
      "52: Best loss 0.7059741546710332, saving...\n",
      "53: Best loss 0.7042310684919356, saving...\n",
      "54: Best loss 0.6970742553472519, saving...\n",
      "55: Best loss 0.6941411584615707, saving...\n",
      "56: Best loss 0.6855994155009588, saving...\n",
      "57: Best loss 0.675931578874588, saving...\n",
      "58: Best loss 0.6679355233907699, saving...\n",
      "59: Best loss 0.6593455900748569, saving...\n",
      "60: Best loss 0.6521960993607839, saving...\n",
      "61: Best loss 0.646113714079062, saving...\n",
      "63: Best loss 0.6435200254122416, saving...\n",
      "65: Best loss 0.6421996980905533, saving...\n",
      "66: Best loss 0.6332694793740908, saving...\n",
      "67: Best loss 0.6210818469524383, saving...\n",
      "68: Best loss 0.6142411962151528, saving...\n",
      "69: Best loss 0.6123363273839156, saving...\n",
      "70: Best loss 0.6047565668821335, saving...\n",
      "71: Best loss 0.5923590580622355, saving...\n",
      "75: Best loss 0.5813156455755234, saving...\n",
      "76: Best loss 0.5677746926744778, saving...\n",
      "77: Best loss 0.5676967749993006, saving...\n",
      "78: Best loss 0.5671181802948315, saving...\n",
      "79: Best loss 0.5645601051549117, saving...\n",
      "81: Best loss 0.5587961872418721, saving...\n",
      "82: Best loss 0.5513532280921936, saving...\n",
      "84: Best loss 0.5446803991993268, saving...\n",
      "91: Best loss 0.5426377316315969, saving...\n",
      "92: Best loss 0.5345659479498863, saving...\n",
      "94: Best loss 0.5330142299334208, saving...\n",
      "95: Best loss 0.5230190450946489, saving...\n",
      "98: Best loss 0.5186119203766186, saving...\n",
      "101: Best loss 0.5100715801119804, saving...\n",
      "103: Best loss 0.5073480481902758, saving...\n",
      "106: Best loss 0.5069566915432612, saving...\n",
      "108: Best loss 0.5045933177073796, saving...\n",
      "109: Best loss 0.5009994799892108, saving...\n",
      "110: Best loss 0.4914880598584811, saving...\n",
      "115: Best loss 0.4875977516174317, saving...\n",
      "118: Best loss 0.47507654999693233, saving...\n",
      "122: Best loss 0.46994880909721054, saving...\n",
      "124: Best loss 0.46538342088460927, saving...\n",
      "125: Best loss 0.46257326056559883, saving...\n",
      "128: Best loss 0.45929456278681746, saving...\n",
      "130: Best loss 0.4575291340549787, saving...\n",
      "132: Best loss 0.4552655411263307, saving...\n",
      "134: Best loss 0.45463134050369264, saving...\n",
      "135: Best loss 0.44511332760254535, saving...\n",
      "136: Best loss 0.44256449838479356, saving...\n",
      "142: Best loss 0.43331414634982746, saving...\n",
      "143: Best loss 0.43124959642688426, saving...\n",
      "145: Best loss 0.42872176518042887, saving...\n",
      "146: Best loss 0.4267372657855351, saving...\n",
      "148: Best loss 0.418947742631038, saving...\n",
      "150: Best loss 0.4160351316134135, saving...\n",
      "152: Best loss 0.41056791742642723, saving...\n",
      "154: Best loss 0.4084994932015738, saving...\n",
      "156: Best loss 0.40083168148994447, saving...\n",
      "158: Best loss 0.398478727042675, saving...\n",
      "160: Best loss 0.38891371513406436, saving...\n",
      "161: Best loss 0.38570714741945267, saving...\n",
      "162: Best loss 0.38306237484018013, saving...\n",
      "163: Best loss 0.3799943017462889, saving...\n",
      "164: Best loss 0.37704292486111324, saving...\n",
      "165: Best loss 0.3734610897799333, saving...\n",
      "166: Best loss 0.36603388016422594, saving...\n",
      "170: Best loss 0.35423821670313677, saving...\n",
      "171: Best loss 0.35285758301615716, saving...\n",
      "173: Best loss 0.3434063896536827, saving...\n",
      "174: Best loss 0.34249159619212155, saving...\n",
      "175: Best loss 0.34162295907735823, saving...\n",
      "177: Best loss 0.33480789487560586, saving...\n",
      "179: Best loss 0.333628257488211, saving...\n",
      "180: Best loss 0.3299963851769765, saving...\n",
      "183: Best loss 0.32597342754403746, saving...\n",
      "184: Best loss 0.3227962305148443, saving...\n",
      "186: Best loss 0.3173655644059181, saving...\n",
      "189: Best loss 0.31166008015473684, saving...\n",
      "191: Best loss 0.31111628785729406, saving...\n",
      "192: Best loss 0.31110570877790455, saving...\n",
      "194: Best loss 0.30822455932696663, saving...\n",
      "195: Best loss 0.30644193142652515, saving...\n",
      "197: Best loss 0.3001150511205196, saving...\n",
      "202: Best loss 0.2923293096323808, saving...\n",
      "205: Best loss 0.291590204834938, saving...\n",
      "206: Best loss 0.2910785923401515, saving...\n",
      "207: Best loss 0.2864520851522684, saving...\n",
      "209: Best loss 0.286287305255731, saving...\n",
      "211: Best loss 0.28240498652060825, saving...\n",
      "213: Best loss 0.28049736395478253, saving...\n",
      "214: Best loss 0.27771329432725905, saving...\n",
      "216: Best loss 0.2773699847360452, saving...\n",
      "217: Best loss 0.2771379876260956, saving...\n",
      "221: Best loss 0.2757818805674712, saving...\n",
      "223: Best loss 0.2751633589466413, saving...\n",
      "227: Best loss 0.26974222660064695, saving...\n",
      "232: Best loss 0.2669489602247874, saving...\n",
      "235: Best loss 0.26079429810245836, saving...\n",
      "240: Best loss 0.2588597645362218, saving...\n",
      "251: Best loss 0.2577343245347341, saving...\n",
      "257: Best loss 0.25437464242180186, saving...\n",
      "259: Best loss 0.2539926260709762, saving...\n",
      "263: Best loss 0.25170286993185675, saving...\n",
      "266: Best loss 0.2498262127240499, saving...\n",
      "270: Best loss 0.24620008344451585, saving...\n",
      "272: Best loss 0.24383265661696593, saving...\n",
      "274: Best loss 0.24246328125397365, saving...\n",
      "280: Best loss 0.24165604909261068, saving...\n",
      "294: Best loss 0.23967489898204808, saving...\n",
      "304: Best loss 0.23798134985069433, saving...\n",
      "305: Best loss 0.23779534623026846, saving...\n",
      "307: Best loss 0.23766334503889083, saving...\n",
      "308: Best loss 0.2330666611591975, saving...\n",
      "321: Best loss 0.23262298131982484, saving...\n",
      "322: Best loss 0.23047380546728768, saving...\n",
      "330: Best loss 0.22932816805938883, saving...\n",
      "344: Best loss 0.2273620774348577, saving...\n",
      "349: Best loss 0.22336209540565807, saving...\n",
      "359: Best loss 0.22204436535636582, saving...\n",
      "370: Best loss 0.21960401336352028, saving...\n",
      "394: Best loss 0.21786514098445572, saving...\n",
      "410: Best loss 0.21692918588717777, saving...\n",
      "411: Best loss 0.21605403597156206, saving...\n",
      "420: Best loss 0.21381616964936256, saving...\n",
      "421: Best loss 0.21257815832893054, saving...\n",
      "430: Best loss 0.2119204471508662, saving...\n",
      "435: Best loss 0.2107616533835729, saving...\n",
      "439: Best loss 0.21040589014689126, saving...\n",
      "445: Best loss 0.21028522724906606, saving...\n",
      "450: Best loss 0.2078848565618197, saving...\n",
      "458: Best loss 0.20611344563464323, saving...\n",
      "488: Best loss 0.2046607866883278, saving...\n",
      "490: Best loss 0.200732076416413, saving...\n",
      "Saving training results to models/bpsk_qpsk_seq_16_3_10_trained_on_2048_tf_ratio_0.25_500_epochs.pt\n",
      "0: Best loss 1.7960159407721625, saving...\n",
      "1: Best loss 1.400021865632799, saving...\n",
      "2: Best loss 1.2056136290232342, saving...\n",
      "3: Best loss 1.1833991103702122, saving...\n",
      "4: Best loss 1.1621954070197211, saving...\n",
      "5: Best loss 1.1418634361690945, saving...\n",
      "6: Best loss 1.13236329820421, saving...\n",
      "7: Best loss 1.123633835050795, saving...\n",
      "8: Best loss 1.1234991815355089, saving...\n",
      "9: Best loss 1.1206339571211072, saving...\n",
      "10: Best loss 1.1133115609486897, saving...\n",
      "13: Best loss 1.099305115805732, saving...\n",
      "14: Best loss 1.0942235946655274, saving...\n",
      "16: Best loss 1.0899354245927597, saving...\n",
      "18: Best loss 1.0845597955915662, saving...\n",
      "21: Best loss 1.0845251666174995, saving...\n",
      "28: Best loss 1.0779357857174343, saving...\n",
      "30: Best loss 1.077033970091078, saving...\n",
      "32: Best loss 1.0765683280097114, saving...\n",
      "33: Best loss 1.0722092787424724, saving...\n",
      "35: Best loss 1.0690473291609024, saving...\n",
      "37: Best loss 1.0646572960747611, saving...\n",
      "40: Best loss 1.056285958819919, saving...\n",
      "50: Best loss 1.052931441201104, saving...\n",
      "53: Best loss 1.0462653160095217, saving...\n",
      "56: Best loss 1.0457234223683676, saving...\n",
      "58: Best loss 1.0374916818406847, saving...\n",
      "60: Best loss 1.0247987800174287, saving...\n",
      "61: Best loss 1.0165141900380452, saving...\n",
      "63: Best loss 1.002705791261461, saving...\n",
      "65: Best loss 0.9938986619313558, saving...\n",
      "67: Best loss 0.9786029444800483, saving...\n",
      "68: Best loss 0.9759146955278184, saving...\n",
      "70: Best loss 0.9636403030819365, saving...\n",
      "72: Best loss 0.9533285935719809, saving...\n",
      "75: Best loss 0.9402228408389621, saving...\n",
      "76: Best loss 0.9385796387990316, saving...\n",
      "77: Best loss 0.9368192672729492, saving...\n",
      "78: Best loss 0.930088292227851, saving...\n",
      "79: Best loss 0.9246009243859186, saving...\n",
      "80: Best loss 0.922732612821791, saving...\n",
      "81: Best loss 0.9185726801554361, saving...\n",
      "82: Best loss 0.915679571363661, saving...\n",
      "83: Best loss 0.9121806250678168, saving...\n",
      "84: Best loss 0.9070562044779459, saving...\n",
      "86: Best loss 0.8979367680019803, saving...\n",
      "87: Best loss 0.8976114432017008, saving...\n",
      "88: Best loss 0.8932503753238253, saving...\n",
      "89: Best loss 0.8894355297088622, saving...\n",
      "90: Best loss 0.8857230345408121, saving...\n",
      "91: Best loss 0.8839933978186714, saving...\n",
      "92: Best loss 0.879000022676256, saving...\n",
      "93: Best loss 0.8765126678678725, saving...\n",
      "95: Best loss 0.8717732270558675, saving...\n",
      "96: Best loss 0.8645451519224379, saving...\n",
      "97: Best loss 0.8613475614123873, saving...\n",
      "100: Best loss 0.8601740413241917, saving...\n",
      "101: Best loss 0.8519638803270129, saving...\n",
      "102: Best loss 0.845476245880127, saving...\n",
      "103: Best loss 0.8428084585401747, saving...\n",
      "104: Best loss 0.84218753973643, saving...\n",
      "105: Best loss 0.8394728660583497, saving...\n",
      "106: Best loss 0.8316007614135741, saving...\n",
      "110: Best loss 0.8203354358673094, saving...\n",
      "115: Best loss 0.8148500018649631, saving...\n",
      "117: Best loss 0.8143857532077367, saving...\n",
      "119: Best loss 0.8080392307705351, saving...\n",
      "122: Best loss 0.8058272096845838, saving...\n",
      "123: Best loss 0.7967463360892402, saving...\n",
      "128: Best loss 0.793367150094774, saving...\n",
      "130: Best loss 0.7876344336403741, saving...\n",
      "132: Best loss 0.7842481401231554, saving...\n",
      "134: Best loss 0.7785317579905193, saving...\n",
      "136: Best loss 0.7730772389305963, saving...\n",
      "140: Best loss 0.7653211938010323, saving...\n",
      "141: Best loss 0.7567725499471027, saving...\n",
      "145: Best loss 0.7505732483334011, saving...\n",
      "147: Best loss 0.7497449000676473, saving...\n",
      "148: Best loss 0.7407968044281007, saving...\n",
      "151: Best loss 0.7396986113654243, saving...\n",
      "152: Best loss 0.7371125459671021, saving...\n",
      "154: Best loss 0.7282707320319282, saving...\n",
      "156: Best loss 0.7233838531706068, saving...\n",
      "158: Best loss 0.7191678418053522, saving...\n",
      "160: Best loss 0.7140534374448988, saving...\n",
      "161: Best loss 0.7140499326917861, saving...\n",
      "162: Best loss 0.7123613463507759, saving...\n",
      "163: Best loss 0.7112680700090197, saving...\n",
      "164: Best loss 0.7104592720667521, saving...\n",
      "165: Best loss 0.7083441893259685, saving...\n",
      "167: Best loss 0.7080695920520359, saving...\n",
      "168: Best loss 0.700924245516459, saving...\n",
      "171: Best loss 0.6950040870242649, saving...\n",
      "174: Best loss 0.6917630725436741, saving...\n",
      "177: Best loss 0.6912585152520074, saving...\n",
      "178: Best loss 0.6894747442669338, saving...\n",
      "179: Best loss 0.6892370091544258, saving...\n",
      "183: Best loss 0.6877586391237046, saving...\n",
      "186: Best loss 0.6835767269134522, saving...\n",
      "192: Best loss 0.675378712018331, saving...\n",
      "198: Best loss 0.6727860742145114, saving...\n",
      "200: Best loss 0.6716809603903029, saving...\n",
      "204: Best loss 0.6658516354031033, saving...\n",
      "206: Best loss 0.6631438374519348, saving...\n",
      "207: Best loss 0.6613824658923679, saving...\n",
      "211: Best loss 0.6610465804735819, saving...\n",
      "216: Best loss 0.6560221009784274, saving...\n",
      "219: Best loss 0.6546708239449395, saving...\n",
      "221: Best loss 0.6526224520471361, saving...\n",
      "223: Best loss 0.6523202551735772, saving...\n",
      "228: Best loss 0.65017655160692, saving...\n",
      "233: Best loss 0.6411749694082473, saving...\n",
      "236: Best loss 0.6410341686672634, saving...\n",
      "238: Best loss 0.6362376477983263, saving...\n",
      "239: Best loss 0.6337954256269667, saving...\n",
      "251: Best loss 0.6292503052287632, saving...\n",
      "252: Best loss 0.6220810823970371, saving...\n",
      "254: Best loss 0.6187398433685303, saving...\n",
      "266: Best loss 0.6180797908041212, saving...\n",
      "274: Best loss 0.6097856190469529, saving...\n",
      "284: Best loss 0.6032074292500813, saving...\n",
      "286: Best loss 0.5968731297387018, saving...\n",
      "304: Best loss 0.593930733203888, saving...\n",
      "306: Best loss 0.5878253738085429, saving...\n",
      "308: Best loss 0.5869521432452732, saving...\n",
      "330: Best loss 0.5837917990154691, saving...\n",
      "336: Best loss 0.5816797097524007, saving...\n",
      "339: Best loss 0.5789128118091159, saving...\n",
      "340: Best loss 0.5777554790178936, saving...\n",
      "346: Best loss 0.5761620203653972, saving...\n",
      "349: Best loss 0.5706534114148881, saving...\n",
      "358: Best loss 0.566002114613851, saving...\n",
      "371: Best loss 0.5637894047631158, saving...\n",
      "377: Best loss 0.5569222609202067, saving...\n",
      "380: Best loss 0.5560162478023105, saving...\n",
      "381: Best loss 0.554791471030977, saving...\n",
      "398: Best loss 0.5510836270120409, saving...\n",
      "402: Best loss 0.5503454387187958, saving...\n",
      "433: Best loss 0.5439275039566889, saving...\n",
      "477: Best loss 0.5389861007531483, saving...\n",
      "479: Best loss 0.535961867041058, saving...\n",
      "Saving training results to models/bpsk_qpsk_seq_16_3_15_trained_on_2048_tf_ratio_0.25_500_epochs.pt\n",
      "0: Best loss 1.8301661968231202, saving...\n",
      "1: Best loss 1.5521538575490315, saving...\n",
      "3: Best loss 1.3959200938542684, saving...\n",
      "4: Best loss 1.3358996868133546, saving...\n",
      "5: Best loss 1.224876399834951, saving...\n",
      "6: Best loss 1.201740491390228, saving...\n",
      "7: Best loss 1.1423144221305848, saving...\n",
      "9: Best loss 1.1175889134407044, saving...\n",
      "10: Best loss 1.1059163053830465, saving...\n",
      "12: Best loss 1.0913877367973328, saving...\n",
      "14: Best loss 1.0876283248265586, saving...\n",
      "16: Best loss 1.0870331327120464, saving...\n",
      "18: Best loss 1.076166240374247, saving...\n",
      "19: Best loss 1.0741440415382386, saving...\n",
      "24: Best loss 1.067900303999583, saving...\n",
      "26: Best loss 1.0667391061782838, saving...\n",
      "29: Best loss 1.0623400926589965, saving...\n",
      "33: Best loss 1.060035264492035, saving...\n",
      "73: Best loss 1.0588553587595622, saving...\n",
      "74: Best loss 1.0505719502766928, saving...\n",
      "75: Best loss 1.0452561378479004, saving...\n",
      "78: Best loss 1.0431147416432698, saving...\n",
      "80: Best loss 1.0332505345344543, saving...\n",
      "84: Best loss 1.0195528149604798, saving...\n",
      "86: Best loss 1.015364098548889, saving...\n",
      "87: Best loss 1.0055110454559326, saving...\n",
      "88: Best loss 1.0010609666506447, saving...\n",
      "91: Best loss 0.9922935128211976, saving...\n",
      "93: Best loss 0.9904057582219442, saving...\n",
      "100: Best loss 0.9871610840161642, saving...\n",
      "101: Best loss 0.9837300101915994, saving...\n",
      "103: Best loss 0.9784475088119506, saving...\n",
      "104: Best loss 0.9772464752197267, saving...\n",
      "108: Best loss 0.9676515579223632, saving...\n",
      "110: Best loss 0.9637214501698812, saving...\n",
      "112: Best loss 0.9630963762601218, saving...\n",
      "114: Best loss 0.9610341231028239, saving...\n",
      "115: Best loss 0.9544868032137552, saving...\n",
      "118: Best loss 0.9533638834953307, saving...\n",
      "119: Best loss 0.947684359550476, saving...\n",
      "122: Best loss 0.9397382219632467, saving...\n",
      "123: Best loss 0.9350641965866088, saving...\n",
      "124: Best loss 0.9338694373766581, saving...\n",
      "127: Best loss 0.927426815032959, saving...\n",
      "128: Best loss 0.9245409965515137, saving...\n",
      "129: Best loss 0.9237380584081014, saving...\n",
      "130: Best loss 0.921375588575999, saving...\n",
      "136: Best loss 0.9120402534802754, saving...\n",
      "140: Best loss 0.905897065003713, saving...\n",
      "143: Best loss 0.9050928711891175, saving...\n",
      "148: Best loss 0.9013320287068685, saving...\n",
      "149: Best loss 0.9009382287661234, saving...\n",
      "150: Best loss 0.8994743585586548, saving...\n",
      "151: Best loss 0.891242527961731, saving...\n",
      "152: Best loss 0.8896861632664999, saving...\n",
      "154: Best loss 0.8868344624837239, saving...\n",
      "160: Best loss 0.8795596877733867, saving...\n",
      "161: Best loss 0.8783485054969788, saving...\n",
      "165: Best loss 0.8710885365804035, saving...\n",
      "168: Best loss 0.866551689306895, saving...\n",
      "171: Best loss 0.8502242803573609, saving...\n",
      "176: Best loss 0.8488767107327779, saving...\n",
      "178: Best loss 0.8443085193634035, saving...\n",
      "192: Best loss 0.8421661575635274, saving...\n",
      "193: Best loss 0.8410707394282024, saving...\n",
      "194: Best loss 0.8357389450073243, saving...\n",
      "195: Best loss 0.8314097086588541, saving...\n",
      "201: Best loss 0.8296313126881918, saving...\n",
      "208: Best loss 0.82755819161733, saving...\n",
      "209: Best loss 0.8234387795130411, saving...\n",
      "213: Best loss 0.8206331968307496, saving...\n",
      "216: Best loss 0.8197405258814494, saving...\n",
      "224: Best loss 0.8101519703865051, saving...\n",
      "225: Best loss 0.800702758630117, saving...\n",
      "249: Best loss 0.7976582884788515, saving...\n",
      "252: Best loss 0.7948006629943849, saving...\n",
      "270: Best loss 0.7901177883148193, saving...\n",
      "271: Best loss 0.7826483925183614, saving...\n",
      "284: Best loss 0.7722232500712078, saving...\n",
      "291: Best loss 0.7680223981539409, saving...\n",
      "295: Best loss 0.7608193318049113, saving...\n",
      "332: Best loss 0.7532735387484233, saving...\n",
      "346: Best loss 0.7397714694341024, saving...\n",
      "430: Best loss 0.7367912173271178, saving...\n",
      "433: Best loss 0.7287119706471761, saving...\n",
      "465: Best loss 0.7249690095583599, saving...\n",
      "468: Best loss 0.7201419234275818, saving...\n",
      "486: Best loss 0.719936712582906, saving...\n",
      "490: Best loss 0.7194778601328532, saving...\n",
      "Saving training results to models/bpsk_qpsk_seq_16_3_20_trained_on_2048_tf_ratio_0.25_500_epochs.pt\n",
      "0: Best loss 1.8516412734985352, saving...\n",
      "1: Best loss 1.692070412635803, saving...\n",
      "4: Best loss 1.400666658083598, saving...\n",
      "5: Best loss 1.3333502292633055, saving...\n",
      "6: Best loss 1.3171746452649433, saving...\n",
      "7: Best loss 1.279660709698995, saving...\n",
      "8: Best loss 1.2634165247281393, saving...\n",
      "14: Best loss 1.2523842374483745, saving...\n",
      "15: Best loss 1.231536622842153, saving...\n",
      "16: Best loss 1.221343966325124, saving...\n",
      "17: Best loss 1.2017821669578552, saving...\n",
      "18: Best loss 1.1885275443394978, saving...\n",
      "20: Best loss 1.1810359120368958, saving...\n",
      "25: Best loss 1.159810439745585, saving...\n",
      "26: Best loss 1.1492337505022685, saving...\n",
      "29: Best loss 1.1462461551030476, saving...\n",
      "30: Best loss 1.131387178103129, saving...\n",
      "33: Best loss 1.1259377042452494, saving...\n",
      "34: Best loss 1.086275760332743, saving...\n",
      "37: Best loss 1.0749307910601298, saving...\n",
      "38: Best loss 1.060539186000824, saving...\n",
      "39: Best loss 1.0532823681831358, saving...\n",
      "40: Best loss 1.0285119414329529, saving...\n",
      "42: Best loss 1.0230493545532227, saving...\n",
      "43: Best loss 1.0104586044947306, saving...\n",
      "44: Best loss 0.9761452674865723, saving...\n",
      "45: Best loss 0.97197927236557, saving...\n",
      "49: Best loss 0.9648764053980508, saving...\n",
      "50: Best loss 0.9245158433914185, saving...\n",
      "52: Best loss 0.8996001720428466, saving...\n",
      "53: Best loss 0.8881524523099265, saving...\n",
      "55: Best loss 0.8764692803223927, saving...\n",
      "56: Best loss 0.8650032023588816, saving...\n",
      "59: Best loss 0.8465729633967082, saving...\n",
      "63: Best loss 0.7881000141302745, saving...\n",
      "65: Best loss 0.7642985959847769, saving...\n",
      "68: Best loss 0.7421970129013061, saving...\n",
      "69: Best loss 0.7398260891437529, saving...\n",
      "70: Best loss 0.7367079704999924, saving...\n",
      "71: Best loss 0.7112484673659006, saving...\n",
      "73: Best loss 0.7047444105148314, saving...\n",
      "74: Best loss 0.7034406294425329, saving...\n",
      "75: Best loss 0.693429845571518, saving...\n",
      "76: Best loss 0.6885703384876252, saving...\n",
      "77: Best loss 0.6737083454926809, saving...\n",
      "79: Best loss 0.6594280759493509, saving...\n",
      "83: Best loss 0.6550151666005453, saving...\n",
      "84: Best loss 0.6460661570231119, saving...\n",
      "85: Best loss 0.6397866268952688, saving...\n",
      "86: Best loss 0.6378989934921264, saving...\n",
      "88: Best loss 0.629646940032641, saving...\n",
      "89: Best loss 0.6276251877347628, saving...\n",
      "92: Best loss 0.6235114643971126, saving...\n",
      "93: Best loss 0.6207160885135332, saving...\n",
      "96: Best loss 0.6138049369057019, saving...\n",
      "98: Best loss 0.6103309998909632, saving...\n",
      "99: Best loss 0.5978908777236939, saving...\n",
      "102: Best loss 0.5960760871569315, saving...\n",
      "109: Best loss 0.5892981747786203, saving...\n",
      "110: Best loss 0.5869897678494453, saving...\n",
      "114: Best loss 0.5768706649541855, saving...\n",
      "118: Best loss 0.5744434341788291, saving...\n",
      "121: Best loss 0.5688572098811467, saving...\n",
      "128: Best loss 0.564894063770771, saving...\n",
      "130: Best loss 0.5530802716811497, saving...\n",
      "135: Best loss 0.5443784435590108, saving...\n",
      "138: Best loss 0.5441342979669571, saving...\n",
      "145: Best loss 0.5402092377344767, saving...\n",
      "146: Best loss 0.5233771850665411, saving...\n",
      "147: Best loss 0.5197556346654892, saving...\n",
      "149: Best loss 0.5113120242953301, saving...\n",
      "151: Best loss 0.5059748828411103, saving...\n",
      "152: Best loss 0.503087689479192, saving...\n",
      "153: Best loss 0.5000182732939721, saving...\n",
      "155: Best loss 0.49959465066591896, saving...\n",
      "158: Best loss 0.4924918889999389, saving...\n",
      "160: Best loss 0.48247121348977096, saving...\n",
      "162: Best loss 0.4651966691017151, saving...\n",
      "163: Best loss 0.4544391366342704, saving...\n",
      "170: Best loss 0.4275416833659013, saving...\n",
      "171: Best loss 0.4266549326479435, saving...\n",
      "174: Best loss 0.41647940104206405, saving...\n",
      "177: Best loss 0.41423261861006416, saving...\n",
      "179: Best loss 0.3991376407444477, saving...\n",
      "183: Best loss 0.3954352493087451, saving...\n",
      "184: Best loss 0.38501834819714226, saving...\n",
      "188: Best loss 0.38177891920010243, saving...\n",
      "194: Best loss 0.3703364983201027, saving...\n",
      "200: Best loss 0.3611417546868324, saving...\n",
      "203: Best loss 0.3602348948518435, saving...\n",
      "204: Best loss 0.3596494476000468, saving...\n",
      "206: Best loss 0.3461144129435221, saving...\n",
      "207: Best loss 0.3458168824513753, saving...\n",
      "209: Best loss 0.33901977414886153, saving...\n",
      "213: Best loss 0.3374807452162107, saving...\n",
      "216: Best loss 0.33657337104280793, saving...\n",
      "218: Best loss 0.33193741713960967, saving...\n",
      "223: Best loss 0.32784171476960183, saving...\n",
      "226: Best loss 0.3226418524980545, saving...\n",
      "227: Best loss 0.31750435158610346, saving...\n",
      "232: Best loss 0.3153911918401718, saving...\n",
      "233: Best loss 0.31380729923645656, saving...\n",
      "236: Best loss 0.31326595991849904, saving...\n",
      "238: Best loss 0.3105602378646533, saving...\n",
      "244: Best loss 0.31023063883185387, saving...\n",
      "249: Best loss 0.3029940436283748, saving...\n",
      "250: Best loss 0.2995748976866404, saving...\n",
      "259: Best loss 0.2977667192618052, saving...\n",
      "260: Best loss 0.2933630670110385, saving...\n",
      "264: Best loss 0.28943607658147813, saving...\n",
      "272: Best loss 0.2882115711768468, saving...\n",
      "275: Best loss 0.28515340586503346, saving...\n",
      "280: Best loss 0.284993609537681, saving...\n",
      "284: Best loss 0.28183560619751613, saving...\n",
      "288: Best loss 0.2794311314821243, saving...\n",
      "294: Best loss 0.27892906715472543, saving...\n",
      "295: Best loss 0.27747742980718615, saving...\n",
      "305: Best loss 0.2774526851872603, saving...\n",
      "308: Best loss 0.2731779426336288, saving...\n",
      "310: Best loss 0.2728440123299758, saving...\n",
      "311: Best loss 0.2705391861498356, saving...\n",
      "323: Best loss 0.2700915609796842, saving...\n",
      "324: Best loss 0.2686844592293104, saving...\n",
      "325: Best loss 0.26729084104299544, saving...\n",
      "330: Best loss 0.2663468509912491, saving...\n",
      "332: Best loss 0.26386800842980546, saving...\n",
      "341: Best loss 0.26317654450734457, saving...\n",
      "347: Best loss 0.2602199065188567, saving...\n",
      "351: Best loss 0.2586923422912757, saving...\n",
      "356: Best loss 0.2524565349022547, saving...\n",
      "374: Best loss 0.2503322280943393, saving...\n",
      "394: Best loss 0.24980259835720064, saving...\n",
      "395: Best loss 0.24631975789864857, saving...\n",
      "411: Best loss 0.2459953452150027, saving...\n",
      "415: Best loss 0.24312179411451024, saving...\n",
      "421: Best loss 0.2402779519557953, saving...\n",
      "431: Best loss 0.23680632958809533, saving...\n",
      "458: Best loss 0.23268386150399845, saving...\n",
      "461: Best loss 0.22971748237808548, saving...\n",
      "476: Best loss 0.22970153292020162, saving...\n",
      "498: Best loss 0.22737236494819324, saving...\n",
      "Saving training results to models/bpsk_qpsk_seq_16_3_10_trained_on_2048_tf_ratio_0.5_500_epochs.pt\n",
      "0: Best loss 1.879278204176161, saving...\n",
      "3: Best loss 1.8055474069383408, saving...\n",
      "5: Best loss 1.5176320870717368, saving...\n",
      "6: Best loss 1.504504919052124, saving...\n",
      "9: Best loss 1.4559315734439426, saving...\n",
      "10: Best loss 1.3531499438815646, saving...\n",
      "11: Best loss 1.3144818888770209, saving...\n",
      "13: Best loss 1.2858236895667183, saving...\n",
      "16: Best loss 1.2046744187672935, saving...\n",
      "17: Best loss 1.1955791367424857, saving...\n",
      "20: Best loss 1.1805360529157851, saving...\n",
      "21: Best loss 1.1660288598802353, saving...\n",
      "29: Best loss 1.1455370479159885, saving...\n",
      "30: Best loss 1.1444212118784587, saving...\n",
      "32: Best loss 1.1274455865224202, saving...\n",
      "33: Best loss 1.1191007137298585, saving...\n",
      "39: Best loss 1.118852228588528, saving...\n",
      "42: Best loss 1.1114566008249918, saving...\n",
      "46: Best loss 1.1038388570149742, saving...\n",
      "47: Best loss 1.0960703478919136, saving...\n",
      "50: Best loss 1.091373242272271, saving...\n",
      "55: Best loss 1.081604962878757, saving...\n",
      "58: Best loss 1.071154170566135, saving...\n",
      "60: Best loss 1.0615482012430828, saving...\n",
      "62: Best loss 1.058925273683336, saving...\n",
      "63: Best loss 1.0515846782260472, saving...\n",
      "66: Best loss 1.0458577632904054, saving...\n",
      "69: Best loss 1.0389025688171387, saving...\n",
      "71: Best loss 1.0227208932240803, saving...\n",
      "73: Best loss 1.0194244596693252, saving...\n",
      "74: Best loss 1.0174036608801948, saving...\n",
      "78: Best loss 1.01516981654697, saving...\n",
      "79: Best loss 1.0137493133544924, saving...\n",
      "80: Best loss 1.0080043739742703, saving...\n",
      "81: Best loss 1.0062115245395236, saving...\n",
      "83: Best loss 0.994349177678426, saving...\n",
      "84: Best loss 0.9900724093119304, saving...\n",
      "91: Best loss 0.9838887638515895, saving...\n",
      "92: Best loss 0.9812154134114585, saving...\n",
      "93: Best loss 0.9776151021321615, saving...\n",
      "95: Best loss 0.9697364118364122, saving...\n",
      "98: Best loss 0.9609234862857394, saving...\n",
      "100: Best loss 0.9526724921332465, saving...\n",
      "102: Best loss 0.9330321894751654, saving...\n",
      "104: Best loss 0.9307279162936741, saving...\n",
      "105: Best loss 0.9280769930945503, saving...\n",
      "111: Best loss 0.9237046877543131, saving...\n",
      "113: Best loss 0.9177930355072021, saving...\n",
      "114: Best loss 0.91484587987264, saving...\n",
      "116: Best loss 0.9074840598636205, saving...\n",
      "120: Best loss 0.8966889010535346, saving...\n",
      "130: Best loss 0.895694022708469, saving...\n",
      "131: Best loss 0.8833121352725559, saving...\n",
      "133: Best loss 0.8825453334384494, saving...\n",
      "134: Best loss 0.8810963047875299, saving...\n",
      "135: Best loss 0.8777237521277534, saving...\n",
      "137: Best loss 0.8719537523057725, saving...\n",
      "138: Best loss 0.8605082246992324, saving...\n",
      "140: Best loss 0.8553286446465387, saving...\n",
      "146: Best loss 0.846662696202596, saving...\n",
      "147: Best loss 0.8463305526309544, saving...\n",
      "148: Best loss 0.8445674101511635, saving...\n",
      "149: Best loss 0.8317287709977892, saving...\n",
      "153: Best loss 0.8306568808025783, saving...\n",
      "154: Best loss 0.8300515757666693, saving...\n",
      "157: Best loss 0.8246819443172879, saving...\n",
      "159: Best loss 0.8179968966378106, saving...\n",
      "168: Best loss 0.8036535077624851, saving...\n",
      "178: Best loss 0.7805588245391846, saving...\n",
      "187: Best loss 0.7709327591790093, saving...\n",
      "188: Best loss 0.7652416043811374, saving...\n",
      "189: Best loss 0.7585502015219794, saving...\n",
      "192: Best loss 0.7554153760274251, saving...\n",
      "198: Best loss 0.748485525449117, saving...\n",
      "200: Best loss 0.741499654452006, saving...\n",
      "201: Best loss 0.731183550092909, saving...\n",
      "207: Best loss 0.7254506958855522, saving...\n",
      "209: Best loss 0.7183218903011745, saving...\n",
      "211: Best loss 0.7175483968522813, saving...\n",
      "213: Best loss 0.71035668320126, saving...\n",
      "214: Best loss 0.7043887986077202, saving...\n",
      "217: Best loss 0.7017058107588027, saving...\n",
      "219: Best loss 0.6991648329628839, saving...\n",
      "220: Best loss 0.6989653375413682, saving...\n",
      "221: Best loss 0.690882232454088, saving...\n",
      "222: Best loss 0.682117305861579, saving...\n",
      "223: Best loss 0.6786538203557333, saving...\n",
      "230: Best loss 0.6675979985131157, saving...\n",
      "233: Best loss 0.6646987676620483, saving...\n",
      "234: Best loss 0.6569301711188422, saving...\n",
      "238: Best loss 0.655830028322008, saving...\n",
      "243: Best loss 0.6468547277980381, saving...\n",
      "245: Best loss 0.646038802464803, saving...\n",
      "247: Best loss 0.6428829140133328, saving...\n",
      "251: Best loss 0.6424684868918524, saving...\n",
      "252: Best loss 0.6308252255121867, saving...\n",
      "262: Best loss 0.623299667570326, saving...\n",
      "275: Best loss 0.6122715592384339, saving...\n",
      "284: Best loss 0.6073922753334045, saving...\n",
      "285: Best loss 0.6061699893739488, saving...\n",
      "298: Best loss 0.6039854294723935, saving...\n",
      "299: Best loss 0.6036389006508722, saving...\n",
      "304: Best loss 0.6013563328319126, saving...\n",
      "312: Best loss 0.5876561813884311, saving...\n",
      "318: Best loss 0.5749497373898824, saving...\n",
      "341: Best loss 0.5733556376563179, saving...\n",
      "429: Best loss 0.5586782866054112, saving...\n",
      "481: Best loss 0.5557005809413061, saving...\n",
      "485: Best loss 0.5460580603943932, saving...\n",
      "489: Best loss 0.542398551106453, saving...\n",
      "Saving training results to models/bpsk_qpsk_seq_16_3_15_trained_on_2048_tf_ratio_0.5_500_epochs.pt\n",
      "0: Best loss 2.0231470187505085, saving...\n",
      "1: Best loss 1.9418012857437132, saving...\n",
      "5: Best loss 1.8179783741633098, saving...\n",
      "9: Best loss 1.6211831967035928, saving...\n",
      "11: Best loss 1.6016488552093506, saving...\n",
      "14: Best loss 1.5613707860310873, saving...\n",
      "16: Best loss 1.510868843396505, saving...\n",
      "17: Best loss 1.278736444314321, saving...\n",
      "18: Best loss 1.1980713923772177, saving...\n",
      "21: Best loss 1.1643900791803996, saving...\n",
      "22: Best loss 1.1631064613660176, saving...\n",
      "24: Best loss 1.1178446213404336, saving...\n",
      "25: Best loss 1.1002597649892172, saving...\n",
      "27: Best loss 1.077983264128367, saving...\n",
      "29: Best loss 1.0634957750638327, saving...\n",
      "44: Best loss 1.060054071744283, saving...\n",
      "49: Best loss 1.0599308570226036, saving...\n",
      "59: Best loss 1.059402124087016, saving...\n",
      "62: Best loss 1.0575763424237568, saving...\n",
      "63: Best loss 1.0570595701535541, saving...\n",
      "87: Best loss 1.0554120540618896, saving...\n",
      "107: Best loss 1.0489072998364768, saving...\n",
      "108: Best loss 1.045083530743917, saving...\n",
      "109: Best loss 1.044111959139506, saving...\n",
      "110: Best loss 1.037093949317932, saving...\n",
      "112: Best loss 1.0218268712361653, saving...\n",
      "115: Best loss 1.0104007005691529, saving...\n",
      "132: Best loss 1.005738886197408, saving...\n",
      "135: Best loss 1.000277543067932, saving...\n",
      "138: Best loss 0.9975610534350079, saving...\n",
      "141: Best loss 0.9904505292574565, saving...\n",
      "142: Best loss 0.986692484219869, saving...\n",
      "143: Best loss 0.9838755607604981, saving...\n",
      "147: Best loss 0.9833599607149759, saving...\n",
      "149: Best loss 0.9737136205037435, saving...\n",
      "155: Best loss 0.9713827013969422, saving...\n",
      "160: Best loss 0.9709948142369589, saving...\n",
      "162: Best loss 0.962877647082011, saving...\n",
      "164: Best loss 0.9586969256401061, saving...\n",
      "179: Best loss 0.9526720166206359, saving...\n",
      "182: Best loss 0.9451834837595621, saving...\n",
      "184: Best loss 0.9430229425430298, saving...\n",
      "189: Best loss 0.9420380314191183, saving...\n",
      "191: Best loss 0.936147399743398, saving...\n",
      "193: Best loss 0.9324496308962503, saving...\n",
      "194: Best loss 0.9266456683476767, saving...\n",
      "197: Best loss 0.9200286904970805, saving...\n",
      "205: Best loss 0.9161938269933065, saving...\n",
      "214: Best loss 0.9143378814061482, saving...\n",
      "217: Best loss 0.9137508908907572, saving...\n",
      "218: Best loss 0.9120966990788777, saving...\n",
      "226: Best loss 0.907755168279012, saving...\n",
      "232: Best loss 0.9071516633033754, saving...\n",
      "234: Best loss 0.9060187856356304, saving...\n",
      "235: Best loss 0.9012021978696187, saving...\n",
      "236: Best loss 0.893863602479299, saving...\n",
      "242: Best loss 0.8818879842758178, saving...\n",
      "254: Best loss 0.8812248786290486, saving...\n",
      "259: Best loss 0.8760610063870747, saving...\n",
      "263: Best loss 0.8753559788068136, saving...\n",
      "268: Best loss 0.8679114977518718, saving...\n",
      "288: Best loss 0.8581468065579733, saving...\n",
      "308: Best loss 0.8427072207132976, saving...\n",
      "312: Best loss 0.8410272518793741, saving...\n",
      "318: Best loss 0.8390748699506123, saving...\n",
      "321: Best loss 0.8366064349810283, saving...\n",
      "324: Best loss 0.8344974120457967, saving...\n",
      "325: Best loss 0.8329073588053386, saving...\n",
      "326: Best loss 0.8305702487627664, saving...\n",
      "353: Best loss 0.8200158119201659, saving...\n",
      "355: Best loss 0.8179603934288024, saving...\n",
      "381: Best loss 0.8034623503684998, saving...\n",
      "421: Best loss 0.7941427985827129, saving...\n",
      "437: Best loss 0.7851421256860096, saving...\n",
      "440: Best loss 0.7709880451361337, saving...\n",
      "447: Best loss 0.770066632827123, saving...\n",
      "453: Best loss 0.7644064664840698, saving...\n",
      "456: Best loss 0.7587212860584259, saving...\n",
      "480: Best loss 0.75568345785141, saving...\n",
      "Saving training results to models/bpsk_qpsk_seq_16_3_20_trained_on_2048_tf_ratio_0.5_500_epochs.pt\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = [10,15,20]\n",
    "model_configs = [(16,3), (128,3)]\n",
    "train_config['num_epochs'] = 250\n",
    "tf_ratios = [0.25,0.5]\n",
    "for tf_ratio in tf_ratios:\n",
    "    for seq_length in seq_lengths:\n",
    "        for model_config in model_configs:\n",
    "            data_config['seq_length'] = seq_length\n",
    "\n",
    "            train_network(data_config, train_config, model_config=(16,3), gpu=True, dropout=0, tf_rate=tf_ratio, numpy_seed=0, torch_seed=0, \n",
    "                        savepath=f\"models/bpsk_qpsk_seq_{model_config[0]}_{model_config[1]}_{data_config['seq_length']}_trained_on_2048_tf_ratio_{tf_ratio}_{train_config['num_epochs']}_epochs.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Best loss 1.735629367828369, saving...\n",
      "1: Best loss 1.2296502669652303, saving...\n",
      "2: Best loss 1.1853167136510212, saving...\n",
      "3: Best loss 1.1622465531031292, saving...\n",
      "5: Best loss 1.1585209210713705, saving...\n",
      "6: Best loss 1.1387441118558248, saving...\n",
      "7: Best loss 1.1354557633399962, saving...\n",
      "8: Best loss 1.128521168231964, saving...\n",
      "9: Best loss 1.1241900483767193, saving...\n",
      "10: Best loss 1.1153491814931233, saving...\n",
      "11: Best loss 1.101545838514964, saving...\n",
      "12: Best loss 1.1014265020688374, saving...\n",
      "14: Best loss 1.0974552830060325, saving...\n",
      "15: Best loss 1.090687918663025, saving...\n",
      "17: Best loss 1.0836688677469888, saving...\n",
      "18: Best loss 1.0817009250322978, saving...\n",
      "20: Best loss 1.0755764722824095, saving...\n",
      "21: Best loss 1.068686298529307, saving...\n",
      "24: Best loss 1.0559995849927266, saving...\n",
      "26: Best loss 1.0458980878194173, saving...\n",
      "27: Best loss 1.0351509491602582, saving...\n",
      "28: Best loss 1.0284817417462666, saving...\n",
      "29: Best loss 1.025569498538971, saving...\n",
      "30: Best loss 1.023124663035075, saving...\n",
      "31: Best loss 1.0189048528671265, saving...\n",
      "32: Best loss 1.0059997677803039, saving...\n",
      "33: Best loss 1.0034250378608705, saving...\n",
      "34: Best loss 0.9910273114840189, saving...\n",
      "37: Best loss 0.9868737459182739, saving...\n",
      "38: Best loss 0.9821993629137675, saving...\n",
      "40: Best loss 0.9758551239967346, saving...\n",
      "41: Best loss 0.9684364318847658, saving...\n",
      "44: Best loss 0.9588586409886678, saving...\n",
      "45: Best loss 0.9497130711873372, saving...\n",
      "47: Best loss 0.9487513502438862, saving...\n",
      "48: Best loss 0.9444610039393109, saving...\n",
      "49: Best loss 0.9349369963010153, saving...\n",
      "55: Best loss 0.9330858906110129, saving...\n",
      "56: Best loss 0.9317215760548909, saving...\n",
      "57: Best loss 0.9252071420351664, saving...\n",
      "58: Best loss 0.9153120001157125, saving...\n",
      "59: Best loss 0.9078823328018188, saving...\n",
      "60: Best loss 0.896975859006246, saving...\n",
      "61: Best loss 0.8846532384554544, saving...\n",
      "62: Best loss 0.8840355634689332, saving...\n",
      "63: Best loss 0.876763101418813, saving...\n",
      "64: Best loss 0.86893496910731, saving...\n",
      "65: Best loss 0.8576457142829895, saving...\n",
      "67: Best loss 0.8567286372184753, saving...\n",
      "68: Best loss 0.8502807418505351, saving...\n",
      "69: Best loss 0.8456037918726602, saving...\n",
      "70: Best loss 0.8439632574717205, saving...\n",
      "71: Best loss 0.8313651084899902, saving...\n",
      "72: Best loss 0.8228380560874939, saving...\n",
      "74: Best loss 0.8144059379895527, saving...\n",
      "75: Best loss 0.797523840268453, saving...\n",
      "79: Best loss 0.7871834913889567, saving...\n",
      "82: Best loss 0.785840598742167, saving...\n",
      "83: Best loss 0.7789302190144856, saving...\n",
      "84: Best loss 0.7742636839548747, saving...\n",
      "86: Best loss 0.7693550745646158, saving...\n",
      "87: Best loss 0.7671727895736694, saving...\n",
      "88: Best loss 0.7623711864153544, saving...\n",
      "89: Best loss 0.7540796001752217, saving...\n",
      "92: Best loss 0.7474119861920674, saving...\n",
      "94: Best loss 0.7396957159042358, saving...\n",
      "96: Best loss 0.7329055587450664, saving...\n",
      "97: Best loss 0.7281274318695068, saving...\n",
      "98: Best loss 0.7199109037717182, saving...\n",
      "100: Best loss 0.7168241441249847, saving...\n",
      "101: Best loss 0.6992499689261118, saving...\n",
      "104: Best loss 0.6973547637462616, saving...\n",
      "105: Best loss 0.6951849063237509, saving...\n",
      "106: Best loss 0.6939770460128785, saving...\n",
      "108: Best loss 0.6865052282810211, saving...\n",
      "110: Best loss 0.6860088288784028, saving...\n",
      "111: Best loss 0.6849240322907766, saving...\n",
      "112: Best loss 0.6594350198904673, saving...\n",
      "117: Best loss 0.6526621282100677, saving...\n",
      "118: Best loss 0.6379644989967346, saving...\n",
      "123: Best loss 0.6264700452486673, saving...\n",
      "125: Best loss 0.6259917060534159, saving...\n",
      "128: Best loss 0.619630362590154, saving...\n",
      "130: Best loss 0.617642343044281, saving...\n",
      "131: Best loss 0.6175697217384974, saving...\n",
      "132: Best loss 0.6104577541351319, saving...\n",
      "133: Best loss 0.6104228496551514, saving...\n",
      "134: Best loss 0.6089600920677185, saving...\n",
      "136: Best loss 0.5985512276490529, saving...\n",
      "138: Best loss 0.5932188232739766, saving...\n",
      "142: Best loss 0.590048732360204, saving...\n",
      "151: Best loss 0.5836339414119721, saving...\n",
      "153: Best loss 0.5817830681800842, saving...\n",
      "157: Best loss 0.5811497688293457, saving...\n",
      "158: Best loss 0.5675940454006195, saving...\n",
      "165: Best loss 0.5652796010176342, saving...\n",
      "166: Best loss 0.5628278305133183, saving...\n",
      "170: Best loss 0.5585215081771214, saving...\n",
      "173: Best loss 0.5521200557549795, saving...\n",
      "178: Best loss 0.5486489415168763, saving...\n",
      "179: Best loss 0.5452122499545415, saving...\n",
      "191: Best loss 0.5383057792981466, saving...\n",
      "193: Best loss 0.5380542606115342, saving...\n",
      "199: Best loss 0.5356525957584382, saving...\n",
      "206: Best loss 0.5344417596856753, saving...\n",
      "209: Best loss 0.5338864311575889, saving...\n",
      "211: Best loss 0.5335268492499987, saving...\n",
      "212: Best loss 0.5327914029359817, saving...\n",
      "215: Best loss 0.5269065551459788, saving...\n",
      "222: Best loss 0.5257650484641393, saving...\n",
      "229: Best loss 0.5209395801027616, saving...\n",
      "246: Best loss 0.5188595528403919, saving...\n",
      "249: Best loss 0.5114653949936232, saving...\n",
      "261: Best loss 0.5108125199874243, saving...\n",
      "296: Best loss 0.510406253238519, saving...\n",
      "302: Best loss 0.5067442874113718, saving...\n",
      "303: Best loss 0.5028228397170703, saving...\n",
      "311: Best loss 0.5018136481444041, saving...\n",
      "325: Best loss 0.49607309599717453, saving...\n",
      "331: Best loss 0.4872232670585315, saving...\n",
      "344: Best loss 0.48128949825962375, saving...\n",
      "353: Best loss 0.4786893794933955, saving...\n",
      "355: Best loss 0.475180650750796, saving...\n",
      "361: Best loss 0.4721818849444389, saving...\n",
      "362: Best loss 0.4681927161912124, saving...\n",
      "365: Best loss 0.46688020229339605, saving...\n",
      "376: Best loss 0.46391011228164036, saving...\n",
      "383: Best loss 0.46076042254765825, saving...\n",
      "385: Best loss 0.4570052067438761, saving...\n",
      "388: Best loss 0.45514995505412414, saving...\n",
      "392: Best loss 0.45437485029300057, saving...\n",
      "394: Best loss 0.45258614122867585, saving...\n",
      "403: Best loss 0.45008901904026666, saving...\n",
      "407: Best loss 0.4470823844273885, saving...\n",
      "410: Best loss 0.443853871524334, saving...\n",
      "413: Best loss 0.4421812862157821, saving...\n",
      "428: Best loss 0.4383788287639618, saving...\n",
      "434: Best loss 0.4356472487250963, saving...\n",
      "442: Best loss 0.4350823027392228, saving...\n",
      "443: Best loss 0.43330302039782204, saving...\n",
      "450: Best loss 0.42498334000508, saving...\n",
      "469: Best loss 0.41963310688734057, saving...\n",
      "495: Best loss 0.41859764556090034, saving...\n",
      "Saving training results to models/bpsk_qpsk_seq_16_3_10_trained_on_2048_dropout_0.25_500_epochs.pt\n",
      "0: Best loss 1.7386300722757975, saving...\n",
      "1: Best loss 1.2313142193688287, saving...\n",
      "2: Best loss 1.164874219894409, saving...\n",
      "3: Best loss 1.1576812267303467, saving...\n",
      "5: Best loss 1.1289666493733723, saving...\n",
      "7: Best loss 1.114821031358507, saving...\n",
      "9: Best loss 1.1032310432857937, saving...\n",
      "11: Best loss 1.0988151338365344, saving...\n",
      "12: Best loss 1.0918384075164795, saving...\n",
      "14: Best loss 1.0891019768185086, saving...\n",
      "17: Best loss 1.0852231820424398, saving...\n",
      "18: Best loss 1.0820530626508924, saving...\n",
      "22: Best loss 1.0819278293185763, saving...\n",
      "23: Best loss 1.0806799199846058, saving...\n",
      "24: Best loss 1.077894475724962, saving...\n",
      "27: Best loss 1.0755951563517252, saving...\n",
      "30: Best loss 1.0732494672139485, saving...\n",
      "32: Best loss 1.0691418488820394, saving...\n",
      "35: Best loss 1.0681284639570445, saving...\n",
      "38: Best loss 1.0640252696143255, saving...\n",
      "40: Best loss 1.0593800491756862, saving...\n",
      "41: Best loss 1.0583196798960368, saving...\n",
      "43: Best loss 1.0576168325212267, saving...\n",
      "44: Best loss 1.0514209959242078, saving...\n",
      "46: Best loss 1.0498984919653997, saving...\n",
      "47: Best loss 1.0489394717746312, saving...\n",
      "48: Best loss 1.0452253500620525, saving...\n",
      "49: Best loss 1.044797643025716, saving...\n",
      "51: Best loss 1.0435040685865613, saving...\n",
      "53: Best loss 1.0376976437038845, saving...\n",
      "56: Best loss 1.0371873325771754, saving...\n",
      "57: Best loss 1.0335805733998618, saving...\n",
      "58: Best loss 1.032282246483697, saving...\n",
      "60: Best loss 1.0260572751363117, saving...\n",
      "61: Best loss 1.0218316343095568, saving...\n",
      "62: Best loss 1.015691100226508, saving...\n",
      "64: Best loss 1.0154405487908258, saving...\n",
      "66: Best loss 1.0125766436258952, saving...\n",
      "68: Best loss 1.0066708352830676, saving...\n",
      "69: Best loss 1.0035435252719456, saving...\n",
      "70: Best loss 0.9988229168785943, saving...\n",
      "72: Best loss 0.9963820457458497, saving...\n",
      "73: Best loss 0.9956806500752766, saving...\n",
      "74: Best loss 0.9938096947140163, saving...\n",
      "75: Best loss 0.9908707035912407, saving...\n",
      "78: Best loss 0.9826102574666341, saving...\n",
      "80: Best loss 0.9811065091027154, saving...\n",
      "81: Best loss 0.9770180278354221, saving...\n",
      "84: Best loss 0.9760024600558812, saving...\n",
      "86: Best loss 0.9678550137413873, saving...\n",
      "87: Best loss 0.966406806310018, saving...\n",
      "90: Best loss 0.9557797008090549, saving...\n",
      "94: Best loss 0.9521221531762016, saving...\n",
      "95: Best loss 0.9461977005004882, saving...\n",
      "96: Best loss 0.9413364834255643, saving...\n",
      "97: Best loss 0.9344157059987387, saving...\n",
      "98: Best loss 0.9309090243445501, saving...\n",
      "100: Best loss 0.9274734179178874, saving...\n",
      "101: Best loss 0.9232146475050185, saving...\n",
      "102: Best loss 0.9197965568966338, saving...\n",
      "103: Best loss 0.9154465516408283, saving...\n",
      "104: Best loss 0.914463636610243, saving...\n",
      "105: Best loss 0.9077944384680854, saving...\n",
      "106: Best loss 0.9062214215596516, saving...\n",
      "109: Best loss 0.9016894128587511, saving...\n",
      "110: Best loss 0.9014308293660482, saving...\n",
      "111: Best loss 0.8964367177751329, saving...\n",
      "113: Best loss 0.8890713850657147, saving...\n",
      "115: Best loss 0.881978448232015, saving...\n",
      "117: Best loss 0.8785029040442573, saving...\n",
      "121: Best loss 0.8748739189571806, saving...\n",
      "122: Best loss 0.8656242953406439, saving...\n",
      "123: Best loss 0.8617386553022597, saving...\n",
      "128: Best loss 0.8560530026753744, saving...\n",
      "129: Best loss 0.8502011775970459, saving...\n",
      "132: Best loss 0.845928144454956, saving...\n",
      "135: Best loss 0.844878149032593, saving...\n",
      "137: Best loss 0.8264228608873156, saving...\n",
      "141: Best loss 0.8155940797593859, saving...\n",
      "144: Best loss 0.8070497459835476, saving...\n",
      "145: Best loss 0.797698328230116, saving...\n",
      "148: Best loss 0.7954185114966499, saving...\n",
      "149: Best loss 0.7849580420388116, saving...\n",
      "154: Best loss 0.7841031577852036, saving...\n",
      "155: Best loss 0.7828857421875, saving...\n",
      "156: Best loss 0.7813883993360732, saving...\n",
      "157: Best loss 0.7754922654893663, saving...\n",
      "158: Best loss 0.772679058710734, saving...\n",
      "161: Best loss 0.7693784077962238, saving...\n",
      "162: Best loss 0.7682290236155193, saving...\n",
      "163: Best loss 0.7560637871424357, saving...\n",
      "164: Best loss 0.7498715029822456, saving...\n",
      "169: Best loss 0.7488731225331624, saving...\n",
      "170: Best loss 0.7454869429270427, saving...\n",
      "171: Best loss 0.7380708403057523, saving...\n",
      "172: Best loss 0.7356433073679607, saving...\n",
      "174: Best loss 0.7349148511886596, saving...\n",
      "176: Best loss 0.7318943871392144, saving...\n",
      "179: Best loss 0.7249165005154079, saving...\n",
      "180: Best loss 0.7218235810597737, saving...\n",
      "182: Best loss 0.7033492962519329, saving...\n",
      "189: Best loss 0.6977538373735216, saving...\n",
      "194: Best loss 0.686594197485182, saving...\n",
      "195: Best loss 0.6831122981177437, saving...\n",
      "198: Best loss 0.6787685659196642, saving...\n",
      "201: Best loss 0.6750687890582614, saving...\n",
      "204: Best loss 0.6702609141667684, saving...\n",
      "205: Best loss 0.6645193444357977, saving...\n",
      "210: Best loss 0.6605804867214627, saving...\n",
      "211: Best loss 0.6557128879759047, saving...\n",
      "214: Best loss 0.6557009670469496, saving...\n",
      "215: Best loss 0.6522884819242689, saving...\n",
      "216: Best loss 0.6514980501598782, saving...\n",
      "217: Best loss 0.6455425924725002, saving...\n",
      "219: Best loss 0.6362824069129096, saving...\n",
      "220: Best loss 0.6362172947989571, saving...\n",
      "223: Best loss 0.6326558801862928, saving...\n",
      "225: Best loss 0.6317881716622247, saving...\n",
      "229: Best loss 0.6260145637724135, saving...\n",
      "232: Best loss 0.6217306560940212, saving...\n",
      "233: Best loss 0.6212338328361511, saving...\n",
      "237: Best loss 0.6185162531005012, saving...\n",
      "240: Best loss 0.6091201914681329, saving...\n",
      "244: Best loss 0.6045654826694065, saving...\n",
      "251: Best loss 0.601024407810635, saving...\n",
      "252: Best loss 0.5972710940572951, saving...\n",
      "255: Best loss 0.5961829278204177, saving...\n",
      "258: Best loss 0.5835096226798163, saving...\n",
      "264: Best loss 0.5758197479777867, saving...\n",
      "272: Best loss 0.5749445239702861, saving...\n",
      "273: Best loss 0.5687643183602227, saving...\n",
      "279: Best loss 0.5657922546068828, saving...\n",
      "280: Best loss 0.5639221959643894, saving...\n",
      "284: Best loss 0.5600563261244031, saving...\n",
      "286: Best loss 0.5561172664165497, saving...\n",
      "290: Best loss 0.5546561837196351, saving...\n",
      "295: Best loss 0.5493371427059173, saving...\n",
      "302: Best loss 0.5476066814528571, saving...\n",
      "303: Best loss 0.5455067886246575, saving...\n",
      "306: Best loss 0.5443024032645756, saving...\n",
      "307: Best loss 0.5430058810445997, saving...\n",
      "309: Best loss 0.5392671896351707, saving...\n",
      "312: Best loss 0.5332932518588172, saving...\n",
      "317: Best loss 0.5251535362667508, saving...\n",
      "327: Best loss 0.517787100871404, saving...\n",
      "341: Best loss 0.515267402595944, saving...\n",
      "345: Best loss 0.5124650663799709, saving...\n",
      "346: Best loss 0.5019712269306182, saving...\n",
      "350: Best loss 0.49696010020044107, saving...\n",
      "356: Best loss 0.48423811793327326, saving...\n",
      "365: Best loss 0.4702838606304593, saving...\n",
      "372: Best loss 0.46969508330027265, saving...\n",
      "373: Best loss 0.4647867547141181, saving...\n",
      "376: Best loss 0.46036075154940287, saving...\n",
      "381: Best loss 0.449075495534473, saving...\n",
      "388: Best loss 0.44882932239108614, saving...\n",
      "397: Best loss 0.4463264326254527, saving...\n",
      "400: Best loss 0.4461618158552382, saving...\n",
      "401: Best loss 0.43669232527414953, saving...\n",
      "406: Best loss 0.43601942227946383, saving...\n",
      "407: Best loss 0.43157474001248675, saving...\n",
      "408: Best loss 0.4303681698110368, saving...\n",
      "416: Best loss 0.429711992210812, saving...\n",
      "418: Best loss 0.42319507797559097, saving...\n",
      "426: Best loss 0.4223668939537472, saving...\n",
      "433: Best loss 0.42159773574935067, saving...\n",
      "435: Best loss 0.42135968936814194, saving...\n",
      "439: Best loss 0.4126595695813497, saving...\n",
      "441: Best loss 0.41258358293109476, saving...\n",
      "443: Best loss 0.40958756175306105, saving...\n",
      "444: Best loss 0.4090994371308221, saving...\n",
      "445: Best loss 0.40892073975669013, saving...\n",
      "449: Best loss 0.391268272863494, saving...\n",
      "460: Best loss 0.39017303519778784, saving...\n",
      "465: Best loss 0.3872544884681701, saving...\n",
      "468: Best loss 0.3860993534326554, saving...\n",
      "469: Best loss 0.38336620463265314, saving...\n",
      "470: Best loss 0.3796418713198768, saving...\n",
      "474: Best loss 0.37489398105276955, saving...\n",
      "479: Best loss 0.3702968411975437, saving...\n",
      "491: Best loss 0.36006668342484366, saving...\n",
      "496: Best loss 0.36002406411700777, saving...\n",
      "Saving training results to models/bpsk_qpsk_seq_16_3_15_trained_on_2048_dropout_0.25_500_epochs.pt\n",
      "0: Best loss 1.7366655429204303, saving...\n",
      "1: Best loss 1.3452104210853577, saving...\n",
      "2: Best loss 1.1729323108990988, saving...\n",
      "3: Best loss 1.1599807063738505, saving...\n",
      "4: Best loss 1.1275922179222106, saving...\n",
      "5: Best loss 1.124750339984894, saving...\n",
      "6: Best loss 1.1033625562985738, saving...\n",
      "7: Best loss 1.1000171144803366, saving...\n",
      "8: Best loss 1.0986121376355489, saving...\n",
      "9: Best loss 1.0844379345575967, saving...\n",
      "11: Best loss 1.0728418827056885, saving...\n",
      "12: Best loss 1.06565793355306, saving...\n",
      "18: Best loss 1.0653182784716286, saving...\n",
      "23: Best loss 1.0631574988365173, saving...\n",
      "24: Best loss 1.0612101038297017, saving...\n",
      "43: Best loss 1.06062916914622, saving...\n",
      "56: Best loss 1.0589670022328694, saving...\n",
      "71: Best loss 1.056715746720632, saving...\n",
      "89: Best loss 1.0559405247370401, saving...\n",
      "91: Best loss 1.0555984298388164, saving...\n",
      "94: Best loss 1.0553924004236857, saving...\n",
      "95: Best loss 1.0509832700093589, saving...\n",
      "96: Best loss 1.0487417618433634, saving...\n",
      "101: Best loss 1.0471485614776612, saving...\n",
      "104: Best loss 1.045720104376475, saving...\n",
      "108: Best loss 1.0405200362205507, saving...\n",
      "111: Best loss 1.0380906224250792, saving...\n",
      "114: Best loss 1.0338214755058288, saving...\n",
      "117: Best loss 1.0336963136990864, saving...\n",
      "119: Best loss 1.0305941462516786, saving...\n",
      "120: Best loss 1.0246650417645773, saving...\n",
      "124: Best loss 1.0213448564211527, saving...\n",
      "126: Best loss 1.0209316929181418, saving...\n",
      "127: Best loss 1.0183217803637188, saving...\n",
      "128: Best loss 1.0150846441586814, saving...\n",
      "131: Best loss 1.0133737166722614, saving...\n",
      "132: Best loss 1.0126536965370176, saving...\n",
      "133: Best loss 1.0067708293596906, saving...\n",
      "134: Best loss 1.0062846779823302, saving...\n",
      "136: Best loss 0.9999182303746541, saving...\n",
      "138: Best loss 0.9975555976231893, saving...\n",
      "140: Best loss 0.9904260158538818, saving...\n",
      "142: Best loss 0.9901647567749023, saving...\n",
      "143: Best loss 0.9857274015744528, saving...\n",
      "146: Best loss 0.9829729835192362, saving...\n",
      "148: Best loss 0.9791818579037983, saving...\n",
      "149: Best loss 0.9790623545646668, saving...\n",
      "151: Best loss 0.9706134915351867, saving...\n",
      "154: Best loss 0.9677364826202393, saving...\n",
      "156: Best loss 0.9664517442385356, saving...\n",
      "157: Best loss 0.9654830813407898, saving...\n",
      "158: Best loss 0.9636464595794678, saving...\n",
      "159: Best loss 0.9606663584709166, saving...\n",
      "161: Best loss 0.952238174279531, saving...\n",
      "163: Best loss 0.9516932169596355, saving...\n",
      "164: Best loss 0.9492700894673665, saving...\n",
      "166: Best loss 0.9476239760716758, saving...\n",
      "171: Best loss 0.9376638412475586, saving...\n",
      "173: Best loss 0.9349670608838399, saving...\n",
      "178: Best loss 0.9319509744644165, saving...\n",
      "180: Best loss 0.9223215301831564, saving...\n",
      "184: Best loss 0.9213525136311849, saving...\n",
      "186: Best loss 0.9177305499712627, saving...\n",
      "188: Best loss 0.9137324412663779, saving...\n",
      "189: Best loss 0.9117419163386026, saving...\n",
      "190: Best loss 0.9104179461797078, saving...\n",
      "192: Best loss 0.9088383873303731, saving...\n",
      "195: Best loss 0.9051370024681091, saving...\n",
      "198: Best loss 0.9004141171773274, saving...\n",
      "199: Best loss 0.9000878135363262, saving...\n",
      "200: Best loss 0.8962668577829997, saving...\n",
      "202: Best loss 0.8957265297571819, saving...\n",
      "203: Best loss 0.888491686185201, saving...\n",
      "207: Best loss 0.8879503766695658, saving...\n",
      "210: Best loss 0.8861628969510398, saving...\n",
      "211: Best loss 0.8829449971516927, saving...\n",
      "213: Best loss 0.875658941268921, saving...\n",
      "215: Best loss 0.8729779799779257, saving...\n",
      "221: Best loss 0.8668060620625813, saving...\n",
      "228: Best loss 0.8644859830538432, saving...\n",
      "230: Best loss 0.8631248195966086, saving...\n",
      "232: Best loss 0.8599482258160909, saving...\n",
      "237: Best loss 0.8595056494077047, saving...\n",
      "240: Best loss 0.8542746384938558, saving...\n",
      "241: Best loss 0.8525408267974854, saving...\n",
      "248: Best loss 0.8463534673055013, saving...\n",
      "252: Best loss 0.8446114659309387, saving...\n",
      "255: Best loss 0.8415868759155274, saving...\n",
      "258: Best loss 0.8380848248799642, saving...\n",
      "267: Best loss 0.8364720741907755, saving...\n",
      "268: Best loss 0.8340416193008422, saving...\n",
      "269: Best loss 0.8320794304211934, saving...\n",
      "270: Best loss 0.8303793470064799, saving...\n",
      "271: Best loss 0.8296283801396687, saving...\n",
      "280: Best loss 0.8271636048952737, saving...\n",
      "282: Best loss 0.8214432915051778, saving...\n",
      "287: Best loss 0.8210278431574504, saving...\n",
      "291: Best loss 0.8202454090118408, saving...\n",
      "292: Best loss 0.8189801772435507, saving...\n",
      "300: Best loss 0.8187612930933634, saving...\n",
      "306: Best loss 0.8117122173309325, saving...\n",
      "308: Best loss 0.8113626639048259, saving...\n",
      "313: Best loss 0.8091026902198791, saving...\n",
      "315: Best loss 0.8084477941195171, saving...\n",
      "316: Best loss 0.8079498569170634, saving...\n",
      "318: Best loss 0.8077036062876384, saving...\n",
      "323: Best loss 0.8068045655886332, saving...\n",
      "325: Best loss 0.8061131775379181, saving...\n",
      "326: Best loss 0.8048442264397939, saving...\n",
      "330: Best loss 0.7980407714843749, saving...\n",
      "339: Best loss 0.79449245929718, saving...\n",
      "341: Best loss 0.7934980551401775, saving...\n",
      "342: Best loss 0.7878555158774058, saving...\n",
      "360: Best loss 0.787350602944692, saving...\n",
      "367: Best loss 0.7850493768850961, saving...\n",
      "369: Best loss 0.7846199949582419, saving...\n",
      "370: Best loss 0.7831157724062602, saving...\n",
      "371: Best loss 0.7809920012950898, saving...\n",
      "375: Best loss 0.7805627842744192, saving...\n",
      "376: Best loss 0.7763255039850872, saving...\n",
      "379: Best loss 0.771324247121811, saving...\n",
      "386: Best loss 0.7682710647583006, saving...\n",
      "390: Best loss 0.7677736083666484, saving...\n",
      "391: Best loss 0.7625646611054738, saving...\n",
      "423: Best loss 0.7623410284519196, saving...\n",
      "424: Best loss 0.7597623745600383, saving...\n",
      "429: Best loss 0.7504684825738271, saving...\n",
      "430: Best loss 0.7465883930524191, saving...\n",
      "448: Best loss 0.743333081404368, saving...\n",
      "485: Best loss 0.7413657804330189, saving...\n",
      "487: Best loss 0.7380439658959707, saving...\n",
      "488: Best loss 0.7361132482687633, saving...\n",
      "495: Best loss 0.7354456265767415, saving...\n",
      "496: Best loss 0.7302305340766906, saving...\n",
      "Saving training results to models/bpsk_qpsk_seq_16_3_20_trained_on_2048_dropout_0.25_500_epochs.pt\n",
      "0: Best loss 1.7400641441345215, saving...\n",
      "1: Best loss 1.2586105982462565, saving...\n",
      "2: Best loss 1.2198742787043255, saving...\n",
      "3: Best loss 1.1707637667655946, saving...\n",
      "4: Best loss 1.1529659787813824, saving...\n",
      "6: Best loss 1.140419884522756, saving...\n",
      "7: Best loss 1.1341190258661906, saving...\n",
      "8: Best loss 1.1322735985120138, saving...\n",
      "10: Best loss 1.1201152483622232, saving...\n",
      "13: Best loss 1.1144596934318542, saving...\n",
      "16: Best loss 1.1117253065109254, saving...\n",
      "19: Best loss 1.1101613283157348, saving...\n",
      "26: Best loss 1.1084100047747294, saving...\n",
      "28: Best loss 1.1051900029182433, saving...\n",
      "30: Best loss 1.1050134221712749, saving...\n",
      "36: Best loss 1.1041928887367247, saving...\n",
      "37: Best loss 1.1009613394737245, saving...\n",
      "38: Best loss 1.1000344435373943, saving...\n",
      "40: Best loss 1.0967504064242044, saving...\n",
      "42: Best loss 1.0891880750656129, saving...\n",
      "43: Best loss 1.0891146381696066, saving...\n",
      "44: Best loss 1.0843923330307008, saving...\n",
      "47: Best loss 1.0787565867106121, saving...\n",
      "49: Best loss 1.076770762602488, saving...\n",
      "50: Best loss 1.0715913852055867, saving...\n",
      "51: Best loss 1.0687360644340516, saving...\n",
      "52: Best loss 1.0650416731834411, saving...\n",
      "53: Best loss 1.0618368109067282, saving...\n",
      "55: Best loss 1.056661558151245, saving...\n",
      "57: Best loss 1.0497370481491088, saving...\n",
      "58: Best loss 1.0431002736091612, saving...\n",
      "60: Best loss 1.0414130330085754, saving...\n",
      "61: Best loss 1.0356712341308594, saving...\n",
      "63: Best loss 1.030931584040324, saving...\n",
      "64: Best loss 1.030928667386373, saving...\n",
      "65: Best loss 1.021685258547465, saving...\n",
      "67: Best loss 1.0170264045397441, saving...\n",
      "68: Best loss 1.0167245944341023, saving...\n",
      "69: Best loss 1.0085408846537272, saving...\n",
      "70: Best loss 1.001898225148519, saving...\n",
      "72: Best loss 1.001068862279256, saving...\n",
      "74: Best loss 0.9919963240623475, saving...\n",
      "75: Best loss 0.9894319494565327, saving...\n",
      "76: Best loss 0.9841092586517335, saving...\n",
      "77: Best loss 0.9797178069750467, saving...\n",
      "79: Best loss 0.9795252323150634, saving...\n",
      "81: Best loss 0.9722641905148823, saving...\n",
      "84: Best loss 0.961185057957967, saving...\n",
      "86: Best loss 0.9602553764979045, saving...\n",
      "87: Best loss 0.9562509298324586, saving...\n",
      "88: Best loss 0.9520493507385254, saving...\n",
      "89: Best loss 0.94428608417511, saving...\n",
      "91: Best loss 0.9436012705167135, saving...\n",
      "92: Best loss 0.9392119288444519, saving...\n",
      "95: Best loss 0.9319233417510986, saving...\n",
      "96: Best loss 0.9153247912724812, saving...\n",
      "100: Best loss 0.9117731054623921, saving...\n",
      "101: Best loss 0.9108673254648845, saving...\n",
      "102: Best loss 0.9061033646265666, saving...\n",
      "104: Best loss 0.9011690974235533, saving...\n",
      "105: Best loss 0.8990300138791403, saving...\n",
      "106: Best loss 0.8885428269704183, saving...\n",
      "112: Best loss 0.8872499624888103, saving...\n",
      "113: Best loss 0.8801332751909892, saving...\n",
      "115: Best loss 0.8786867976188661, saving...\n",
      "116: Best loss 0.8768107096354166, saving...\n",
      "118: Best loss 0.8671939690907797, saving...\n",
      "122: Best loss 0.8657430926958719, saving...\n",
      "128: Best loss 0.8619353930155436, saving...\n",
      "130: Best loss 0.8609508633613585, saving...\n",
      "131: Best loss 0.8548540592193603, saving...\n",
      "133: Best loss 0.8541815161705018, saving...\n",
      "134: Best loss 0.8430887381235758, saving...\n",
      "135: Best loss 0.8399413824081421, saving...\n",
      "136: Best loss 0.8324710925420125, saving...\n",
      "140: Best loss 0.8302068869272867, saving...\n",
      "141: Best loss 0.8251318136850992, saving...\n",
      "143: Best loss 0.8182106932004293, saving...\n",
      "145: Best loss 0.8122492154439291, saving...\n",
      "146: Best loss 0.8058403571446737, saving...\n",
      "151: Best loss 0.7950148145357767, saving...\n",
      "156: Best loss 0.7899412433306376, saving...\n",
      "158: Best loss 0.7774960875511169, saving...\n",
      "165: Best loss 0.7733442862828572, saving...\n",
      "168: Best loss 0.7687239527702332, saving...\n",
      "171: Best loss 0.7675254583358765, saving...\n",
      "173: Best loss 0.7642243822415671, saving...\n",
      "174: Best loss 0.7569513718287151, saving...\n",
      "176: Best loss 0.7516532460848491, saving...\n",
      "178: Best loss 0.7500772754351297, saving...\n",
      "179: Best loss 0.7480562965075176, saving...\n",
      "180: Best loss 0.740899391969045, saving...\n",
      "182: Best loss 0.7355114221572876, saving...\n",
      "186: Best loss 0.7286090175310771, saving...\n",
      "191: Best loss 0.7264841854572297, saving...\n",
      "193: Best loss 0.7246236642201741, saving...\n",
      "194: Best loss 0.7225672721862794, saving...\n",
      "197: Best loss 0.7200725356737772, saving...\n",
      "201: Best loss 0.709147709608078, saving...\n",
      "204: Best loss 0.6980232715606689, saving...\n",
      "208: Best loss 0.697211500008901, saving...\n",
      "213: Best loss 0.6860345979531606, saving...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2179586e48f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mdata_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq_length'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             train_network(data_config, train_config, model_config=(16,3), gpu=True, dropout=dropout, tf_rate=0, numpy_seed=0, torch_seed=0, \n\u001b[0m\u001b[1;32m     11\u001b[0m                         savepath=f\"models/bpsk_qpsk_seq_{model_config[0]}_{model_config[1]}_{data_config['seq_length']}_trained_on_2048_dropout_{dropout}_{train_config['num_epochs']}_epochs.pt\")\n",
      "\u001b[0;32m<ipython-input-10-5c88cc926914>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(data_config, train_config, model_config, gpu, dropout, tf_rate, numpy_seed, torch_seed, savepath)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-65bc28f9edb4>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_train, y_train, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, tf_ratio, gpu, num_classes)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hdd/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hdd/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seq_lengths = [10,15,20]\n",
    "model_configs = [(16,3), (128,3)]\n",
    "train_config['num_epochs'] = 250\n",
    "dropouts = [0.25,0.5]\n",
    "for dropout in dropouts:\n",
    "    for seq_length in seq_lengths:\n",
    "        for model_config in model_configs:\n",
    "            data_config['seq_length'] = seq_length\n",
    "\n",
    "            train_network(data_config, train_config, model_config=(16,3), gpu=True, dropout=dropout, tf_rate=0, numpy_seed=0, torch_seed=0, \n",
    "                        savepath=f\"models/bpsk_qpsk_seq_{model_config[0]}_{model_config[1]}_{data_config['seq_length']}_trained_on_2048_dropout_{dropout}_{train_config['num_epochs']}_epochs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9768d9a9a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "legend_elements = []\n",
    "for idx, model_config in enumerate(model_configs):\n",
    "    model = torch.load(f\"models/bpsk_qpsk_seq_{model_config[0]}_{model_config[1]}_10_trained_on_2048.pt\")\n",
    "    plt.plot(model['losses'], f'C{idx}')\n",
    "    plt.plot(model['val_losses'], f'C{idx}--')\n",
    "\n",
    "    legend_elements.append(Line2D([0], [0], color=f'C{idx}', label=f\"Layers: {model_config[1]}, Size: {model_config[0]}\"))\n",
    "\n",
    "plt.legend(handles=legend_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9768bee340>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "legend_elements = []\n",
    "for idx, model_config in enumerate(model_configs):\n",
    "    model = torch.load(f\"models/bpsk_qpsk_seq_{model_config[0]}_{model_config[1]}_20_trained_on_2048.pt\")\n",
    "    plt.plot(model['losses'], f'C{idx}')\n",
    "    plt.plot(model['val_losses'], f'C{idx}--')\n",
    "\n",
    "    legend_elements.append(Line2D([0], [0], color=f'C{idx}', label=f\"Layers: {model_config[1]}, Size: {model_config[0]}\"))\n",
    "\n",
    "plt.legend(handles=legend_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mod_accuracy(encoder, decoder, snr_range, mod_order=2, num_examples=256, seq_length=5, sps=4, num_classes=6):\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for snr in snr_range:\n",
    "\n",
    "        x, y = gen_data(num_examples, seq_length, mod_order=mod_order, sps=sps, snr=snr)\n",
    "        if mod_order == 4:\n",
    "            y += 2\n",
    "        if mod_order == 8:\n",
    "            y += 6\n",
    "        x,y = to_tensors(x,y,batch_first=True)\n",
    "\n",
    "        # create dataset and dataloader\n",
    "        test_dataset = torch.utils.data.TensorDataset(x, y)\n",
    "        test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "        corrects, totals = 0, 0\n",
    "        for x,y in test_dataloader:\n",
    "            x = x.permute((1,0,2))\n",
    "            y = y.unsqueeze(2).permute((1,0,2))\n",
    "            y_hat = inference(x, encoder, decoder, num_classes=num_classes)\n",
    "\n",
    "            if mod_order == 2:\n",
    "                corrects += torch.sum(y_hat.argmax(axis=2) < 2)\n",
    "            elif mod_order == 4:\n",
    "                corrects += torch.sum(sum([y_hat.argmax(axis=2) == val for val in [2,3,4,5]]))\n",
    "            elif mod_order == 8:\n",
    "                corrects += torch.sum(y_hat.argmax(axis=2) > 5)\n",
    "\n",
    "            totals += y.numel()\n",
    "\n",
    "        accuracy = np.array(corrects)/totals\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 3)\n",
      "(128, 3)\n"
     ]
    }
   ],
   "source": [
    "for model_config in model_configs:\n",
    "    print(model_config)\n",
    "    for seq_length in seq_lengths:\n",
    "        # Create the encoder and decoder models\n",
    "        encoder = EncoderRNN(hidden_size=model_config[0], input_size=2, num_layers=model_config[1], device = 'cuda' if gpu else 'cpu')\n",
    "        decoder = DecoderRNN(hidden_size=model_config[0], input_size=6, num_layers=model_config[1], device = 'cuda' if gpu else 'cpu')\n",
    "        \n",
    "        encoder.cpu()\n",
    "        decoder.cpu()\n",
    "        \n",
    "        model = torch.load(f\"models/bpsk_qpsk_seq_{model_config[0]}_{model_config[1]}_{seq_length}_trained_on_2048.pt\")\n",
    "        encoder.load_state_dict(model['encoder'])\n",
    "        decoder.load_state_dict(model['decoder'])\n",
    "\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        bpsk_accs = eval_accuracy(encoder, decoder, snr_range, num_examples=512, mod_order=2, seq_length=seq_length, gpu=False)\n",
    "        qpsk_accs = eval_accuracy(encoder, decoder, snr_range, num_examples=512, mod_order=4, seq_length=seq_length, gpu=False)\n",
    "        \n",
    "        bpsk_class_accuracy = eval_mod_accuracy(encoder, decoder, snr_range, num_examples=512, mod_order=2, seq_length=seq_length)\n",
    "        qpsk_class_accuracy = eval_mod_accuracy(encoder, decoder, snr_range, num_examples=512, mod_order=4, seq_length=seq_length)\n",
    "        \n",
    "        results = {'snr_range': snr_range,\n",
    "                'bpsk_accs': bpsk_accs,\n",
    "                'qpsk_accs': qpsk_accs,\n",
    "                'bpsk_class_accuracy': bpsk_class_accuracy,\n",
    "                'qpsk_class_accuracy': qpsk_class_accuracy}\n",
    "        \n",
    "        torch.save(results, f\"results/bpsk_qpsk_seq_{model_config[0]}_{model_config[1]}_{seq_length}_trained_on_2048.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 3)\n",
      "0.25\n",
      "0.5\n",
      "(128, 3)\n",
      "0.25\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "tf_ratios = [0.25, 0.5]\n",
    "for model_config in model_configs:\n",
    "    print(model_config)\n",
    "    for tf_ratio in tf_ratios:\n",
    "        print(tf_ratio)\n",
    "        for seq_length in seq_lengths:\n",
    "            # Create the encoder and decoder models\n",
    "            encoder = EncoderRNN(hidden_size=model_config[0], input_size=2, num_layers=model_config[1], device = 'cuda' if gpu else 'cpu')\n",
    "            decoder = DecoderRNN(hidden_size=model_config[0], input_size=6, num_layers=model_config[1], device = 'cuda' if gpu else 'cpu')\n",
    "            \n",
    "            encoder.cpu()\n",
    "            decoder.cpu()\n",
    "            \n",
    "            model = torch.load(f\"models/bpsk_qpsk_seq_{model_config[0]}_{model_config[1]}_{seq_length}_trained_on_2048_tf_ratio_{tf_ratio}.pt\")\n",
    "            encoder.load_state_dict(model['encoder'])\n",
    "            decoder.load_state_dict(model['decoder'])\n",
    "\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            \n",
    "            bpsk_accs = eval_accuracy(encoder, decoder, snr_range, num_examples=512, mod_order=2, seq_length=seq_length, gpu=False)\n",
    "            qpsk_accs = eval_accuracy(encoder, decoder, snr_range, num_examples=512, mod_order=4, seq_length=seq_length, gpu=False)\n",
    "            \n",
    "            bpsk_class_accuracy = eval_mod_accuracy(encoder, decoder, snr_range, num_examples=512, mod_order=2, seq_length=seq_length)\n",
    "            qpsk_class_accuracy = eval_mod_accuracy(encoder, decoder, snr_range, num_examples=512, mod_order=4, seq_length=seq_length)\n",
    "            \n",
    "            results = {'snr_range': snr_range,\n",
    "                    'bpsk_accs': bpsk_accs,\n",
    "                    'qpsk_accs': qpsk_accs,\n",
    "                    'bpsk_class_accuracy': bpsk_class_accuracy,\n",
    "                    'qpsk_class_accuracy': qpsk_class_accuracy}\n",
    "            \n",
    "            torch.save(results, f\"results/bpsk_qpsk_seq_{model_config[0]}_{model_config[1]}_{seq_length}_trained_on_2048_tf_ratio_{tf_ratio}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_config in model_configs:\n",
    "    print(model_config)\n",
    "    for dropout in dropouts:\n",
    "        print(dropout)\n",
    "        for seq_length in seq_lengths:\n",
    "            # Create the encoder and decoder models\n",
    "            encoder = EncoderRNN(hidden_size=model_config[0], input_size=2, num_layers=model_config[1], device = 'cuda' if gpu else 'cpu')\n",
    "            decoder = DecoderRNN(hidden_size=model_config[0], input_size=6, num_layers=model_config[1], device = 'cuda' if gpu else 'cpu')\n",
    "            \n",
    "            encoder.cpu()\n",
    "            decoder.cpu()\n",
    "            \n",
    "            model = torch.load(f\"models/bpsk_qpsk_seq_{model_config[0]}_{model_config[1]}_{seq_length}_trained_on_2048_dropout_{dropout}.pt\")\n",
    "            encoder.load_state_dict(model['encoder'])\n",
    "            decoder.load_state_dict(model['decoder'])\n",
    "\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            \n",
    "            bpsk_accs = eval_accuracy(encoder, decoder, snr_range, num_examples=512, mod_order=2, seq_length=seq_length, gpu=False)\n",
    "            qpsk_accs = eval_accuracy(encoder, decoder, snr_range, num_examples=512, mod_order=4, seq_length=seq_length, gpu=False)\n",
    "            \n",
    "            bpsk_class_accuracy = eval_mod_accuracy(encoder, decoder, snr_range, num_examples=512, mod_order=2, seq_length=seq_length)\n",
    "            qpsk_class_accuracy = eval_mod_accuracy(encoder, decoder, snr_range, num_examples=512, mod_order=4, seq_length=seq_length)\n",
    "            \n",
    "            results = {'snr_range': snr_range,\n",
    "                    'bpsk_accs': bpsk_accs,\n",
    "                    'qpsk_accs': qpsk_accs,\n",
    "                    'bpsk_class_accuracy': bpsk_class_accuracy,\n",
    "                    'qpsk_class_accuracy': qpsk_class_accuracy}\n",
    "            \n",
    "            torch.save(results, f\"results/bpsk_qpsk_seq_{model_config[0]}_{model_config[1]}_{seq_length}_trained_on_2048_dropout_{dropout}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
